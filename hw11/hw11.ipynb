{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b5cFq_TgWlQ_"
   },
   "source": [
    "# Homework 11 - Transfer Learning (Domain Adversarial Training)\n",
    "\n",
    "> Author: Arvin Liu (r09922071@ntu.edu.tw)\n",
    "\n",
    "若有任何問題，歡迎來信至助教信箱 ntu-ml-2021spring-ta@googlegroups.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vNiZCGrIYKdR"
   },
   "source": [
    "# Readme\n",
    "\n",
    "\n",
    "這份作業的任務是Transfer Learning中的Domain Adversarial Training。\n",
    "\n",
    "<img src=\"https://i.imgur.com/iMVIxCH.png\" width=\"500px\">\n",
    "\n",
    "> 也就是左下角的那一塊。\n",
    "\n",
    "## Scenario and Why Domain Adversarial Training\n",
    "你現在有Source Data + label，其中Source Data和Target Data可能有點關係，所以你想要訓練一個model做在Source Data上並Predict在Target Data上。\n",
    "\n",
    "但這樣有什麼樣的問題? 相信大家學過Anomaly Detection就會知道，如果有data是在Source Data沒有出現過的(或稱Abnormal的)，那麼model大部分都會因為不熟悉這個data而可能亂做一發。 \n",
    "\n",
    "以下我們將model拆成Feature Extractor(上半部)和Classifier(下半部)來作例子:\n",
    "<img src=\"https://i.imgur.com/IL0PxCY.png\" width=\"500px\">\n",
    "\n",
    "整個Model在學習Source Data的時候，Feature Extrator因為看過很多次Source Data，所以所抽取出來的Feature可能就頗具意義，例如像圖上的藍色Distribution，已經將圖片分成各個Cluster，所以這個時候Classifier就可以依照這個Cluster去預測結果。\n",
    "\n",
    "但是在做Target Data的時候，Feature Extractor會沒看過這樣的Data，導致輸出的Target Feature可能不屬於在Source Feature Distribution上，這樣的Feature給Classifier預測結果顯然就不會做得好。\n",
    "\n",
    "## Domain Adversarial Training of Nerural Networks (DaNN)\n",
    "基於如此，是不是只要讓Soucre Data和Target Data經過Feature Extractor都在同個Distribution上，就會做得好了呢? 這就是DaNN的主要核心。\n",
    "\n",
    "<img src=\"https://i.imgur.com/vrOE5a6.png\" width=\"500px\">\n",
    "\n",
    "我們追加一個Domain Classifier，在學習的過程中，讓Domain Classifier去判斷經過Feature Extractor後的Feature是源自於哪個domain，讓Feature Extractor學習如何產生Feature以**騙過**Domain Classifier。 持久下來，通常Feature Extractor都會打贏Domain Classifier。(因為Domain Classifier的Input來自於Feature Extractor，而且對Feature Extractor來說Domain&Classification的任務並沒有衝突。)\n",
    "\n",
    "如此一來，我們就可以確信不管是哪一個Domain，Feature Extractor都會把它產生在同一個Feature Distribution上。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3-qnUkspmap3"
   },
   "source": [
    "# Data Introduce\n",
    "\n",
    "這次的任務是Source Data: 真實照片，Target Data: 手畫塗鴉。\n",
    "\n",
    "我們必須讓model看過真實照片以及標籤，嘗試去預測手畫塗鴉的標籤為何。\n",
    "\n",
    "資料位於[這裡](https://drive.google.com/open?id=12-07DSquGdzN3JBHBChN4nMo3i8BqTiL)，以下的code分別為下載和觀看這次的資料大概長甚麼樣子。\n",
    "\n",
    "特別注意一點: **這次的source和target data的圖片都是平衡的，你們可以使用這個資訊做其他事情。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2YULyb-rqawq",
    "outputId": "03be1894-7538-430e-d2c7-b644a1f00ca5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May 30 10:44:49 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   36C    P0    24W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DF-i0sVlnUbq",
    "outputId": "4c157755-3820-4a15-eb63-ca7a85849a7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1P4fGNb9JhJj8W0DA_Qrp7mbrRHfF5U_f\n",
      "To: /content/real_or_drawing.zip\n",
      "107MB [00:00, 205MB/s] \n",
      "Archive:  real_or_drawing.zip\n",
      "replace real_or_drawing/test_data/0/00000.bmp? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
      "replace real_or_drawing/test_data/0/00001.bmp? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
     ]
    }
   ],
   "source": [
    "# Download dataset\n",
    "#!gdown --id '1P4fGNb9JhJj8W0DA_Qrp7mbrRHfF5U_f' --output real_or_drawing.zip\n",
    "# Unzip the files\n",
    "#!unzip real_or_drawing.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "id": "0_uO-ZSDoR6i",
    "outputId": "abaece5b-6bd2-4387-b94a-630e8579fc9d"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x1296 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def no_axis_show(img, title='', cmap=None):\n",
    "  # imshow, and set the interpolation mode to be \"nearest\"。\n",
    "  fig = plt.imshow(img, interpolation='nearest', cmap=cmap)\n",
    "  # do not show the axes in the images.\n",
    "  fig.axes.get_xaxis().set_visible(False)\n",
    "  fig.axes.get_yaxis().set_visible(False)\n",
    "  plt.title(title)\n",
    "\n",
    "titles = ['horse', 'bed', 'clock', 'apple', 'cat', 'plane', 'television', 'dog', 'dolphin', 'spider']\n",
    "plt.figure(figsize=(18, 18))\n",
    "for i in range(10):\n",
    "  plt.subplot(1, 10, i+1)\n",
    "  fig = no_axis_show(plt.imread(f'real_or_drawing/train_data/{i}/{500*i}.bmp'), title=titles[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120
    },
    "id": "3eMs7DbVt4Ee",
    "outputId": "8926b441-8c3b-4f7e-ef66-c984eba79c00"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x1296 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(18, 18))\n",
    "for i in range(10):\n",
    "  plt.subplot(1, 10, i+1)\n",
    "  fig = no_axis_show(plt.imread(f'real_or_drawing/test_data/0/' + str(i).rjust(5, '0') + '.bmp'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "moXQw9To5TqZ"
   },
   "source": [
    "# Special Domain Knowledge\n",
    "\n",
    "因為大家塗鴉的時候通常只會畫輪廓，我們可以根據這點將source data做點邊緣偵測處理，讓source data更像target data一點。\n",
    "\n",
    "## Canny Edge Detection\n",
    "算法這邊不贅述，只教大家怎麼用。若有興趣歡迎參考wiki或[這裡](https://medium.com/@pomelyu5199/canny-edge-detector-%E5%AF%A6%E4%BD%9C-opencv-f7d1a0a57d19)。\n",
    "\n",
    "cv2.Canny使用非常方便，只需要兩個參數: low_threshold, high_threshold。\n",
    "\n",
    "```cv2.Canny(image, low_threshold, high_threshold)```\n",
    "\n",
    "簡單來說就是當邊緣值超過high_threshold，我們就確定它是edge。如果只有超過low_threshold，那就先判斷一下再決定是不是edge。\n",
    "\n",
    "以下我們直接拿source data做做看。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "id": "mn2MkDLV7E2-",
    "outputId": "ad2d7c39-8c4a-4245-fdb6-f2d1d37d9346"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x1296 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "titles = ['horse', 'bed', 'clock', 'apple', 'cat', 'plane', 'television', 'dog', 'dolphin', 'spider']\n",
    "plt.figure(figsize=(18, 18))\n",
    "\n",
    "original_img = plt.imread(f'real_or_drawing/train_data/0/0.bmp')\n",
    "plt.subplot(1, 5, 1)\n",
    "no_axis_show(original_img, title='original')\n",
    "\n",
    "gray_img = cv2.cvtColor(original_img, cv2.COLOR_RGB2GRAY)\n",
    "plt.subplot(1, 5, 2)\n",
    "no_axis_show(gray_img, title='gray scale', cmap='gray')\n",
    "\n",
    "gray_img = cv2.cvtColor(original_img, cv2.COLOR_RGB2GRAY)\n",
    "plt.subplot(1, 5, 2)\n",
    "no_axis_show(gray_img, title='gray scale', cmap='gray')\n",
    "\n",
    "canny_50100 = cv2.Canny(gray_img, 50, 100)\n",
    "plt.subplot(1, 5, 3)\n",
    "no_axis_show(canny_50100, title='Canny(50, 100)', cmap='gray')\n",
    "\n",
    "canny_150200 = cv2.Canny(gray_img, 150, 200)\n",
    "plt.subplot(1, 5, 4)\n",
    "no_axis_show(canny_150200, title='Canny(150, 200)', cmap='gray')\n",
    "\n",
    "canny_250300 = cv2.Canny(gray_img, 250, 300)\n",
    "plt.subplot(1, 5, 5)\n",
    "no_axis_show(canny_250300, title='Canny(250, 300)', cmap='gray')\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8THSdt_hmwYh"
   },
   "source": [
    "# Data Process\n",
    "\n",
    "在這裡我故意將data用成可以使用torchvision.ImageFolder的形式，所以只要使用該函式便可以做出一個datasets。\n",
    "\n",
    "transform的部分請參考以下註解。\n",
    "<!-- \n",
    "#### 一些細節\n",
    "\n",
    "在一般的版本上，對灰階圖片使用RandomRotation使用```transforms.RandomRotation(15)```即可。但在colab上需要加上```fill=(0,)```才可運行。\n",
    "在n98上執行需要把```fill=(0,)```拿掉才可運行。 -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "WZHIBGknmi8Z"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function\n",
    "\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def same_seeds(seed):\n",
    "    # Python built-in random module\n",
    "    random.seed(seed)\n",
    "    # Numpy\n",
    "    np.random.seed(seed)\n",
    "    # Torch\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "same_seeds(1116)\n",
    "\n",
    "source_transform = transforms.Compose([\n",
    "    # Turn RGB to grayscale. (Bacause Canny do not support RGB images.)\n",
    "    transforms.Grayscale(),\n",
    "    # cv2 do not support skimage.Image, so we transform it to np.array, \n",
    "    # and then adopt cv2.Canny algorithm.\n",
    "    transforms.Lambda(lambda x: cv2.Canny(np.array(x), 170, 300)),\n",
    "    # Transform np.array back to the skimage.Image.\n",
    "    transforms.ToPILImage(),\n",
    "    # 50% Horizontal Flip. (For Augmentation)\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    # Rotate +- 15 degrees. (For Augmentation), and filled with zero \n",
    "    # if there's empty pixel after rotation.\n",
    "    transforms.RandomRotation(15, fill=(0,)),\n",
    "    # Transform to tensor for model inputs.\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "target_transform = transforms.Compose([\n",
    "    # Turn RGB to grayscale.\n",
    "    transforms.Grayscale(),\n",
    "    # Resize: size of source data is 32x32, thus we need to \n",
    "    #  enlarge the size of target data from 28x28 to 32x32。\n",
    "    transforms.Resize((32, 32)),\n",
    "    # 50% Horizontal Flip. (For Augmentation)\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    # Rotate +- 15 degrees. (For Augmentation), and filled with zero \n",
    "    # if there's empty pixel after rotation.\n",
    "    transforms.RandomRotation(15, fill=(0,)),\n",
    "    # Transform to tensor for model inputs.\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "source_dataset = ImageFolder('real_or_drawing/train_data', transform=source_transform)\n",
    "target_dataset = ImageFolder('real_or_drawing/test_data', transform=target_transform)\n",
    "\n",
    "source_dataloader = DataLoader(source_dataset, batch_size=32, shuffle=True)\n",
    "target_dataloader = DataLoader(target_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(target_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hdwDEMrOycs5"
   },
   "source": [
    "# Model\n",
    "\n",
    "Feature Extractor: 典型的VGG-like疊法。\n",
    "\n",
    "Label Predictor / Domain Classifier: MLP到尾。\n",
    "\n",
    "相信作業寫到這邊大家對以下的Layer都很熟悉，因此不再贅述。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "3uw2eP09z-pD"
   },
   "outputs": [],
   "source": [
    "class FeatureExtractor(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 3, 1, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, 1, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(128, 256, 3, 1, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(256, 256, 3, 1, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(256, 512, 3, 1, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x).squeeze()\n",
    "        return x\n",
    "\n",
    "class LabelPredictor(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LabelPredictor, self).__init__()\n",
    "\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, h):\n",
    "        c = self.layer(h)\n",
    "        return c\n",
    "\n",
    "class DomainClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DomainClassifier, self).__init__()\n",
    "\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(512, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(512, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(512, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(512, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(512, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, h):\n",
    "        y = self.layer(h)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lxdBIPhF0Icb"
   },
   "source": [
    "# Pre-processing\n",
    "\n",
    "這裡我們選用Adam來當Optimizer。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "hrxKelBy0PJ7"
   },
   "outputs": [],
   "source": [
    "feature_extractor = FeatureExtractor().cuda()\n",
    "label_predictor = LabelPredictor().cuda()\n",
    "domain_classifier = DomainClassifier().cuda()\n",
    "\n",
    "class_criterion = nn.CrossEntropyLoss()\n",
    "domain_criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer_F = optim.Adam(feature_extractor.parameters())\n",
    "optimizer_C = optim.Adam(label_predictor.parameters())\n",
    "optimizer_D = optim.Adam(domain_classifier.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xuAE4cqJ0itR"
   },
   "source": [
    "# Start Training\n",
    "\n",
    "\n",
    "## 如何實作DaNN?\n",
    "\n",
    "理論上，在原始paper中是加上Gradient Reversal Layer，並將Feature Extractor / Label Predictor / Domain Classifier 一起train，但其實我們也可以交換的train Domain Classfier & Feature Extractor(就像在train GAN的Generator & Discriminator一樣)，這也是可行的。\n",
    "\n",
    "在code實現中，我們採取後者的方式，畢竟GAN是之前的作業，應該會比較熟悉:)。\n",
    "\n",
    "## 小提醒\n",
    "* 原文中的lambda(控制Domain Adversarial Loss的係數)是有Adaptive的版本，如果有興趣可以參考[原文](https://arxiv.org/pdf/1505.07818.pdf)。\n",
    "* 因為我們完全沒有target的label，所以結果如何，只好丟kaggle看看囉:)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "6sLsxT9_pp42"
   },
   "outputs": [],
   "source": [
    "def adaptive_lambda(epoch, num_epoch):\n",
    "    p = epoch / num_epoch\n",
    "    return 2. / (1+np.exp(-10*p)) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "lRAFFKvX0p9y"
   },
   "outputs": [],
   "source": [
    "def train_epoch(source_dataloader, target_dataloader, lamb):\n",
    "    '''\n",
    "      Args:\n",
    "        source_dataloader: source data的dataloader\n",
    "        target_dataloader: target data的dataloader\n",
    "        lamb: control the balance of domain adaptatoin and classification.\n",
    "    '''\n",
    "\n",
    "    # D loss: Domain Classifier的loss\n",
    "    # F loss: Feature Extrator & Label Predictor的loss\n",
    "    running_D_loss, running_F_loss = 0.0, 0.0\n",
    "    total_hit, total_num = 0.0, 0.0\n",
    "\n",
    "    for i, ((source_data, source_label), (target_data, _)) in enumerate(zip(source_dataloader, target_dataloader)):\n",
    "\n",
    "        source_data = source_data.cuda()\n",
    "        source_label = source_label.cuda()\n",
    "        target_data = target_data.cuda()\n",
    "        \n",
    "        # Mixed the source data and target data, or it'll mislead the running params\n",
    "        #   of batch_norm. (runnning mean/var of soucre and target data are different.)\n",
    "        mixed_data = torch.cat([source_data, target_data], dim=0)\n",
    "        domain_label = torch.zeros([source_data.shape[0] + target_data.shape[0], 1]).cuda()\n",
    "        # set domain label of source data to be 1.\n",
    "        domain_label[:source_data.shape[0]] = 1\n",
    "\n",
    "        # Step 1 : train domain classifier\n",
    "        feature = feature_extractor(mixed_data)\n",
    "        # We don't need to train feature extractor in step 1.\n",
    "        # Thus we detach the feature neuron to avoid backpropgation.\n",
    "        domain_logits = domain_classifier(feature.detach())\n",
    "        loss = domain_criterion(domain_logits, domain_label)\n",
    "        running_D_loss+= loss.item()\n",
    "        loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # Step 2 : train feature extractor and label classifier\n",
    "        class_logits = label_predictor(feature[:source_data.shape[0]])\n",
    "        domain_logits = domain_classifier(feature)\n",
    "        # loss = cross entropy of classification - lamb * domain binary cross entropy.\n",
    "        #  The reason why using subtraction is similar to generator loss in disciminator of GAN\n",
    "        loss = class_criterion(class_logits, source_label) - lamb * domain_criterion(domain_logits, domain_label)\n",
    "        running_F_loss+= loss.item()\n",
    "        loss.backward()\n",
    "        optimizer_F.step()\n",
    "        optimizer_C.step()\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "        optimizer_F.zero_grad()\n",
    "        optimizer_C.zero_grad()\n",
    "\n",
    "        total_hit += torch.sum(torch.argmax(class_logits, dim=1) == source_label).item()\n",
    "        total_num += source_data.shape[0]\n",
    "        print(i, end='\\r')\n",
    "\n",
    "    return running_D_loss / (i+1), running_F_loss / (i+1), total_hit / total_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gQH46Ayyp0g3",
    "outputId": "3021dfa8-6056-4ecc-c2f8-4ad6b696cbd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   0: train D loss: 0.1890, train F loss: 1.9491, acc 0.2720\n",
      "epoch   1: train D loss: 0.1816, train F loss: 1.6009, acc 0.4326\n",
      "epoch   2: train D loss: 0.1775, train F loss: 1.4754, acc 0.4912\n",
      "epoch   3: train D loss: 0.2084, train F loss: 1.4040, acc 0.5114\n",
      "epoch   4: train D loss: 0.2344, train F loss: 1.3337, acc 0.5354\n",
      "epoch   5: train D loss: 0.2528, train F loss: 1.3055, acc 0.5432\n",
      "epoch   6: train D loss: 0.2280, train F loss: 1.2620, acc 0.5594\n",
      "epoch   7: train D loss: 0.2661, train F loss: 1.2133, acc 0.5818\n",
      "epoch   8: train D loss: 0.2598, train F loss: 1.1775, acc 0.5880\n",
      "epoch   9: train D loss: 0.2444, train F loss: 1.1357, acc 0.6040\n",
      "epoch  10: train D loss: 0.2680, train F loss: 1.0953, acc 0.6160\n",
      "epoch  11: train D loss: 0.2611, train F loss: 1.0782, acc 0.6208\n",
      "epoch  12: train D loss: 0.2719, train F loss: 1.0522, acc 0.6306\n",
      "epoch  13: train D loss: 0.2690, train F loss: 0.9969, acc 0.6494\n",
      "epoch  14: train D loss: 0.2704, train F loss: 0.9730, acc 0.6624\n",
      "epoch  15: train D loss: 0.2874, train F loss: 0.9298, acc 0.6750\n",
      "epoch  16: train D loss: 0.2642, train F loss: 0.8906, acc 0.6868\n",
      "epoch  17: train D loss: 0.2961, train F loss: 0.8573, acc 0.6958\n",
      "epoch  18: train D loss: 0.2930, train F loss: 0.8391, acc 0.7028\n",
      "epoch  19: train D loss: 0.2824, train F loss: 0.8101, acc 0.7192\n",
      "epoch  20: train D loss: 0.2810, train F loss: 0.7764, acc 0.7254\n",
      "epoch  21: train D loss: 0.2805, train F loss: 0.7314, acc 0.7434\n",
      "epoch  22: train D loss: 0.2929, train F loss: 0.7043, acc 0.7554\n",
      "epoch  23: train D loss: 0.2944, train F loss: 0.6631, acc 0.7584\n",
      "epoch  24: train D loss: 0.2983, train F loss: 0.6160, acc 0.7804\n",
      "epoch  25: train D loss: 0.3000, train F loss: 0.5906, acc 0.7874\n",
      "epoch  26: train D loss: 0.2862, train F loss: 0.5787, acc 0.7972\n",
      "epoch  27: train D loss: 0.2948, train F loss: 0.5372, acc 0.8060\n",
      "epoch  28: train D loss: 0.2878, train F loss: 0.5211, acc 0.8158\n",
      "epoch  29: train D loss: 0.3093, train F loss: 0.4931, acc 0.8218\n",
      "epoch  30: train D loss: 0.3073, train F loss: 0.4618, acc 0.8310\n",
      "epoch  31: train D loss: 0.2986, train F loss: 0.4477, acc 0.8370\n",
      "epoch  32: train D loss: 0.3051, train F loss: 0.4034, acc 0.8548\n",
      "epoch  33: train D loss: 0.3132, train F loss: 0.3878, acc 0.8614\n",
      "epoch  34: train D loss: 0.2903, train F loss: 0.3833, acc 0.8606\n",
      "epoch  35: train D loss: 0.2987, train F loss: 0.3612, acc 0.8690\n",
      "epoch  36: train D loss: 0.3040, train F loss: 0.3390, acc 0.8760\n",
      "epoch  37: train D loss: 0.2959, train F loss: 0.3235, acc 0.8862\n",
      "epoch  38: train D loss: 0.3095, train F loss: 0.2908, acc 0.8916\n",
      "epoch  39: train D loss: 0.3112, train F loss: 0.2893, acc 0.8926\n",
      "epoch  40: train D loss: 0.3150, train F loss: 0.2673, acc 0.9022\n",
      "epoch  41: train D loss: 0.3208, train F loss: 0.2567, acc 0.9038\n",
      "epoch  42: train D loss: 0.3406, train F loss: 0.2537, acc 0.9048\n",
      "epoch  43: train D loss: 0.3400, train F loss: 0.2246, acc 0.9122\n",
      "epoch  44: train D loss: 0.3476, train F loss: 0.2310, acc 0.9068\n",
      "epoch  45: train D loss: 0.3411, train F loss: 0.2085, acc 0.9152\n",
      "epoch  46: train D loss: 0.3550, train F loss: 0.2008, acc 0.9188\n",
      "epoch  47: train D loss: 0.3499, train F loss: 0.2082, acc 0.9166\n",
      "epoch  48: train D loss: 0.3419, train F loss: 0.2082, acc 0.9202\n",
      "epoch  49: train D loss: 0.3575, train F loss: 0.1851, acc 0.9242\n",
      "epoch  50: train D loss: 0.3589, train F loss: 0.1626, acc 0.9346\n",
      "epoch  51: train D loss: 0.3623, train F loss: 0.1389, acc 0.9412\n",
      "epoch  52: train D loss: 0.3616, train F loss: 0.1407, acc 0.9382\n",
      "epoch  53: train D loss: 0.3729, train F loss: 0.1622, acc 0.9370\n",
      "epoch  54: train D loss: 0.3613, train F loss: 0.1629, acc 0.9300\n",
      "epoch  55: train D loss: 0.3847, train F loss: 0.1264, acc 0.9406\n",
      "epoch  56: train D loss: 0.3882, train F loss: 0.1540, acc 0.9338\n",
      "epoch  57: train D loss: 0.3788, train F loss: 0.1392, acc 0.9350\n",
      "epoch  58: train D loss: 0.4019, train F loss: 0.1222, acc 0.9430\n",
      "epoch  59: train D loss: 0.3994, train F loss: 0.1084, acc 0.9466\n",
      "epoch  60: train D loss: 0.3949, train F loss: 0.1134, acc 0.9420\n",
      "epoch  61: train D loss: 0.4099, train F loss: 0.0999, acc 0.9438\n",
      "epoch  62: train D loss: 0.4222, train F loss: 0.0947, acc 0.9460\n",
      "epoch  63: train D loss: 0.4172, train F loss: 0.1140, acc 0.9416\n",
      "epoch  64: train D loss: 0.4204, train F loss: 0.1012, acc 0.9438\n",
      "epoch  65: train D loss: 0.4164, train F loss: 0.0701, acc 0.9568\n",
      "epoch  66: train D loss: 0.4356, train F loss: 0.0949, acc 0.9458\n",
      "epoch  67: train D loss: 0.4287, train F loss: 0.0869, acc 0.9492\n",
      "epoch  68: train D loss: 0.4515, train F loss: 0.0633, acc 0.9562\n",
      "epoch  69: train D loss: 0.4435, train F loss: 0.0936, acc 0.9440\n",
      "epoch  70: train D loss: 0.4585, train F loss: 0.0605, acc 0.9568\n",
      "epoch  71: train D loss: 0.4497, train F loss: 0.0733, acc 0.9528\n",
      "epoch  72: train D loss: 0.4390, train F loss: 0.0878, acc 0.9420\n",
      "epoch  73: train D loss: 0.4437, train F loss: 0.0493, acc 0.9612\n",
      "epoch  74: train D loss: 0.4577, train F loss: 0.0728, acc 0.9512\n",
      "epoch  75: train D loss: 0.4426, train F loss: 0.0504, acc 0.9556\n",
      "epoch  76: train D loss: 0.4609, train F loss: 0.0604, acc 0.9524\n",
      "epoch  77: train D loss: 0.4860, train F loss: 0.0243, acc 0.9622\n",
      "epoch  78: train D loss: 0.4649, train F loss: 0.0325, acc 0.9606\n",
      "epoch  79: train D loss: 0.4615, train F loss: 0.0422, acc 0.9630\n",
      "epoch  80: train D loss: 0.4493, train F loss: 0.0274, acc 0.9608\n",
      "epoch  81: train D loss: 0.4791, train F loss: 0.0363, acc 0.9604\n",
      "epoch  82: train D loss: 0.4794, train F loss: 0.0402, acc 0.9566\n",
      "epoch  83: train D loss: 0.4861, train F loss: 0.0112, acc 0.9630\n",
      "epoch  84: train D loss: 0.4859, train F loss: 0.0273, acc 0.9584\n",
      "epoch  85: train D loss: 0.4810, train F loss: 0.0225, acc 0.9600\n",
      "epoch  86: train D loss: 0.4739, train F loss: 0.0157, acc 0.9632\n",
      "epoch  87: train D loss: 0.4955, train F loss: 0.0191, acc 0.9630\n",
      "epoch  88: train D loss: 0.4878, train F loss: 0.0141, acc 0.9632\n",
      "epoch  89: train D loss: 0.5005, train F loss: 0.0231, acc 0.9566\n",
      "epoch  90: train D loss: 0.5018, train F loss: 0.0012, acc 0.9660\n",
      "epoch  91: train D loss: 0.4864, train F loss: -0.0003, acc 0.9658\n",
      "epoch  92: train D loss: 0.4991, train F loss: 0.0079, acc 0.9634\n",
      "epoch  93: train D loss: 0.4995, train F loss: 0.0012, acc 0.9642\n",
      "epoch  94: train D loss: 0.4931, train F loss: 0.0137, acc 0.9612\n",
      "epoch  95: train D loss: 0.5004, train F loss: 0.0130, acc 0.9594\n",
      "epoch  96: train D loss: 0.4970, train F loss: 0.0092, acc 0.9598\n",
      "epoch  97: train D loss: 0.5192, train F loss: -0.0015, acc 0.9634\n",
      "epoch  98: train D loss: 0.5051, train F loss: -0.0136, acc 0.9664\n",
      "epoch  99: train D loss: 0.5125, train F loss: -0.0106, acc 0.9654\n",
      "epoch 100: train D loss: 0.5075, train F loss: -0.0157, acc 0.9688\n",
      "epoch 101: train D loss: 0.5130, train F loss: -0.0051, acc 0.9618\n",
      "epoch 102: train D loss: 0.5142, train F loss: -0.0224, acc 0.9698\n",
      "epoch 103: train D loss: 0.5088, train F loss: -0.0188, acc 0.9672\n",
      "epoch 104: train D loss: 0.5117, train F loss: -0.0122, acc 0.9646\n",
      "epoch 105: train D loss: 0.5378, train F loss: -0.0297, acc 0.9690\n",
      "epoch 106: train D loss: 0.5152, train F loss: -0.0152, acc 0.9622\n",
      "epoch 107: train D loss: 0.5146, train F loss: -0.0314, acc 0.9696\n",
      "epoch 108: train D loss: 0.5270, train F loss: -0.0209, acc 0.9650\n",
      "epoch 109: train D loss: 0.5212, train F loss: -0.0283, acc 0.9654\n",
      "epoch 110: train D loss: 0.5321, train F loss: -0.0357, acc 0.9674\n",
      "epoch 111: train D loss: 0.5258, train F loss: -0.0338, acc 0.9666\n",
      "epoch 112: train D loss: 0.5345, train F loss: -0.0443, acc 0.9678\n",
      "epoch 113: train D loss: 0.5318, train F loss: -0.0485, acc 0.9712\n",
      "epoch 114: train D loss: 0.5408, train F loss: -0.0442, acc 0.9684\n",
      "epoch 115: train D loss: 0.5260, train F loss: -0.0467, acc 0.9688\n",
      "epoch 116: train D loss: 0.5363, train F loss: -0.0450, acc 0.9656\n",
      "epoch 117: train D loss: 0.5387, train F loss: -0.0625, acc 0.9714\n",
      "epoch 118: train D loss: 0.5443, train F loss: -0.0337, acc 0.9600\n",
      "epoch 119: train D loss: 0.5471, train F loss: -0.0544, acc 0.9676\n",
      "epoch 120: train D loss: 0.5361, train F loss: -0.0549, acc 0.9686\n",
      "epoch 121: train D loss: 0.5414, train F loss: -0.0726, acc 0.9750\n",
      "epoch 122: train D loss: 0.5319, train F loss: -0.0735, acc 0.9742\n",
      "epoch 123: train D loss: 0.5467, train F loss: -0.0577, acc 0.9686\n",
      "epoch 124: train D loss: 0.5551, train F loss: -0.0790, acc 0.9732\n",
      "epoch 125: train D loss: 0.5441, train F loss: -0.0771, acc 0.9736\n",
      "epoch 126: train D loss: 0.5617, train F loss: -0.0710, acc 0.9714\n",
      "epoch 127: train D loss: 0.5450, train F loss: -0.0778, acc 0.9738\n",
      "epoch 128: train D loss: 0.5555, train F loss: -0.0826, acc 0.9722\n",
      "epoch 129: train D loss: 0.5606, train F loss: -0.0572, acc 0.9638\n",
      "epoch 130: train D loss: 0.5484, train F loss: -0.0915, acc 0.9762\n",
      "epoch 131: train D loss: 0.5537, train F loss: -0.0850, acc 0.9704\n",
      "epoch 132: train D loss: 0.5597, train F loss: -0.0801, acc 0.9738\n",
      "epoch 133: train D loss: 0.5564, train F loss: -0.0875, acc 0.9766\n",
      "epoch 134: train D loss: 0.5544, train F loss: -0.0646, acc 0.9664\n",
      "epoch 135: train D loss: 0.5489, train F loss: -0.0946, acc 0.9752\n",
      "epoch 136: train D loss: 0.5576, train F loss: -0.0972, acc 0.9752\n",
      "epoch 137: train D loss: 0.5590, train F loss: -0.0872, acc 0.9678\n",
      "epoch 138: train D loss: 0.5647, train F loss: -0.1053, acc 0.9754\n",
      "epoch 139: train D loss: 0.5731, train F loss: -0.0862, acc 0.9702\n",
      "epoch 140: train D loss: 0.5539, train F loss: -0.0972, acc 0.9728\n",
      "epoch 141: train D loss: 0.5644, train F loss: -0.0954, acc 0.9730\n",
      "epoch 142: train D loss: 0.5721, train F loss: -0.0858, acc 0.9692\n",
      "epoch 143: train D loss: 0.5692, train F loss: -0.1067, acc 0.9720\n",
      "epoch 144: train D loss: 0.5637, train F loss: -0.1139, acc 0.9778\n",
      "epoch 145: train D loss: 0.5655, train F loss: -0.1202, acc 0.9764\n",
      "epoch 146: train D loss: 0.5741, train F loss: -0.1218, acc 0.9724\n",
      "epoch 147: train D loss: 0.5788, train F loss: -0.1232, acc 0.9768\n",
      "epoch 148: train D loss: 0.5855, train F loss: -0.0945, acc 0.9692\n",
      "epoch 149: train D loss: 0.5631, train F loss: -0.0970, acc 0.9720\n",
      "epoch 150: train D loss: 0.5618, train F loss: -0.1251, acc 0.9754\n",
      "epoch 151: train D loss: 0.5738, train F loss: -0.1328, acc 0.9784\n",
      "epoch 152: train D loss: 0.5794, train F loss: -0.1135, acc 0.9704\n",
      "epoch 153: train D loss: 0.5744, train F loss: -0.1254, acc 0.9744\n",
      "epoch 154: train D loss: 0.5732, train F loss: -0.1204, acc 0.9760\n",
      "epoch 155: train D loss: 0.5860, train F loss: -0.1239, acc 0.9706\n",
      "epoch 156: train D loss: 0.5823, train F loss: -0.1463, acc 0.9790\n",
      "epoch 157: train D loss: 0.5828, train F loss: -0.1302, acc 0.9724\n",
      "epoch 158: train D loss: 0.5741, train F loss: -0.1352, acc 0.9754\n",
      "epoch 159: train D loss: 0.5943, train F loss: -0.1299, acc 0.9720\n",
      "epoch 160: train D loss: 0.5850, train F loss: -0.1334, acc 0.9750\n",
      "epoch 161: train D loss: 0.5807, train F loss: -0.1429, acc 0.9768\n",
      "epoch 162: train D loss: 0.5867, train F loss: -0.1247, acc 0.9714\n",
      "epoch 163: train D loss: 0.5821, train F loss: -0.1473, acc 0.9768\n",
      "epoch 164: train D loss: 0.5935, train F loss: -0.1495, acc 0.9758\n",
      "epoch 165: train D loss: 0.5801, train F loss: -0.1219, acc 0.9702\n",
      "epoch 166: train D loss: 0.5916, train F loss: -0.1547, acc 0.9772\n",
      "epoch 167: train D loss: 0.5986, train F loss: -0.1556, acc 0.9770\n",
      "epoch 168: train D loss: 0.5896, train F loss: -0.1390, acc 0.9716\n",
      "epoch 169: train D loss: 0.5970, train F loss: -0.1417, acc 0.9722\n",
      "epoch 170: train D loss: 0.5908, train F loss: -0.1384, acc 0.9722\n",
      "epoch 171: train D loss: 0.5957, train F loss: -0.1550, acc 0.9732\n",
      "epoch 172: train D loss: 0.5910, train F loss: -0.1618, acc 0.9788\n",
      "epoch 173: train D loss: 0.5840, train F loss: -0.1637, acc 0.9796\n",
      "epoch 174: train D loss: 0.5866, train F loss: -0.1436, acc 0.9738\n",
      "epoch 175: train D loss: 0.5967, train F loss: -0.1740, acc 0.9800\n",
      "epoch 176: train D loss: 0.6062, train F loss: -0.1614, acc 0.9738\n",
      "epoch 177: train D loss: 0.6056, train F loss: -0.1685, acc 0.9750\n",
      "epoch 178: train D loss: 0.5979, train F loss: -0.1748, acc 0.9788\n",
      "epoch 179: train D loss: 0.6161, train F loss: -0.1765, acc 0.9756\n",
      "epoch 180: train D loss: 0.6052, train F loss: -0.1729, acc 0.9764\n",
      "epoch 181: train D loss: 0.6037, train F loss: -0.1659, acc 0.9754\n",
      "epoch 182: train D loss: 0.6039, train F loss: -0.1767, acc 0.9796\n",
      "epoch 183: train D loss: 0.5862, train F loss: -0.1690, acc 0.9772\n",
      "epoch 184: train D loss: 0.6031, train F loss: -0.1754, acc 0.9748\n",
      "epoch 185: train D loss: 0.5888, train F loss: -0.1609, acc 0.9722\n",
      "epoch 186: train D loss: 0.6003, train F loss: -0.1716, acc 0.9746\n",
      "epoch 187: train D loss: 0.6064, train F loss: -0.1849, acc 0.9784\n",
      "epoch 188: train D loss: 0.5989, train F loss: -0.1883, acc 0.9798\n",
      "epoch 189: train D loss: 0.6123, train F loss: -0.2050, acc 0.9812\n",
      "epoch 190: train D loss: 0.6062, train F loss: -0.1936, acc 0.9788\n",
      "epoch 191: train D loss: 0.6183, train F loss: -0.1920, acc 0.9734\n",
      "epoch 192: train D loss: 0.6000, train F loss: -0.1739, acc 0.9756\n",
      "epoch 193: train D loss: 0.6010, train F loss: -0.1906, acc 0.9780\n",
      "epoch 194: train D loss: 0.5984, train F loss: -0.1926, acc 0.9812\n",
      "epoch 195: train D loss: 0.6056, train F loss: -0.2063, acc 0.9814\n",
      "epoch 196: train D loss: 0.6097, train F loss: -0.1854, acc 0.9726\n",
      "epoch 197: train D loss: 0.5981, train F loss: -0.1948, acc 0.9784\n",
      "epoch 198: train D loss: 0.6042, train F loss: -0.2059, acc 0.9784\n",
      "epoch 199: train D loss: 0.6084, train F loss: -0.1942, acc 0.9768\n",
      "epoch 200: train D loss: 0.6096, train F loss: -0.2210, acc 0.9828\n",
      "epoch 201: train D loss: 0.6135, train F loss: -0.2119, acc 0.9814\n",
      "epoch 202: train D loss: 0.6071, train F loss: -0.2079, acc 0.9782\n",
      "epoch 203: train D loss: 0.6086, train F loss: -0.2069, acc 0.9790\n",
      "epoch 204: train D loss: 0.6070, train F loss: -0.2129, acc 0.9794\n",
      "epoch 205: train D loss: 0.6186, train F loss: -0.2063, acc 0.9708\n",
      "epoch 206: train D loss: 0.6140, train F loss: -0.2132, acc 0.9780\n",
      "epoch 207: train D loss: 0.6136, train F loss: -0.1993, acc 0.9760\n",
      "epoch 208: train D loss: 0.6095, train F loss: -0.2224, acc 0.9792\n",
      "epoch 209: train D loss: 0.6184, train F loss: -0.2361, acc 0.9816\n",
      "epoch 210: train D loss: 0.6106, train F loss: -0.2058, acc 0.9742\n",
      "epoch 211: train D loss: 0.6059, train F loss: -0.2175, acc 0.9794\n",
      "epoch 212: train D loss: 0.6261, train F loss: -0.2349, acc 0.9792\n",
      "epoch 213: train D loss: 0.6209, train F loss: -0.2288, acc 0.9810\n",
      "epoch 214: train D loss: 0.6288, train F loss: -0.2455, acc 0.9818\n",
      "epoch 215: train D loss: 0.6196, train F loss: -0.2298, acc 0.9792\n",
      "epoch 216: train D loss: 0.6131, train F loss: -0.2272, acc 0.9778\n",
      "epoch 217: train D loss: 0.6045, train F loss: -0.2288, acc 0.9812\n",
      "epoch 218: train D loss: 0.6260, train F loss: -0.2363, acc 0.9786\n",
      "epoch 219: train D loss: 0.6208, train F loss: -0.2439, acc 0.9826\n",
      "epoch 220: train D loss: 0.6183, train F loss: -0.2308, acc 0.9746\n",
      "epoch 221: train D loss: 0.6139, train F loss: -0.2321, acc 0.9784\n",
      "epoch 222: train D loss: 0.6251, train F loss: -0.2475, acc 0.9806\n",
      "epoch 223: train D loss: 0.6241, train F loss: -0.2434, acc 0.9774\n",
      "epoch 224: train D loss: 0.6148, train F loss: -0.2311, acc 0.9764\n",
      "epoch 225: train D loss: 0.6293, train F loss: -0.2424, acc 0.9754\n",
      "epoch 226: train D loss: 0.6269, train F loss: -0.2571, acc 0.9812\n",
      "epoch 227: train D loss: 0.6218, train F loss: -0.2515, acc 0.9842\n",
      "epoch 228: train D loss: 0.6249, train F loss: -0.2459, acc 0.9796\n",
      "epoch 229: train D loss: 0.6280, train F loss: -0.2493, acc 0.9772\n",
      "epoch 230: train D loss: 0.6135, train F loss: -0.2314, acc 0.9764\n",
      "epoch 231: train D loss: 0.6160, train F loss: -0.2499, acc 0.9808\n",
      "epoch 232: train D loss: 0.6286, train F loss: -0.2581, acc 0.9792\n",
      "epoch 233: train D loss: 0.6293, train F loss: -0.2597, acc 0.9792\n",
      "epoch 234: train D loss: 0.6161, train F loss: -0.2569, acc 0.9824\n",
      "epoch 235: train D loss: 0.6306, train F loss: -0.2582, acc 0.9800\n",
      "epoch 236: train D loss: 0.6229, train F loss: -0.2647, acc 0.9818\n",
      "epoch 237: train D loss: 0.6267, train F loss: -0.2516, acc 0.9790\n",
      "epoch 238: train D loss: 0.6248, train F loss: -0.2699, acc 0.9810\n",
      "epoch 239: train D loss: 0.6234, train F loss: -0.2535, acc 0.9786\n",
      "epoch 240: train D loss: 0.6315, train F loss: -0.2780, acc 0.9814\n",
      "epoch 241: train D loss: 0.6222, train F loss: -0.2722, acc 0.9808\n",
      "epoch 242: train D loss: 0.6205, train F loss: -0.2606, acc 0.9804\n",
      "epoch 243: train D loss: 0.6206, train F loss: -0.2661, acc 0.9812\n",
      "epoch 244: train D loss: 0.6235, train F loss: -0.2726, acc 0.9798\n",
      "epoch 245: train D loss: 0.6276, train F loss: -0.2521, acc 0.9758\n",
      "epoch 246: train D loss: 0.6235, train F loss: -0.2624, acc 0.9768\n",
      "epoch 247: train D loss: 0.6155, train F loss: -0.2723, acc 0.9840\n",
      "epoch 248: train D loss: 0.6239, train F loss: -0.2672, acc 0.9796\n",
      "epoch 249: train D loss: 0.6206, train F loss: -0.2727, acc 0.9790\n",
      "epoch 250: train D loss: 0.6284, train F loss: -0.2838, acc 0.9794\n",
      "epoch 251: train D loss: 0.6326, train F loss: -0.2691, acc 0.9776\n",
      "epoch 252: train D loss: 0.6310, train F loss: -0.2741, acc 0.9804\n",
      "epoch 253: train D loss: 0.6306, train F loss: -0.2727, acc 0.9788\n",
      "epoch 254: train D loss: 0.6322, train F loss: -0.3001, acc 0.9846\n",
      "epoch 255: train D loss: 0.6381, train F loss: -0.2934, acc 0.9836\n",
      "epoch 256: train D loss: 0.6360, train F loss: -0.2948, acc 0.9832\n",
      "epoch 257: train D loss: 0.6271, train F loss: -0.2907, acc 0.9822\n",
      "epoch 258: train D loss: 0.6365, train F loss: -0.3000, acc 0.9824\n",
      "epoch 259: train D loss: 0.6318, train F loss: -0.2784, acc 0.9784\n",
      "epoch 260: train D loss: 0.6376, train F loss: -0.3017, acc 0.9822\n",
      "epoch 261: train D loss: 0.6286, train F loss: -0.2772, acc 0.9784\n",
      "epoch 262: train D loss: 0.6396, train F loss: -0.2997, acc 0.9806\n",
      "epoch 263: train D loss: 0.6283, train F loss: -0.2987, acc 0.9830\n",
      "epoch 264: train D loss: 0.6392, train F loss: -0.3062, acc 0.9826\n",
      "epoch 265: train D loss: 0.6399, train F loss: -0.3043, acc 0.9830\n",
      "epoch 266: train D loss: 0.6433, train F loss: -0.3128, acc 0.9800\n",
      "epoch 267: train D loss: 0.6304, train F loss: -0.2999, acc 0.9814\n",
      "epoch 268: train D loss: 0.6405, train F loss: -0.3079, acc 0.9808\n",
      "epoch 269: train D loss: 0.6424, train F loss: -0.3074, acc 0.9794\n",
      "epoch 270: train D loss: 0.6408, train F loss: -0.3024, acc 0.9784\n",
      "epoch 271: train D loss: 0.6372, train F loss: -0.3027, acc 0.9806\n",
      "epoch 272: train D loss: 0.6337, train F loss: -0.2984, acc 0.9796\n",
      "epoch 273: train D loss: 0.6427, train F loss: -0.3171, acc 0.9824\n",
      "epoch 274: train D loss: 0.6353, train F loss: -0.3025, acc 0.9788\n",
      "epoch 275: train D loss: 0.6345, train F loss: -0.3138, acc 0.9830\n",
      "epoch 276: train D loss: 0.6322, train F loss: -0.3160, acc 0.9828\n",
      "epoch 277: train D loss: 0.6352, train F loss: -0.3063, acc 0.9808\n",
      "epoch 278: train D loss: 0.6369, train F loss: -0.3099, acc 0.9802\n",
      "epoch 279: train D loss: 0.6398, train F loss: -0.3158, acc 0.9796\n",
      "epoch 280: train D loss: 0.6407, train F loss: -0.3093, acc 0.9778\n",
      "epoch 281: train D loss: 0.6338, train F loss: -0.3182, acc 0.9834\n",
      "epoch 282: train D loss: 0.6362, train F loss: -0.3285, acc 0.9838\n",
      "epoch 283: train D loss: 0.6431, train F loss: -0.3275, acc 0.9818\n",
      "epoch 284: train D loss: 0.6317, train F loss: -0.3305, acc 0.9850\n",
      "epoch 285: train D loss: 0.6366, train F loss: -0.3350, acc 0.9846\n",
      "epoch 286: train D loss: 0.6477, train F loss: -0.3299, acc 0.9792\n",
      "epoch 287: train D loss: 0.6351, train F loss: -0.3270, acc 0.9812\n",
      "epoch 288: train D loss: 0.6465, train F loss: -0.3293, acc 0.9810\n",
      "epoch 289: train D loss: 0.6444, train F loss: -0.3158, acc 0.9760\n",
      "epoch 290: train D loss: 0.6400, train F loss: -0.3478, acc 0.9870\n",
      "epoch 291: train D loss: 0.6394, train F loss: -0.3337, acc 0.9836\n",
      "epoch 292: train D loss: 0.6368, train F loss: -0.3351, acc 0.9836\n",
      "epoch 293: train D loss: 0.6387, train F loss: -0.3215, acc 0.9784\n",
      "epoch 294: train D loss: 0.6433, train F loss: -0.3397, acc 0.9822\n",
      "epoch 295: train D loss: 0.6456, train F loss: -0.3362, acc 0.9804\n",
      "epoch 296: train D loss: 0.6402, train F loss: -0.3451, acc 0.9836\n",
      "epoch 297: train D loss: 0.6381, train F loss: -0.3279, acc 0.9798\n",
      "epoch 298: train D loss: 0.6397, train F loss: -0.3222, acc 0.9806\n",
      "epoch 299: train D loss: 0.6394, train F loss: -0.3267, acc 0.9770\n",
      "epoch 300: train D loss: 0.6337, train F loss: -0.3452, acc 0.9842\n",
      "epoch 301: train D loss: 0.6410, train F loss: -0.3476, acc 0.9850\n",
      "epoch 302: train D loss: 0.6449, train F loss: -0.3511, acc 0.9810\n",
      "epoch 303: train D loss: 0.6484, train F loss: -0.3526, acc 0.9826\n",
      "epoch 304: train D loss: 0.6478, train F loss: -0.3577, acc 0.9846\n",
      "epoch 305: train D loss: 0.6470, train F loss: -0.3557, acc 0.9826\n",
      "epoch 306: train D loss: 0.6391, train F loss: -0.3532, acc 0.9846\n",
      "epoch 307: train D loss: 0.6488, train F loss: -0.3633, acc 0.9860\n",
      "epoch 308: train D loss: 0.6468, train F loss: -0.3584, acc 0.9850\n",
      "epoch 309: train D loss: 0.6452, train F loss: -0.3652, acc 0.9850\n",
      "epoch 310: train D loss: 0.6429, train F loss: -0.3356, acc 0.9768\n",
      "epoch 311: train D loss: 0.6447, train F loss: -0.3622, acc 0.9844\n",
      "epoch 312: train D loss: 0.6420, train F loss: -0.3450, acc 0.9794\n",
      "epoch 313: train D loss: 0.6502, train F loss: -0.3652, acc 0.9836\n",
      "epoch 314: train D loss: 0.6468, train F loss: -0.3682, acc 0.9862\n",
      "epoch 315: train D loss: 0.6480, train F loss: -0.3474, acc 0.9792\n",
      "epoch 316: train D loss: 0.6517, train F loss: -0.3639, acc 0.9822\n",
      "epoch 317: train D loss: 0.6391, train F loss: -0.3599, acc 0.9828\n",
      "epoch 318: train D loss: 0.6472, train F loss: -0.3694, acc 0.9858\n",
      "epoch 319: train D loss: 0.6499, train F loss: -0.3688, acc 0.9828\n",
      "epoch 320: train D loss: 0.6424, train F loss: -0.3608, acc 0.9826\n",
      "epoch 321: train D loss: 0.6459, train F loss: -0.3656, acc 0.9840\n",
      "epoch 322: train D loss: 0.6430, train F loss: -0.3636, acc 0.9832\n",
      "epoch 323: train D loss: 0.6492, train F loss: -0.3661, acc 0.9824\n",
      "epoch 324: train D loss: 0.6470, train F loss: -0.3748, acc 0.9832\n",
      "epoch 325: train D loss: 0.6518, train F loss: -0.3786, acc 0.9834\n",
      "epoch 326: train D loss: 0.6503, train F loss: -0.3751, acc 0.9842\n",
      "epoch 327: train D loss: 0.6480, train F loss: -0.3763, acc 0.9824\n",
      "epoch 328: train D loss: 0.6504, train F loss: -0.3748, acc 0.9832\n",
      "epoch 329: train D loss: 0.6507, train F loss: -0.3766, acc 0.9834\n",
      "epoch 330: train D loss: 0.6521, train F loss: -0.3792, acc 0.9834\n",
      "epoch 331: train D loss: 0.6455, train F loss: -0.3756, acc 0.9844\n",
      "epoch 332: train D loss: 0.6472, train F loss: -0.3781, acc 0.9832\n",
      "epoch 333: train D loss: 0.6479, train F loss: -0.3856, acc 0.9838\n",
      "epoch 334: train D loss: 0.6466, train F loss: -0.3811, acc 0.9846\n",
      "epoch 335: train D loss: 0.6489, train F loss: -0.3785, acc 0.9800\n",
      "epoch 336: train D loss: 0.6518, train F loss: -0.3908, acc 0.9834\n",
      "epoch 337: train D loss: 0.6441, train F loss: -0.3854, acc 0.9838\n",
      "epoch 338: train D loss: 0.6488, train F loss: -0.3914, acc 0.9850\n",
      "epoch 339: train D loss: 0.6459, train F loss: -0.3919, acc 0.9844\n",
      "epoch 340: train D loss: 0.6437, train F loss: -0.3805, acc 0.9828\n",
      "epoch 341: train D loss: 0.6488, train F loss: -0.3782, acc 0.9818\n",
      "epoch 342: train D loss: 0.6446, train F loss: -0.3867, acc 0.9834\n",
      "epoch 343: train D loss: 0.6509, train F loss: -0.3833, acc 0.9816\n",
      "epoch 344: train D loss: 0.6433, train F loss: -0.3842, acc 0.9818\n",
      "epoch 345: train D loss: 0.6469, train F loss: -0.3864, acc 0.9834\n",
      "epoch 346: train D loss: 0.6496, train F loss: -0.3934, acc 0.9836\n",
      "epoch 347: train D loss: 0.6483, train F loss: -0.3991, acc 0.9858\n",
      "epoch 348: train D loss: 0.6487, train F loss: -0.3785, acc 0.9804\n",
      "epoch 349: train D loss: 0.6518, train F loss: -0.3990, acc 0.9846\n",
      "epoch 350: train D loss: 0.6563, train F loss: -0.4111, acc 0.9856\n",
      "epoch 351: train D loss: 0.6426, train F loss: -0.3918, acc 0.9836\n",
      "epoch 352: train D loss: 0.6516, train F loss: -0.4009, acc 0.9842\n",
      "epoch 353: train D loss: 0.6509, train F loss: -0.4064, acc 0.9854\n",
      "epoch 354: train D loss: 0.6492, train F loss: -0.3830, acc 0.9772\n",
      "epoch 355: train D loss: 0.6531, train F loss: -0.4016, acc 0.9844\n",
      "epoch 356: train D loss: 0.6427, train F loss: -0.4053, acc 0.9852\n",
      "epoch 357: train D loss: 0.6496, train F loss: -0.3935, acc 0.9812\n",
      "epoch 358: train D loss: 0.6499, train F loss: -0.3950, acc 0.9818\n",
      "epoch 359: train D loss: 0.6511, train F loss: -0.4102, acc 0.9834\n",
      "epoch 360: train D loss: 0.6521, train F loss: -0.4074, acc 0.9842\n",
      "epoch 361: train D loss: 0.6465, train F loss: -0.4090, acc 0.9860\n",
      "epoch 362: train D loss: 0.6598, train F loss: -0.4104, acc 0.9832\n",
      "epoch 363: train D loss: 0.6470, train F loss: -0.4030, acc 0.9824\n",
      "epoch 364: train D loss: 0.6451, train F loss: -0.4010, acc 0.9838\n",
      "epoch 365: train D loss: 0.6496, train F loss: -0.4039, acc 0.9818\n",
      "epoch 366: train D loss: 0.6472, train F loss: -0.4154, acc 0.9854\n",
      "epoch 367: train D loss: 0.6534, train F loss: -0.4192, acc 0.9846\n",
      "epoch 368: train D loss: 0.6508, train F loss: -0.4203, acc 0.9852\n",
      "epoch 369: train D loss: 0.6527, train F loss: -0.4198, acc 0.9868\n",
      "epoch 370: train D loss: 0.6520, train F loss: -0.4175, acc 0.9848\n",
      "epoch 371: train D loss: 0.6480, train F loss: -0.4194, acc 0.9878\n",
      "epoch 372: train D loss: 0.6582, train F loss: -0.4234, acc 0.9840\n",
      "epoch 373: train D loss: 0.6545, train F loss: -0.4212, acc 0.9826\n",
      "epoch 374: train D loss: 0.6521, train F loss: -0.4241, acc 0.9862\n",
      "epoch 375: train D loss: 0.6558, train F loss: -0.4182, acc 0.9834\n",
      "epoch 376: train D loss: 0.6517, train F loss: -0.4212, acc 0.9862\n",
      "epoch 377: train D loss: 0.6536, train F loss: -0.4237, acc 0.9848\n",
      "epoch 378: train D loss: 0.6590, train F loss: -0.4234, acc 0.9804\n",
      "epoch 379: train D loss: 0.6520, train F loss: -0.4207, acc 0.9830\n",
      "epoch 380: train D loss: 0.6472, train F loss: -0.4193, acc 0.9840\n",
      "epoch 381: train D loss: 0.6481, train F loss: -0.4213, acc 0.9842\n",
      "epoch 382: train D loss: 0.6476, train F loss: -0.4222, acc 0.9866\n",
      "epoch 383: train D loss: 0.6509, train F loss: -0.4284, acc 0.9862\n",
      "epoch 384: train D loss: 0.6498, train F loss: -0.4247, acc 0.9852\n",
      "epoch 385: train D loss: 0.6620, train F loss: -0.4378, acc 0.9846\n",
      "epoch 386: train D loss: 0.6513, train F loss: -0.4178, acc 0.9828\n",
      "epoch 387: train D loss: 0.6556, train F loss: -0.4239, acc 0.9820\n",
      "epoch 388: train D loss: 0.6440, train F loss: -0.4223, acc 0.9832\n",
      "epoch 389: train D loss: 0.6587, train F loss: -0.4402, acc 0.9878\n",
      "epoch 390: train D loss: 0.6424, train F loss: -0.4185, acc 0.9814\n",
      "epoch 391: train D loss: 0.6619, train F loss: -0.4393, acc 0.9834\n",
      "epoch 392: train D loss: 0.6501, train F loss: -0.4354, acc 0.9856\n",
      "epoch 393: train D loss: 0.6491, train F loss: -0.4349, acc 0.9858\n",
      "epoch 394: train D loss: 0.6569, train F loss: -0.4474, acc 0.9864\n",
      "epoch 395: train D loss: 0.6548, train F loss: -0.4362, acc 0.9838\n",
      "epoch 396: train D loss: 0.6536, train F loss: -0.4313, acc 0.9844\n",
      "epoch 397: train D loss: 0.6494, train F loss: -0.4373, acc 0.9852\n",
      "epoch 398: train D loss: 0.6554, train F loss: -0.4365, acc 0.9864\n",
      "epoch 399: train D loss: 0.6589, train F loss: -0.4389, acc 0.9840\n",
      "epoch 400: train D loss: 0.6566, train F loss: -0.4329, acc 0.9830\n",
      "epoch 401: train D loss: 0.6550, train F loss: -0.4356, acc 0.9842\n",
      "epoch 402: train D loss: 0.6560, train F loss: -0.4463, acc 0.9844\n",
      "epoch 403: train D loss: 0.6540, train F loss: -0.4505, acc 0.9860\n",
      "epoch 404: train D loss: 0.6551, train F loss: -0.4498, acc 0.9850\n",
      "epoch 405: train D loss: 0.6496, train F loss: -0.4424, acc 0.9866\n",
      "epoch 406: train D loss: 0.6579, train F loss: -0.4479, acc 0.9846\n",
      "epoch 407: train D loss: 0.6609, train F loss: -0.4450, acc 0.9814\n",
      "epoch 408: train D loss: 0.6527, train F loss: -0.4432, acc 0.9856\n",
      "epoch 409: train D loss: 0.6490, train F loss: -0.4386, acc 0.9850\n",
      "epoch 410: train D loss: 0.6510, train F loss: -0.4457, acc 0.9850\n",
      "epoch 411: train D loss: 0.6502, train F loss: -0.4455, acc 0.9856\n",
      "epoch 412: train D loss: 0.6489, train F loss: -0.4467, acc 0.9844\n",
      "epoch 413: train D loss: 0.6524, train F loss: -0.4457, acc 0.9850\n",
      "epoch 414: train D loss: 0.6642, train F loss: -0.4634, acc 0.9864\n",
      "epoch 415: train D loss: 0.6506, train F loss: -0.4506, acc 0.9852\n",
      "epoch 416: train D loss: 0.6556, train F loss: -0.4512, acc 0.9856\n",
      "epoch 417: train D loss: 0.6514, train F loss: -0.4494, acc 0.9848\n",
      "epoch 418: train D loss: 0.6532, train F loss: -0.4518, acc 0.9854\n",
      "epoch 419: train D loss: 0.6562, train F loss: -0.4617, acc 0.9866\n",
      "epoch 420: train D loss: 0.6600, train F loss: -0.4559, acc 0.9852\n",
      "epoch 421: train D loss: 0.6569, train F loss: -0.4515, acc 0.9794\n",
      "epoch 422: train D loss: 0.6579, train F loss: -0.4586, acc 0.9856\n",
      "epoch 423: train D loss: 0.6486, train F loss: -0.4571, acc 0.9862\n",
      "epoch 424: train D loss: 0.6565, train F loss: -0.4496, acc 0.9844\n",
      "epoch 425: train D loss: 0.6534, train F loss: -0.4614, acc 0.9852\n",
      "epoch 426: train D loss: 0.6555, train F loss: -0.4646, acc 0.9860\n",
      "epoch 427: train D loss: 0.6549, train F loss: -0.4389, acc 0.9808\n",
      "epoch 428: train D loss: 0.6580, train F loss: -0.4677, acc 0.9866\n",
      "epoch 429: train D loss: 0.6585, train F loss: -0.4593, acc 0.9858\n",
      "epoch 430: train D loss: 0.6532, train F loss: -0.4683, acc 0.9870\n",
      "epoch 431: train D loss: 0.6601, train F loss: -0.4681, acc 0.9850\n",
      "epoch 432: train D loss: 0.6553, train F loss: -0.4607, acc 0.9830\n",
      "epoch 433: train D loss: 0.6585, train F loss: -0.4597, acc 0.9828\n",
      "epoch 434: train D loss: 0.6584, train F loss: -0.4750, acc 0.9878\n",
      "epoch 435: train D loss: 0.6553, train F loss: -0.4721, acc 0.9882\n",
      "epoch 436: train D loss: 0.6607, train F loss: -0.4680, acc 0.9854\n",
      "epoch 437: train D loss: 0.6616, train F loss: -0.4779, acc 0.9866\n",
      "epoch 438: train D loss: 0.6582, train F loss: -0.4756, acc 0.9868\n",
      "epoch 439: train D loss: 0.6580, train F loss: -0.4684, acc 0.9834\n",
      "epoch 440: train D loss: 0.6609, train F loss: -0.4822, acc 0.9866\n",
      "epoch 441: train D loss: 0.6625, train F loss: -0.4821, acc 0.9854\n",
      "epoch 442: train D loss: 0.6440, train F loss: -0.4672, acc 0.9852\n",
      "epoch 443: train D loss: 0.6564, train F loss: -0.4781, acc 0.9876\n",
      "epoch 444: train D loss: 0.6576, train F loss: -0.4761, acc 0.9850\n",
      "epoch 445: train D loss: 0.6568, train F loss: -0.4563, acc 0.9826\n",
      "epoch 446: train D loss: 0.6540, train F loss: -0.4548, acc 0.9818\n",
      "epoch 447: train D loss: 0.6570, train F loss: -0.4777, acc 0.9850\n",
      "epoch 448: train D loss: 0.6642, train F loss: -0.4798, acc 0.9866\n",
      "epoch 449: train D loss: 0.6533, train F loss: -0.4679, acc 0.9848\n",
      "epoch 450: train D loss: 0.6605, train F loss: -0.4825, acc 0.9868\n",
      "epoch 451: train D loss: 0.6516, train F loss: -0.4807, acc 0.9868\n",
      "epoch 452: train D loss: 0.6577, train F loss: -0.4785, acc 0.9840\n",
      "epoch 453: train D loss: 0.6502, train F loss: -0.4782, acc 0.9860\n",
      "epoch 454: train D loss: 0.6534, train F loss: -0.4737, acc 0.9842\n",
      "epoch 455: train D loss: 0.6619, train F loss: -0.4891, acc 0.9874\n",
      "epoch 456: train D loss: 0.6572, train F loss: -0.4842, acc 0.9872\n",
      "epoch 457: train D loss: 0.6496, train F loss: -0.4710, acc 0.9856\n",
      "epoch 458: train D loss: 0.6578, train F loss: -0.4848, acc 0.9856\n",
      "epoch 459: train D loss: 0.6633, train F loss: -0.4991, acc 0.9884\n",
      "epoch 460: train D loss: 0.6634, train F loss: -0.4898, acc 0.9864\n",
      "epoch 461: train D loss: 0.6583, train F loss: -0.4831, acc 0.9856\n",
      "epoch 462: train D loss: 0.6524, train F loss: -0.4855, acc 0.9858\n",
      "epoch 463: train D loss: 0.6578, train F loss: -0.4878, acc 0.9868\n",
      "epoch 464: train D loss: 0.6571, train F loss: -0.4909, acc 0.9876\n",
      "epoch 465: train D loss: 0.6543, train F loss: -0.4724, acc 0.9820\n",
      "epoch 466: train D loss: 0.6615, train F loss: -0.4878, acc 0.9866\n",
      "epoch 467: train D loss: 0.6512, train F loss: -0.4699, acc 0.9836\n",
      "epoch 468: train D loss: 0.6558, train F loss: -0.4875, acc 0.9866\n",
      "epoch 469: train D loss: 0.6581, train F loss: -0.4729, acc 0.9818\n",
      "epoch 470: train D loss: 0.6559, train F loss: -0.4839, acc 0.9858\n",
      "epoch 471: train D loss: 0.6635, train F loss: -0.4957, acc 0.9882\n",
      "epoch 472: train D loss: 0.6565, train F loss: -0.4927, acc 0.9870\n",
      "epoch 473: train D loss: 0.6663, train F loss: -0.5002, acc 0.9864\n",
      "epoch 474: train D loss: 0.6538, train F loss: -0.4871, acc 0.9846\n",
      "epoch 475: train D loss: 0.6616, train F loss: -0.4975, acc 0.9868\n",
      "epoch 476: train D loss: 0.6507, train F loss: -0.4920, acc 0.9888\n",
      "epoch 477: train D loss: 0.6530, train F loss: -0.4970, acc 0.9874\n",
      "epoch 478: train D loss: 0.6574, train F loss: -0.5040, acc 0.9882\n",
      "epoch 479: train D loss: 0.6562, train F loss: -0.4937, acc 0.9868\n",
      "epoch 480: train D loss: 0.6648, train F loss: -0.5056, acc 0.9854\n",
      "epoch 481: train D loss: 0.6659, train F loss: -0.5015, acc 0.9868\n",
      "epoch 482: train D loss: 0.6605, train F loss: -0.4987, acc 0.9856\n",
      "epoch 483: train D loss: 0.6611, train F loss: -0.4899, acc 0.9836\n",
      "epoch 484: train D loss: 0.6629, train F loss: -0.5017, acc 0.9870\n",
      "epoch 485: train D loss: 0.6600, train F loss: -0.4997, acc 0.9856\n",
      "epoch 486: train D loss: 0.6604, train F loss: -0.4970, acc 0.9846\n",
      "epoch 487: train D loss: 0.6493, train F loss: -0.4890, acc 0.9874\n",
      "epoch 488: train D loss: 0.6619, train F loss: -0.5032, acc 0.9852\n",
      "epoch 489: train D loss: 0.6571, train F loss: -0.4933, acc 0.9856\n",
      "epoch 490: train D loss: 0.6604, train F loss: -0.5055, acc 0.9858\n",
      "epoch 491: train D loss: 0.6579, train F loss: -0.5046, acc 0.9886\n",
      "epoch 492: train D loss: 0.6669, train F loss: -0.5151, acc 0.9862\n",
      "epoch 493: train D loss: 0.6582, train F loss: -0.5025, acc 0.9848\n",
      "epoch 494: train D loss: 0.6666, train F loss: -0.5076, acc 0.9852\n",
      "epoch 495: train D loss: 0.6629, train F loss: -0.5148, acc 0.9874\n",
      "epoch 496: train D loss: 0.6644, train F loss: -0.5097, acc 0.9852\n",
      "epoch 497: train D loss: 0.6598, train F loss: -0.5102, acc 0.9876\n",
      "epoch 498: train D loss: 0.6635, train F loss: -0.4966, acc 0.9848\n",
      "epoch 499: train D loss: 0.6583, train F loss: -0.5066, acc 0.9866\n",
      "epoch 500: train D loss: 0.6607, train F loss: -0.5016, acc 0.9846\n",
      "epoch 501: train D loss: 0.6586, train F loss: -0.5039, acc 0.9858\n",
      "epoch 502: train D loss: 0.6553, train F loss: -0.5016, acc 0.9846\n",
      "epoch 503: train D loss: 0.6580, train F loss: -0.5017, acc 0.9848\n",
      "epoch 504: train D loss: 0.6560, train F loss: -0.5122, acc 0.9870\n",
      "epoch 505: train D loss: 0.6598, train F loss: -0.5120, acc 0.9870\n",
      "epoch 506: train D loss: 0.6568, train F loss: -0.5037, acc 0.9856\n",
      "epoch 507: train D loss: 0.6617, train F loss: -0.5001, acc 0.9848\n",
      "epoch 508: train D loss: 0.6614, train F loss: -0.5215, acc 0.9898\n",
      "epoch 509: train D loss: 0.6567, train F loss: -0.5046, acc 0.9854\n",
      "epoch 510: train D loss: 0.6533, train F loss: -0.5025, acc 0.9852\n",
      "epoch 511: train D loss: 0.6649, train F loss: -0.5204, acc 0.9870\n",
      "epoch 512: train D loss: 0.6632, train F loss: -0.5029, acc 0.9814\n",
      "epoch 513: train D loss: 0.6535, train F loss: -0.5148, acc 0.9856\n",
      "epoch 514: train D loss: 0.6590, train F loss: -0.5107, acc 0.9866\n",
      "epoch 515: train D loss: 0.6627, train F loss: -0.5112, acc 0.9852\n",
      "epoch 516: train D loss: 0.6558, train F loss: -0.5145, acc 0.9878\n",
      "epoch 517: train D loss: 0.6579, train F loss: -0.5191, acc 0.9878\n",
      "epoch 518: train D loss: 0.6643, train F loss: -0.5244, acc 0.9876\n",
      "epoch 519: train D loss: 0.6605, train F loss: -0.5177, acc 0.9858\n",
      "epoch 520: train D loss: 0.6629, train F loss: -0.5114, acc 0.9856\n",
      "epoch 521: train D loss: 0.6622, train F loss: -0.4903, acc 0.9796\n",
      "epoch 522: train D loss: 0.6576, train F loss: -0.5227, acc 0.9878\n",
      "epoch 523: train D loss: 0.6612, train F loss: -0.5309, acc 0.9900\n",
      "epoch 524: train D loss: 0.6594, train F loss: -0.5088, acc 0.9848\n",
      "epoch 525: train D loss: 0.6619, train F loss: -0.5217, acc 0.9874\n",
      "epoch 526: train D loss: 0.6568, train F loss: -0.5173, acc 0.9864\n",
      "epoch 527: train D loss: 0.6612, train F loss: -0.5206, acc 0.9862\n",
      "epoch 528: train D loss: 0.6585, train F loss: -0.5206, acc 0.9864\n",
      "epoch 529: train D loss: 0.6676, train F loss: -0.5264, acc 0.9852\n",
      "epoch 530: train D loss: 0.6615, train F loss: -0.5253, acc 0.9878\n",
      "epoch 531: train D loss: 0.6610, train F loss: -0.5259, acc 0.9868\n",
      "epoch 532: train D loss: 0.6609, train F loss: -0.5226, acc 0.9852\n",
      "epoch 533: train D loss: 0.6662, train F loss: -0.5367, acc 0.9894\n",
      "epoch 534: train D loss: 0.6592, train F loss: -0.5222, acc 0.9854\n",
      "epoch 535: train D loss: 0.6607, train F loss: -0.5253, acc 0.9880\n",
      "epoch 536: train D loss: 0.6613, train F loss: -0.5250, acc 0.9870\n",
      "epoch 537: train D loss: 0.6662, train F loss: -0.5256, acc 0.9868\n",
      "epoch 538: train D loss: 0.6619, train F loss: -0.5084, acc 0.9828\n",
      "epoch 539: train D loss: 0.6576, train F loss: -0.5170, acc 0.9838\n",
      "epoch 540: train D loss: 0.6591, train F loss: -0.5293, acc 0.9872\n",
      "epoch 541: train D loss: 0.6583, train F loss: -0.5082, acc 0.9826\n",
      "epoch 542: train D loss: 0.6661, train F loss: -0.5374, acc 0.9886\n",
      "epoch 543: train D loss: 0.6576, train F loss: -0.5327, acc 0.9890\n",
      "epoch 544: train D loss: 0.6620, train F loss: -0.5373, acc 0.9898\n",
      "epoch 545: train D loss: 0.6644, train F loss: -0.5319, acc 0.9872\n",
      "epoch 546: train D loss: 0.6658, train F loss: -0.5359, acc 0.9882\n",
      "epoch 547: train D loss: 0.6662, train F loss: -0.5388, acc 0.9894\n",
      "epoch 548: train D loss: 0.6612, train F loss: -0.5228, acc 0.9866\n",
      "epoch 549: train D loss: 0.6670, train F loss: -0.5382, acc 0.9876\n",
      "epoch 550: train D loss: 0.6623, train F loss: -0.5382, acc 0.9882\n",
      "epoch 551: train D loss: 0.6610, train F loss: -0.5416, acc 0.9898\n",
      "epoch 552: train D loss: 0.6638, train F loss: -0.5328, acc 0.9882\n",
      "epoch 553: train D loss: 0.6684, train F loss: -0.5492, acc 0.9898\n",
      "epoch 554: train D loss: 0.6649, train F loss: -0.5380, acc 0.9882\n",
      "epoch 555: train D loss: 0.6568, train F loss: -0.5284, acc 0.9868\n",
      "epoch 556: train D loss: 0.6687, train F loss: -0.5371, acc 0.9862\n",
      "epoch 557: train D loss: 0.6633, train F loss: -0.5237, acc 0.9824\n",
      "epoch 558: train D loss: 0.6589, train F loss: -0.5253, acc 0.9844\n",
      "epoch 559: train D loss: 0.6612, train F loss: -0.5394, acc 0.9874\n",
      "epoch 560: train D loss: 0.6598, train F loss: -0.5318, acc 0.9880\n",
      "epoch 561: train D loss: 0.6656, train F loss: -0.5336, acc 0.9848\n",
      "epoch 562: train D loss: 0.6560, train F loss: -0.5408, acc 0.9890\n",
      "epoch 563: train D loss: 0.6695, train F loss: -0.5471, acc 0.9880\n",
      "epoch 564: train D loss: 0.6615, train F loss: -0.5342, acc 0.9862\n",
      "epoch 565: train D loss: 0.6593, train F loss: -0.5408, acc 0.9880\n",
      "epoch 566: train D loss: 0.6656, train F loss: -0.5514, acc 0.9884\n",
      "epoch 567: train D loss: 0.6639, train F loss: -0.5309, acc 0.9846\n",
      "epoch 568: train D loss: 0.6617, train F loss: -0.5391, acc 0.9878\n",
      "epoch 569: train D loss: 0.6695, train F loss: -0.5513, acc 0.9874\n",
      "epoch 570: train D loss: 0.6614, train F loss: -0.5300, acc 0.9876\n",
      "epoch 571: train D loss: 0.6590, train F loss: -0.5421, acc 0.9894\n",
      "epoch 572: train D loss: 0.6612, train F loss: -0.5270, acc 0.9832\n",
      "epoch 573: train D loss: 0.6605, train F loss: -0.5358, acc 0.9858\n",
      "epoch 574: train D loss: 0.6621, train F loss: -0.5456, acc 0.9886\n",
      "epoch 575: train D loss: 0.6642, train F loss: -0.5327, acc 0.9854\n",
      "epoch 576: train D loss: 0.6671, train F loss: -0.5531, acc 0.9888\n",
      "epoch 577: train D loss: 0.6711, train F loss: -0.5641, acc 0.9900\n",
      "epoch 578: train D loss: 0.6620, train F loss: -0.5293, acc 0.9846\n",
      "epoch 579: train D loss: 0.6563, train F loss: -0.5405, acc 0.9870\n",
      "epoch 580: train D loss: 0.6632, train F loss: -0.5460, acc 0.9880\n",
      "epoch 581: train D loss: 0.6678, train F loss: -0.5560, acc 0.9876\n",
      "epoch 582: train D loss: 0.6650, train F loss: -0.5455, acc 0.9866\n",
      "epoch 583: train D loss: 0.6625, train F loss: -0.5426, acc 0.9862\n",
      "epoch 584: train D loss: 0.6670, train F loss: -0.5326, acc 0.9848\n",
      "epoch 585: train D loss: 0.6699, train F loss: -0.5441, acc 0.9852\n",
      "epoch 586: train D loss: 0.6589, train F loss: -0.5355, acc 0.9854\n",
      "epoch 587: train D loss: 0.6640, train F loss: -0.5521, acc 0.9892\n",
      "epoch 588: train D loss: 0.6638, train F loss: -0.5471, acc 0.9872\n",
      "epoch 589: train D loss: 0.6650, train F loss: -0.5501, acc 0.9880\n",
      "epoch 590: train D loss: 0.6613, train F loss: -0.5390, acc 0.9856\n",
      "epoch 591: train D loss: 0.6639, train F loss: -0.5523, acc 0.9888\n",
      "epoch 592: train D loss: 0.6654, train F loss: -0.5557, acc 0.9886\n",
      "epoch 593: train D loss: 0.6654, train F loss: -0.5553, acc 0.9882\n",
      "epoch 594: train D loss: 0.6667, train F loss: -0.5573, acc 0.9896\n",
      "epoch 595: train D loss: 0.6629, train F loss: -0.5518, acc 0.9864\n",
      "epoch 596: train D loss: 0.6596, train F loss: -0.5433, acc 0.9858\n",
      "epoch 597: train D loss: 0.6684, train F loss: -0.5571, acc 0.9884\n",
      "epoch 598: train D loss: 0.6628, train F loss: -0.5588, acc 0.9894\n",
      "epoch 599: train D loss: 0.6607, train F loss: -0.5345, acc 0.9830\n",
      "epoch 600: train D loss: 0.6632, train F loss: -0.5414, acc 0.9858\n",
      "epoch 601: train D loss: 0.6650, train F loss: -0.5468, acc 0.9860\n",
      "epoch 602: train D loss: 0.6607, train F loss: -0.5511, acc 0.9882\n",
      "epoch 603: train D loss: 0.6584, train F loss: -0.5437, acc 0.9868\n",
      "epoch 604: train D loss: 0.6651, train F loss: -0.5562, acc 0.9868\n",
      "epoch 605: train D loss: 0.6599, train F loss: -0.5431, acc 0.9858\n",
      "epoch 606: train D loss: 0.6635, train F loss: -0.5551, acc 0.9882\n",
      "epoch 607: train D loss: 0.6642, train F loss: -0.5556, acc 0.9874\n",
      "epoch 608: train D loss: 0.6680, train F loss: -0.5639, acc 0.9898\n",
      "epoch 609: train D loss: 0.6618, train F loss: -0.5495, acc 0.9884\n",
      "epoch 610: train D loss: 0.6609, train F loss: -0.5592, acc 0.9894\n",
      "epoch 611: train D loss: 0.6681, train F loss: -0.5634, acc 0.9882\n",
      "epoch 612: train D loss: 0.6640, train F loss: -0.5449, acc 0.9850\n",
      "epoch 613: train D loss: 0.6603, train F loss: -0.5592, acc 0.9880\n",
      "epoch 614: train D loss: 0.6658, train F loss: -0.5645, acc 0.9890\n",
      "epoch 615: train D loss: 0.6673, train F loss: -0.5372, acc 0.9832\n",
      "epoch 616: train D loss: 0.6607, train F loss: -0.5379, acc 0.9836\n",
      "epoch 617: train D loss: 0.6629, train F loss: -0.5548, acc 0.9880\n",
      "epoch 618: train D loss: 0.6701, train F loss: -0.5697, acc 0.9874\n",
      "epoch 619: train D loss: 0.6651, train F loss: -0.5550, acc 0.9878\n",
      "epoch 620: train D loss: 0.6617, train F loss: -0.5452, acc 0.9848\n",
      "epoch 621: train D loss: 0.6598, train F loss: -0.5425, acc 0.9844\n",
      "epoch 622: train D loss: 0.6571, train F loss: -0.5560, acc 0.9890\n",
      "epoch 623: train D loss: 0.6636, train F loss: -0.5577, acc 0.9896\n",
      "epoch 624: train D loss: 0.6580, train F loss: -0.5583, acc 0.9892\n",
      "epoch 625: train D loss: 0.6587, train F loss: -0.5423, acc 0.9860\n",
      "epoch 626: train D loss: 0.6642, train F loss: -0.5674, acc 0.9880\n",
      "epoch 627: train D loss: 0.6655, train F loss: -0.5604, acc 0.9866\n",
      "epoch 628: train D loss: 0.6616, train F loss: -0.5516, acc 0.9852\n",
      "epoch 629: train D loss: 0.6616, train F loss: -0.5585, acc 0.9882\n",
      "epoch 630: train D loss: 0.6595, train F loss: -0.5532, acc 0.9878\n",
      "epoch 631: train D loss: 0.6586, train F loss: -0.5559, acc 0.9884\n",
      "epoch 632: train D loss: 0.6599, train F loss: -0.5694, acc 0.9922\n",
      "epoch 633: train D loss: 0.6671, train F loss: -0.5683, acc 0.9890\n",
      "epoch 634: train D loss: 0.6587, train F loss: -0.5600, acc 0.9886\n",
      "epoch 635: train D loss: 0.6663, train F loss: -0.5704, acc 0.9902\n",
      "epoch 636: train D loss: 0.6647, train F loss: -0.5600, acc 0.9842\n",
      "epoch 637: train D loss: 0.6649, train F loss: -0.5482, acc 0.9828\n",
      "epoch 638: train D loss: 0.6633, train F loss: -0.5541, acc 0.9862\n",
      "epoch 639: train D loss: 0.6639, train F loss: -0.5574, acc 0.9870\n",
      "epoch 640: train D loss: 0.6628, train F loss: -0.5598, acc 0.9868\n",
      "epoch 641: train D loss: 0.6716, train F loss: -0.5769, acc 0.9904\n",
      "epoch 642: train D loss: 0.6637, train F loss: -0.5678, acc 0.9872\n",
      "epoch 643: train D loss: 0.6672, train F loss: -0.5706, acc 0.9880\n",
      "epoch 644: train D loss: 0.6611, train F loss: -0.5703, acc 0.9888\n",
      "epoch 645: train D loss: 0.6635, train F loss: -0.5565, acc 0.9854\n",
      "epoch 646: train D loss: 0.6597, train F loss: -0.5489, acc 0.9842\n",
      "epoch 647: train D loss: 0.6632, train F loss: -0.5647, acc 0.9884\n",
      "epoch 648: train D loss: 0.6630, train F loss: -0.5743, acc 0.9908\n",
      "epoch 649: train D loss: 0.6608, train F loss: -0.5639, acc 0.9870\n",
      "epoch 650: train D loss: 0.6710, train F loss: -0.5684, acc 0.9868\n",
      "epoch 651: train D loss: 0.6654, train F loss: -0.5721, acc 0.9890\n",
      "epoch 652: train D loss: 0.6640, train F loss: -0.5600, acc 0.9866\n",
      "epoch 653: train D loss: 0.6652, train F loss: -0.5773, acc 0.9880\n",
      "epoch 654: train D loss: 0.6618, train F loss: -0.5755, acc 0.9908\n",
      "epoch 655: train D loss: 0.6635, train F loss: -0.5653, acc 0.9894\n",
      "epoch 656: train D loss: 0.6663, train F loss: -0.5667, acc 0.9850\n",
      "epoch 657: train D loss: 0.6619, train F loss: -0.5654, acc 0.9868\n",
      "epoch 658: train D loss: 0.6644, train F loss: -0.5639, acc 0.9878\n",
      "epoch 659: train D loss: 0.6658, train F loss: -0.5720, acc 0.9878\n",
      "epoch 660: train D loss: 0.6662, train F loss: -0.5778, acc 0.9890\n",
      "epoch 661: train D loss: 0.6616, train F loss: -0.5710, acc 0.9894\n",
      "epoch 662: train D loss: 0.6708, train F loss: -0.5759, acc 0.9882\n",
      "epoch 663: train D loss: 0.6608, train F loss: -0.5671, acc 0.9884\n",
      "epoch 664: train D loss: 0.6684, train F loss: -0.5660, acc 0.9856\n",
      "epoch 665: train D loss: 0.6679, train F loss: -0.5806, acc 0.9884\n",
      "epoch 666: train D loss: 0.6597, train F loss: -0.5745, acc 0.9908\n",
      "epoch 667: train D loss: 0.6698, train F loss: -0.5804, acc 0.9892\n",
      "epoch 668: train D loss: 0.6703, train F loss: -0.5634, acc 0.9840\n",
      "epoch 669: train D loss: 0.6622, train F loss: -0.5612, acc 0.9866\n",
      "epoch 670: train D loss: 0.6664, train F loss: -0.5732, acc 0.9864\n",
      "epoch 671: train D loss: 0.6630, train F loss: -0.5659, acc 0.9870\n",
      "epoch 672: train D loss: 0.6613, train F loss: -0.5747, acc 0.9900\n",
      "epoch 673: train D loss: 0.6617, train F loss: -0.5607, acc 0.9866\n",
      "epoch 674: train D loss: 0.6620, train F loss: -0.5675, acc 0.9882\n",
      "epoch 675: train D loss: 0.6694, train F loss: -0.5776, acc 0.9892\n",
      "epoch 676: train D loss: 0.6697, train F loss: -0.5762, acc 0.9874\n",
      "epoch 677: train D loss: 0.6652, train F loss: -0.5758, acc 0.9896\n",
      "epoch 678: train D loss: 0.6627, train F loss: -0.5766, acc 0.9870\n",
      "epoch 679: train D loss: 0.6640, train F loss: -0.5745, acc 0.9878\n",
      "epoch 680: train D loss: 0.6631, train F loss: -0.5756, acc 0.9898\n",
      "epoch 681: train D loss: 0.6649, train F loss: -0.5688, acc 0.9868\n",
      "epoch 682: train D loss: 0.6675, train F loss: -0.5772, acc 0.9876\n",
      "epoch 683: train D loss: 0.6680, train F loss: -0.5703, acc 0.9842\n",
      "epoch 684: train D loss: 0.6592, train F loss: -0.5629, acc 0.9868\n",
      "epoch 685: train D loss: 0.6718, train F loss: -0.5867, acc 0.9910\n",
      "epoch 686: train D loss: 0.6660, train F loss: -0.5873, acc 0.9906\n",
      "epoch 687: train D loss: 0.6668, train F loss: -0.5793, acc 0.9888\n",
      "epoch 688: train D loss: 0.6646, train F loss: -0.5729, acc 0.9890\n",
      "epoch 689: train D loss: 0.6682, train F loss: -0.5803, acc 0.9892\n",
      "epoch 690: train D loss: 0.6708, train F loss: -0.5872, acc 0.9884\n",
      "epoch 691: train D loss: 0.6656, train F loss: -0.5839, acc 0.9876\n",
      "epoch 692: train D loss: 0.6672, train F loss: -0.5854, acc 0.9894\n",
      "epoch 693: train D loss: 0.6650, train F loss: -0.5758, acc 0.9890\n",
      "epoch 694: train D loss: 0.6601, train F loss: -0.5831, acc 0.9906\n",
      "epoch 695: train D loss: 0.6708, train F loss: -0.5682, acc 0.9874\n",
      "epoch 696: train D loss: 0.6670, train F loss: -0.5735, acc 0.9852\n",
      "epoch 697: train D loss: 0.6634, train F loss: -0.5853, acc 0.9898\n",
      "epoch 698: train D loss: 0.6657, train F loss: -0.5842, acc 0.9902\n",
      "epoch 699: train D loss: 0.6599, train F loss: -0.5748, acc 0.9882\n",
      "epoch 700: train D loss: 0.6693, train F loss: -0.5788, acc 0.9860\n",
      "epoch 701: train D loss: 0.6666, train F loss: -0.5854, acc 0.9886\n",
      "epoch 702: train D loss: 0.6632, train F loss: -0.5790, acc 0.9880\n",
      "epoch 703: train D loss: 0.6618, train F loss: -0.5586, acc 0.9854\n",
      "epoch 704: train D loss: 0.6640, train F loss: -0.5773, acc 0.9880\n",
      "epoch 705: train D loss: 0.6665, train F loss: -0.5838, acc 0.9898\n",
      "epoch 706: train D loss: 0.6658, train F loss: -0.5849, acc 0.9898\n",
      "epoch 707: train D loss: 0.6636, train F loss: -0.5788, acc 0.9892\n",
      "epoch 708: train D loss: 0.6652, train F loss: -0.5830, acc 0.9878\n",
      "epoch 709: train D loss: 0.6659, train F loss: -0.5872, acc 0.9888\n",
      "epoch 710: train D loss: 0.6639, train F loss: -0.5637, acc 0.9882\n",
      "epoch 711: train D loss: 0.6694, train F loss: -0.5914, acc 0.9902\n",
      "epoch 712: train D loss: 0.6641, train F loss: -0.5874, acc 0.9906\n",
      "epoch 713: train D loss: 0.6639, train F loss: -0.5893, acc 0.9920\n",
      "epoch 714: train D loss: 0.6723, train F loss: -0.5773, acc 0.9832\n",
      "epoch 715: train D loss: 0.6629, train F loss: -0.5800, acc 0.9878\n",
      "epoch 716: train D loss: 0.6656, train F loss: -0.5837, acc 0.9894\n",
      "epoch 717: train D loss: 0.6679, train F loss: -0.5936, acc 0.9900\n",
      "epoch 718: train D loss: 0.6659, train F loss: -0.5817, acc 0.9884\n",
      "epoch 719: train D loss: 0.6644, train F loss: -0.5808, acc 0.9882\n",
      "epoch 720: train D loss: 0.6674, train F loss: -0.5852, acc 0.9896\n",
      "epoch 721: train D loss: 0.6623, train F loss: -0.5881, acc 0.9912\n",
      "epoch 722: train D loss: 0.6690, train F loss: -0.5859, acc 0.9882\n",
      "epoch 723: train D loss: 0.6619, train F loss: -0.5669, acc 0.9842\n",
      "epoch 724: train D loss: 0.6617, train F loss: -0.5770, acc 0.9880\n",
      "epoch 725: train D loss: 0.6636, train F loss: -0.5828, acc 0.9886\n",
      "epoch 726: train D loss: 0.6622, train F loss: -0.5878, acc 0.9900\n",
      "epoch 727: train D loss: 0.6643, train F loss: -0.5851, acc 0.9894\n",
      "epoch 728: train D loss: 0.6673, train F loss: -0.5831, acc 0.9876\n",
      "epoch 729: train D loss: 0.6690, train F loss: -0.5761, acc 0.9862\n",
      "epoch 730: train D loss: 0.6714, train F loss: -0.5814, acc 0.9866\n",
      "epoch 731: train D loss: 0.6627, train F loss: -0.5838, acc 0.9888\n",
      "epoch 732: train D loss: 0.6661, train F loss: -0.5930, acc 0.9904\n",
      "epoch 733: train D loss: 0.6640, train F loss: -0.5943, acc 0.9904\n",
      "epoch 734: train D loss: 0.6650, train F loss: -0.5854, acc 0.9894\n",
      "epoch 735: train D loss: 0.6685, train F loss: -0.5859, acc 0.9880\n",
      "epoch 736: train D loss: 0.6667, train F loss: -0.5753, acc 0.9856\n",
      "epoch 737: train D loss: 0.6649, train F loss: -0.5912, acc 0.9902\n",
      "epoch 738: train D loss: 0.6722, train F loss: -0.6033, acc 0.9918\n",
      "epoch 739: train D loss: 0.6717, train F loss: -0.5925, acc 0.9892\n",
      "epoch 740: train D loss: 0.6652, train F loss: -0.5848, acc 0.9864\n",
      "epoch 741: train D loss: 0.6656, train F loss: -0.5830, acc 0.9862\n",
      "epoch 742: train D loss: 0.6619, train F loss: -0.5751, acc 0.9840\n",
      "epoch 743: train D loss: 0.6599, train F loss: -0.5868, acc 0.9892\n",
      "epoch 744: train D loss: 0.6682, train F loss: -0.5954, acc 0.9888\n",
      "epoch 745: train D loss: 0.6711, train F loss: -0.5849, acc 0.9848\n",
      "epoch 746: train D loss: 0.6684, train F loss: -0.5927, acc 0.9896\n",
      "epoch 747: train D loss: 0.6634, train F loss: -0.5947, acc 0.9906\n",
      "epoch 748: train D loss: 0.6665, train F loss: -0.5861, acc 0.9870\n",
      "epoch 749: train D loss: 0.6655, train F loss: -0.5840, acc 0.9874\n",
      "epoch 750: train D loss: 0.6621, train F loss: -0.5850, acc 0.9890\n",
      "epoch 751: train D loss: 0.6650, train F loss: -0.5769, acc 0.9882\n",
      "epoch 752: train D loss: 0.6672, train F loss: -0.5869, acc 0.9874\n",
      "epoch 753: train D loss: 0.6656, train F loss: -0.5927, acc 0.9900\n",
      "epoch 754: train D loss: 0.6669, train F loss: -0.5834, acc 0.9882\n",
      "epoch 755: train D loss: 0.6658, train F loss: -0.5905, acc 0.9882\n",
      "epoch 756: train D loss: 0.6684, train F loss: -0.5892, acc 0.9870\n",
      "epoch 757: train D loss: 0.6663, train F loss: -0.5812, acc 0.9880\n",
      "epoch 758: train D loss: 0.6576, train F loss: -0.5826, acc 0.9888\n",
      "epoch 759: train D loss: 0.6708, train F loss: -0.5910, acc 0.9902\n",
      "epoch 760: train D loss: 0.6672, train F loss: -0.5897, acc 0.9870\n",
      "epoch 761: train D loss: 0.6698, train F loss: -0.5960, acc 0.9902\n",
      "epoch 762: train D loss: 0.6635, train F loss: -0.5864, acc 0.9884\n",
      "epoch 763: train D loss: 0.6629, train F loss: -0.5923, acc 0.9894\n",
      "epoch 764: train D loss: 0.6672, train F loss: -0.5912, acc 0.9872\n",
      "epoch 765: train D loss: 0.6670, train F loss: -0.5999, acc 0.9902\n",
      "epoch 766: train D loss: 0.6665, train F loss: -0.5984, acc 0.9898\n",
      "epoch 767: train D loss: 0.6696, train F loss: -0.5984, acc 0.9880\n",
      "epoch 768: train D loss: 0.6696, train F loss: -0.5983, acc 0.9892\n",
      "epoch 769: train D loss: 0.6698, train F loss: -0.5976, acc 0.9878\n",
      "epoch 770: train D loss: 0.6671, train F loss: -0.5875, acc 0.9872\n",
      "epoch 771: train D loss: 0.6651, train F loss: -0.5964, acc 0.9900\n",
      "epoch 772: train D loss: 0.6625, train F loss: -0.5830, acc 0.9858\n",
      "epoch 773: train D loss: 0.6698, train F loss: -0.5859, acc 0.9860\n",
      "epoch 774: train D loss: 0.6659, train F loss: -0.5969, acc 0.9878\n",
      "epoch 775: train D loss: 0.6688, train F loss: -0.6008, acc 0.9894\n",
      "epoch 776: train D loss: 0.6708, train F loss: -0.6019, acc 0.9884\n",
      "epoch 777: train D loss: 0.6650, train F loss: -0.5946, acc 0.9894\n",
      "epoch 778: train D loss: 0.6678, train F loss: -0.5897, acc 0.9874\n",
      "epoch 779: train D loss: 0.6669, train F loss: -0.5929, acc 0.9894\n",
      "epoch 780: train D loss: 0.6625, train F loss: -0.5691, acc 0.9858\n",
      "epoch 781: train D loss: 0.6561, train F loss: -0.5735, acc 0.9854\n",
      "epoch 782: train D loss: 0.6699, train F loss: -0.6038, acc 0.9890\n",
      "epoch 783: train D loss: 0.6679, train F loss: -0.6063, acc 0.9906\n",
      "epoch 784: train D loss: 0.6720, train F loss: -0.6011, acc 0.9886\n",
      "epoch 785: train D loss: 0.6670, train F loss: -0.5896, acc 0.9854\n",
      "epoch 786: train D loss: 0.6687, train F loss: -0.6012, acc 0.9900\n",
      "epoch 787: train D loss: 0.6661, train F loss: -0.5950, acc 0.9894\n",
      "epoch 788: train D loss: 0.6607, train F loss: -0.5800, acc 0.9860\n",
      "epoch 789: train D loss: 0.6671, train F loss: -0.5906, acc 0.9876\n",
      "epoch 790: train D loss: 0.6662, train F loss: -0.5917, acc 0.9888\n",
      "epoch 791: train D loss: 0.6686, train F loss: -0.6016, acc 0.9894\n",
      "epoch 792: train D loss: 0.6675, train F loss: -0.6037, acc 0.9904\n",
      "epoch 793: train D loss: 0.6671, train F loss: -0.6072, acc 0.9916\n",
      "epoch 794: train D loss: 0.6709, train F loss: -0.5973, acc 0.9866\n",
      "epoch 795: train D loss: 0.6672, train F loss: -0.5892, acc 0.9874\n",
      "epoch 796: train D loss: 0.6665, train F loss: -0.6044, acc 0.9912\n",
      "epoch 797: train D loss: 0.6636, train F loss: -0.5949, acc 0.9894\n",
      "epoch 798: train D loss: 0.6722, train F loss: -0.5983, acc 0.9894\n",
      "epoch 799: train D loss: 0.6690, train F loss: -0.6088, acc 0.9910\n",
      "epoch 800: train D loss: 0.6672, train F loss: -0.6051, acc 0.9904\n",
      "epoch 801: train D loss: 0.6718, train F loss: -0.6100, acc 0.9904\n",
      "epoch 802: train D loss: 0.6690, train F loss: -0.6017, acc 0.9876\n",
      "epoch 803: train D loss: 0.6754, train F loss: -0.6075, acc 0.9876\n",
      "epoch 804: train D loss: 0.6683, train F loss: -0.6135, acc 0.9918\n",
      "epoch 805: train D loss: 0.6633, train F loss: -0.5900, acc 0.9896\n",
      "epoch 806: train D loss: 0.6708, train F loss: -0.5895, acc 0.9854\n",
      "epoch 807: train D loss: 0.6658, train F loss: -0.5995, acc 0.9896\n",
      "epoch 808: train D loss: 0.6686, train F loss: -0.6015, acc 0.9906\n",
      "epoch 809: train D loss: 0.6705, train F loss: -0.5995, acc 0.9874\n",
      "epoch 810: train D loss: 0.6657, train F loss: -0.5908, acc 0.9870\n",
      "epoch 811: train D loss: 0.6647, train F loss: -0.5900, acc 0.9872\n",
      "epoch 812: train D loss: 0.6628, train F loss: -0.6010, acc 0.9914\n",
      "epoch 813: train D loss: 0.6716, train F loss: -0.6062, acc 0.9910\n",
      "epoch 814: train D loss: 0.6651, train F loss: -0.5980, acc 0.9886\n",
      "epoch 815: train D loss: 0.6741, train F loss: -0.6064, acc 0.9884\n",
      "epoch 816: train D loss: 0.6702, train F loss: -0.6165, acc 0.9928\n",
      "epoch 817: train D loss: 0.6696, train F loss: -0.6110, acc 0.9916\n",
      "epoch 818: train D loss: 0.6690, train F loss: -0.6057, acc 0.9894\n",
      "epoch 819: train D loss: 0.6695, train F loss: -0.6063, acc 0.9900\n",
      "epoch 820: train D loss: 0.6764, train F loss: -0.6093, acc 0.9888\n",
      "epoch 821: train D loss: 0.6668, train F loss: -0.5602, acc 0.9854\n",
      "epoch 822: train D loss: 0.6682, train F loss: -0.5821, acc 0.9838\n",
      "epoch 823: train D loss: 0.6677, train F loss: -0.6113, acc 0.9910\n",
      "epoch 824: train D loss: 0.6685, train F loss: -0.6108, acc 0.9916\n",
      "epoch 825: train D loss: 0.6707, train F loss: -0.5996, acc 0.9874\n",
      "epoch 826: train D loss: 0.6734, train F loss: -0.6009, acc 0.9888\n",
      "epoch 827: train D loss: 0.6702, train F loss: -0.6097, acc 0.9898\n",
      "epoch 828: train D loss: 0.6680, train F loss: -0.6170, acc 0.9926\n",
      "epoch 829: train D loss: 0.6663, train F loss: -0.6131, acc 0.9924\n",
      "epoch 830: train D loss: 0.6708, train F loss: -0.6079, acc 0.9904\n",
      "epoch 831: train D loss: 0.6712, train F loss: -0.6118, acc 0.9900\n",
      "epoch 832: train D loss: 0.6731, train F loss: -0.6093, acc 0.9890\n",
      "epoch 833: train D loss: 0.6655, train F loss: -0.5950, acc 0.9900\n",
      "epoch 834: train D loss: 0.6744, train F loss: -0.6035, acc 0.9858\n",
      "epoch 835: train D loss: 0.6703, train F loss: -0.6061, acc 0.9880\n",
      "epoch 836: train D loss: 0.6692, train F loss: -0.6049, acc 0.9894\n",
      "epoch 837: train D loss: 0.6703, train F loss: -0.6079, acc 0.9890\n",
      "epoch 838: train D loss: 0.6696, train F loss: -0.5887, acc 0.9908\n",
      "epoch 839: train D loss: 0.6683, train F loss: -0.5634, acc 0.9816\n",
      "epoch 840: train D loss: 0.6707, train F loss: -0.6053, acc 0.9876\n",
      "epoch 841: train D loss: 0.6652, train F loss: -0.6100, acc 0.9908\n",
      "epoch 842: train D loss: 0.6666, train F loss: -0.6113, acc 0.9924\n",
      "epoch 843: train D loss: 0.6646, train F loss: -0.5997, acc 0.9866\n",
      "epoch 844: train D loss: 0.6707, train F loss: -0.6123, acc 0.9890\n",
      "epoch 845: train D loss: 0.6684, train F loss: -0.6063, acc 0.9880\n",
      "epoch 846: train D loss: 0.6664, train F loss: -0.6173, acc 0.9926\n",
      "epoch 847: train D loss: 0.6731, train F loss: -0.6162, acc 0.9908\n",
      "epoch 848: train D loss: 0.6667, train F loss: -0.6133, acc 0.9906\n",
      "epoch 849: train D loss: 0.6669, train F loss: -0.6052, acc 0.9908\n",
      "epoch 850: train D loss: 0.6747, train F loss: -0.6101, acc 0.9884\n",
      "epoch 851: train D loss: 0.6634, train F loss: -0.5927, acc 0.9872\n",
      "epoch 852: train D loss: 0.6731, train F loss: -0.6076, acc 0.9896\n",
      "epoch 853: train D loss: 0.6730, train F loss: -0.6121, acc 0.9884\n",
      "epoch 854: train D loss: 0.6659, train F loss: -0.6023, acc 0.9886\n",
      "epoch 855: train D loss: 0.6633, train F loss: -0.5978, acc 0.9876\n",
      "epoch 856: train D loss: 0.6725, train F loss: -0.6176, acc 0.9912\n",
      "epoch 857: train D loss: 0.6664, train F loss: -0.5939, acc 0.9888\n",
      "epoch 858: train D loss: 0.6580, train F loss: -0.5664, acc 0.9860\n",
      "epoch 859: train D loss: 0.6678, train F loss: -0.6056, acc 0.9896\n",
      "epoch 860: train D loss: 0.6627, train F loss: -0.6049, acc 0.9894\n",
      "epoch 861: train D loss: 0.6623, train F loss: -0.5971, acc 0.9882\n",
      "epoch 862: train D loss: 0.6678, train F loss: -0.6022, acc 0.9880\n",
      "epoch 863: train D loss: 0.6685, train F loss: -0.6030, acc 0.9866\n",
      "epoch 864: train D loss: 0.6694, train F loss: -0.6065, acc 0.9884\n",
      "epoch 865: train D loss: 0.6631, train F loss: -0.6107, acc 0.9912\n",
      "epoch 866: train D loss: 0.6655, train F loss: -0.6051, acc 0.9896\n",
      "epoch 867: train D loss: 0.6645, train F loss: -0.6112, acc 0.9904\n",
      "epoch 868: train D loss: 0.6685, train F loss: -0.6116, acc 0.9914\n",
      "epoch 869: train D loss: 0.6666, train F loss: -0.6089, acc 0.9890\n",
      "epoch 870: train D loss: 0.6675, train F loss: -0.6091, acc 0.9908\n",
      "epoch 871: train D loss: 0.6651, train F loss: -0.6128, acc 0.9906\n",
      "epoch 872: train D loss: 0.6668, train F loss: -0.6108, acc 0.9904\n",
      "epoch 873: train D loss: 0.6676, train F loss: -0.6105, acc 0.9916\n",
      "epoch 874: train D loss: 0.6780, train F loss: -0.6213, acc 0.9902\n",
      "epoch 875: train D loss: 0.6703, train F loss: -0.6129, acc 0.9888\n",
      "epoch 876: train D loss: 0.6675, train F loss: -0.6111, acc 0.9900\n",
      "epoch 877: train D loss: 0.6693, train F loss: -0.6066, acc 0.9874\n",
      "epoch 878: train D loss: 0.6724, train F loss: -0.6122, acc 0.9878\n",
      "epoch 879: train D loss: 0.6690, train F loss: -0.6156, acc 0.9900\n",
      "epoch 880: train D loss: 0.6723, train F loss: -0.6187, acc 0.9912\n",
      "epoch 881: train D loss: 0.6604, train F loss: -0.5995, acc 0.9908\n",
      "epoch 882: train D loss: 0.6689, train F loss: -0.6070, acc 0.9888\n",
      "epoch 883: train D loss: 0.6757, train F loss: -0.6175, acc 0.9906\n",
      "epoch 884: train D loss: 0.6698, train F loss: -0.6043, acc 0.9868\n",
      "epoch 885: train D loss: 0.6692, train F loss: -0.6121, acc 0.9872\n",
      "epoch 886: train D loss: 0.6678, train F loss: -0.6064, acc 0.9894\n",
      "epoch 887: train D loss: 0.6694, train F loss: -0.5944, acc 0.9878\n",
      "epoch 888: train D loss: 0.6686, train F loss: -0.6150, acc 0.9920\n",
      "epoch 889: train D loss: 0.6737, train F loss: -0.6167, acc 0.9894\n",
      "epoch 890: train D loss: 0.6709, train F loss: -0.6168, acc 0.9896\n",
      "epoch 891: train D loss: 0.6687, train F loss: -0.6131, acc 0.9892\n",
      "epoch 892: train D loss: 0.6693, train F loss: -0.6116, acc 0.9896\n",
      "epoch 893: train D loss: 0.6673, train F loss: -0.6098, acc 0.9894\n",
      "epoch 894: train D loss: 0.6721, train F loss: -0.6064, acc 0.9880\n",
      "epoch 895: train D loss: 0.6732, train F loss: -0.6159, acc 0.9902\n",
      "epoch 896: train D loss: 0.6715, train F loss: -0.6208, acc 0.9900\n",
      "epoch 897: train D loss: 0.6691, train F loss: -0.6137, acc 0.9904\n",
      "epoch 898: train D loss: 0.6706, train F loss: -0.6133, acc 0.9880\n",
      "epoch 899: train D loss: 0.6662, train F loss: -0.6200, acc 0.9912\n",
      "epoch 900: train D loss: 0.6697, train F loss: -0.6019, acc 0.9880\n",
      "epoch 901: train D loss: 0.6659, train F loss: -0.6083, acc 0.9880\n",
      "epoch 902: train D loss: 0.6682, train F loss: -0.6049, acc 0.9870\n",
      "epoch 903: train D loss: 0.6692, train F loss: -0.6191, acc 0.9914\n",
      "epoch 904: train D loss: 0.6650, train F loss: -0.6091, acc 0.9904\n",
      "epoch 905: train D loss: 0.6681, train F loss: -0.6148, acc 0.9902\n",
      "epoch 906: train D loss: 0.6709, train F loss: -0.5923, acc 0.9840\n",
      "epoch 907: train D loss: 0.6650, train F loss: -0.6093, acc 0.9894\n",
      "epoch 908: train D loss: 0.6650, train F loss: -0.6055, acc 0.9890\n",
      "epoch 909: train D loss: 0.6646, train F loss: -0.6131, acc 0.9898\n",
      "epoch 910: train D loss: 0.6673, train F loss: -0.5960, acc 0.9878\n",
      "epoch 911: train D loss: 0.6690, train F loss: -0.6095, acc 0.9890\n",
      "epoch 912: train D loss: 0.6655, train F loss: -0.6121, acc 0.9902\n",
      "epoch 913: train D loss: 0.6694, train F loss: -0.6026, acc 0.9888\n",
      "epoch 914: train D loss: 0.6668, train F loss: -0.6137, acc 0.9912\n",
      "epoch 915: train D loss: 0.6703, train F loss: -0.6107, acc 0.9886\n",
      "epoch 916: train D loss: 0.6735, train F loss: -0.6126, acc 0.9872\n",
      "epoch 917: train D loss: 0.6711, train F loss: -0.6073, acc 0.9862\n",
      "epoch 918: train D loss: 0.6672, train F loss: -0.6196, acc 0.9906\n",
      "epoch 919: train D loss: 0.6688, train F loss: -0.6166, acc 0.9910\n",
      "epoch 920: train D loss: 0.6676, train F loss: -0.6068, acc 0.9890\n",
      "epoch 921: train D loss: 0.6736, train F loss: -0.6114, acc 0.9888\n",
      "epoch 922: train D loss: 0.6666, train F loss: -0.5935, acc 0.9872\n",
      "epoch 923: train D loss: 0.6656, train F loss: -0.6123, acc 0.9916\n",
      "epoch 924: train D loss: 0.6638, train F loss: -0.6123, acc 0.9906\n",
      "epoch 925: train D loss: 0.6694, train F loss: -0.6091, acc 0.9880\n",
      "epoch 926: train D loss: 0.6740, train F loss: -0.6252, acc 0.9894\n",
      "epoch 927: train D loss: 0.6691, train F loss: -0.6149, acc 0.9912\n",
      "epoch 928: train D loss: 0.6681, train F loss: -0.6218, acc 0.9916\n",
      "epoch 929: train D loss: 0.6700, train F loss: -0.6150, acc 0.9882\n",
      "epoch 930: train D loss: 0.6639, train F loss: -0.6024, acc 0.9884\n",
      "epoch 931: train D loss: 0.6628, train F loss: -0.6040, acc 0.9894\n",
      "epoch 932: train D loss: 0.6755, train F loss: -0.6224, acc 0.9896\n",
      "epoch 933: train D loss: 0.6694, train F loss: -0.6223, acc 0.9914\n",
      "epoch 934: train D loss: 0.6660, train F loss: -0.6184, acc 0.9920\n",
      "epoch 935: train D loss: 0.6722, train F loss: -0.6157, acc 0.9878\n",
      "epoch 936: train D loss: 0.6678, train F loss: -0.6159, acc 0.9894\n",
      "epoch 937: train D loss: 0.6697, train F loss: -0.6187, acc 0.9894\n",
      "epoch 938: train D loss: 0.6681, train F loss: -0.6207, acc 0.9912\n",
      "epoch 939: train D loss: 0.6713, train F loss: -0.6195, acc 0.9910\n",
      "epoch 940: train D loss: 0.6738, train F loss: -0.6191, acc 0.9896\n",
      "epoch 941: train D loss: 0.6743, train F loss: -0.6193, acc 0.9890\n",
      "epoch 942: train D loss: 0.6698, train F loss: -0.6149, acc 0.9896\n",
      "epoch 943: train D loss: 0.6717, train F loss: -0.6145, acc 0.9880\n",
      "epoch 944: train D loss: 0.6690, train F loss: -0.6120, acc 0.9874\n",
      "epoch 945: train D loss: 0.6723, train F loss: -0.6199, acc 0.9888\n",
      "epoch 946: train D loss: 0.6722, train F loss: -0.6159, acc 0.9888\n",
      "epoch 947: train D loss: 0.6707, train F loss: -0.6260, acc 0.9900\n",
      "epoch 948: train D loss: 0.6733, train F loss: -0.6151, acc 0.9886\n",
      "epoch 949: train D loss: 0.6703, train F loss: -0.6230, acc 0.9904\n",
      "epoch 950: train D loss: 0.6641, train F loss: -0.6227, acc 0.9926\n",
      "epoch 951: train D loss: 0.6660, train F loss: -0.6081, acc 0.9884\n",
      "epoch 952: train D loss: 0.6684, train F loss: -0.6148, acc 0.9900\n",
      "epoch 953: train D loss: 0.6703, train F loss: -0.6170, acc 0.9902\n",
      "epoch 954: train D loss: 0.6712, train F loss: -0.6208, acc 0.9900\n",
      "epoch 955: train D loss: 0.6744, train F loss: -0.6197, acc 0.9890\n",
      "epoch 956: train D loss: 0.6728, train F loss: -0.6282, acc 0.9924\n",
      "epoch 957: train D loss: 0.6684, train F loss: -0.6200, acc 0.9908\n",
      "epoch 958: train D loss: 0.6710, train F loss: -0.6216, acc 0.9904\n",
      "epoch 959: train D loss: 0.6791, train F loss: -0.6393, acc 0.9932\n",
      "epoch 960: train D loss: 0.6745, train F loss: -0.6293, acc 0.9908\n",
      "epoch 961: train D loss: 0.6703, train F loss: -0.6154, acc 0.9892\n",
      "epoch 962: train D loss: 0.6719, train F loss: -0.6211, acc 0.9896\n",
      "epoch 963: train D loss: 0.6713, train F loss: -0.6225, acc 0.9916\n",
      "epoch 964: train D loss: 0.6744, train F loss: -0.6200, acc 0.9882\n",
      "epoch 965: train D loss: 0.6687, train F loss: -0.6085, acc 0.9886\n",
      "epoch 966: train D loss: 0.6675, train F loss: -0.6160, acc 0.9902\n",
      "epoch 967: train D loss: 0.6706, train F loss: -0.6189, acc 0.9892\n",
      "epoch 968: train D loss: 0.6721, train F loss: -0.6294, acc 0.9926\n",
      "epoch 969: train D loss: 0.6733, train F loss: -0.6197, acc 0.9896\n",
      "epoch 970: train D loss: 0.6697, train F loss: -0.6248, acc 0.9910\n",
      "epoch 971: train D loss: 0.6760, train F loss: -0.6330, acc 0.9916\n",
      "epoch 972: train D loss: 0.6731, train F loss: -0.6281, acc 0.9904\n",
      "epoch 973: train D loss: 0.6707, train F loss: -0.6080, acc 0.9880\n",
      "epoch 974: train D loss: 0.6691, train F loss: -0.6147, acc 0.9884\n",
      "epoch 975: train D loss: 0.6740, train F loss: -0.6268, acc 0.9908\n",
      "epoch 976: train D loss: 0.6693, train F loss: -0.6275, acc 0.9910\n",
      "epoch 977: train D loss: 0.6726, train F loss: -0.6250, acc 0.9900\n",
      "epoch 978: train D loss: 0.6706, train F loss: -0.6224, acc 0.9910\n",
      "epoch 979: train D loss: 0.6698, train F loss: -0.6141, acc 0.9896\n",
      "epoch 980: train D loss: 0.6713, train F loss: -0.6181, acc 0.9894\n",
      "epoch 981: train D loss: 0.6699, train F loss: -0.6216, acc 0.9914\n",
      "epoch 982: train D loss: 0.6723, train F loss: -0.6065, acc 0.9866\n",
      "epoch 983: train D loss: 0.6695, train F loss: -0.6220, acc 0.9906\n",
      "epoch 984: train D loss: 0.6720, train F loss: -0.6215, acc 0.9898\n",
      "epoch 985: train D loss: 0.6775, train F loss: -0.6263, acc 0.9900\n",
      "epoch 986: train D loss: 0.6713, train F loss: -0.6253, acc 0.9916\n",
      "epoch 987: train D loss: 0.6696, train F loss: -0.6206, acc 0.9894\n",
      "epoch 988: train D loss: 0.6727, train F loss: -0.6301, acc 0.9922\n",
      "epoch 989: train D loss: 0.6729, train F loss: -0.6156, acc 0.9882\n",
      "epoch 990: train D loss: 0.6717, train F loss: -0.6158, acc 0.9894\n",
      "epoch 991: train D loss: 0.6638, train F loss: -0.6160, acc 0.9900\n",
      "epoch 992: train D loss: 0.6730, train F loss: -0.6256, acc 0.9898\n",
      "epoch 993: train D loss: 0.6724, train F loss: -0.6155, acc 0.9894\n",
      "epoch 994: train D loss: 0.6701, train F loss: -0.6248, acc 0.9900\n",
      "epoch 995: train D loss: 0.6735, train F loss: -0.6233, acc 0.9916\n",
      "epoch 996: train D loss: 0.6702, train F loss: -0.6232, acc 0.9908\n",
      "epoch 997: train D loss: 0.6683, train F loss: -0.6256, acc 0.9914\n",
      "epoch 998: train D loss: 0.6643, train F loss: -0.6126, acc 0.9906\n",
      "epoch 999: train D loss: 0.6717, train F loss: -0.6265, acc 0.9918\n",
      "epoch 1000: train D loss: 0.6776, train F loss: -0.6357, acc 0.9914\n",
      "epoch 1001: train D loss: 0.6736, train F loss: -0.6312, acc 0.9908\n",
      "epoch 1002: train D loss: 0.6750, train F loss: -0.6341, acc 0.9922\n",
      "epoch 1003: train D loss: 0.6728, train F loss: -0.6314, acc 0.9916\n",
      "epoch 1004: train D loss: 0.6744, train F loss: -0.6164, acc 0.9890\n",
      "epoch 1005: train D loss: 0.6693, train F loss: -0.6187, acc 0.9902\n",
      "epoch 1006: train D loss: 0.6727, train F loss: -0.6136, acc 0.9860\n",
      "epoch 1007: train D loss: 0.6686, train F loss: -0.6288, acc 0.9920\n",
      "epoch 1008: train D loss: 0.6693, train F loss: -0.6214, acc 0.9914\n",
      "epoch 1009: train D loss: 0.6658, train F loss: -0.6174, acc 0.9910\n",
      "epoch 1010: train D loss: 0.6711, train F loss: -0.6240, acc 0.9902\n",
      "epoch 1011: train D loss: 0.6773, train F loss: -0.6248, acc 0.9882\n",
      "epoch 1012: train D loss: 0.6704, train F loss: -0.6239, acc 0.9896\n",
      "epoch 1013: train D loss: 0.6660, train F loss: -0.6171, acc 0.9898\n",
      "epoch 1014: train D loss: 0.6704, train F loss: -0.6092, acc 0.9876\n",
      "epoch 1015: train D loss: 0.6741, train F loss: -0.6290, acc 0.9886\n",
      "epoch 1016: train D loss: 0.6715, train F loss: -0.6310, acc 0.9904\n",
      "epoch 1017: train D loss: 0.6737, train F loss: -0.6080, acc 0.9872\n",
      "epoch 1018: train D loss: 0.6678, train F loss: -0.6260, acc 0.9912\n",
      "epoch 1019: train D loss: 0.6728, train F loss: -0.6268, acc 0.9904\n",
      "epoch 1020: train D loss: 0.6773, train F loss: -0.6311, acc 0.9918\n",
      "epoch 1021: train D loss: 0.6711, train F loss: -0.6230, acc 0.9886\n",
      "epoch 1022: train D loss: 0.6713, train F loss: -0.6306, acc 0.9916\n",
      "epoch 1023: train D loss: 0.6721, train F loss: -0.6219, acc 0.9886\n",
      "epoch 1024: train D loss: 0.6703, train F loss: -0.6176, acc 0.9912\n",
      "epoch 1025: train D loss: 0.6706, train F loss: -0.6221, acc 0.9900\n",
      "epoch 1026: train D loss: 0.6691, train F loss: -0.6210, acc 0.9902\n",
      "epoch 1027: train D loss: 0.6706, train F loss: -0.6295, acc 0.9912\n",
      "epoch 1028: train D loss: 0.6778, train F loss: -0.6296, acc 0.9904\n",
      "epoch 1029: train D loss: 0.6723, train F loss: -0.6280, acc 0.9896\n",
      "epoch 1030: train D loss: 0.6710, train F loss: -0.6238, acc 0.9884\n",
      "epoch 1031: train D loss: 0.6701, train F loss: -0.6204, acc 0.9894\n",
      "epoch 1032: train D loss: 0.6690, train F loss: -0.6213, acc 0.9916\n",
      "epoch 1033: train D loss: 0.6711, train F loss: -0.6175, acc 0.9888\n",
      "epoch 1034: train D loss: 0.6709, train F loss: -0.6106, acc 0.9870\n",
      "epoch 1035: train D loss: 0.6686, train F loss: -0.6185, acc 0.9892\n",
      "epoch 1036: train D loss: 0.6691, train F loss: -0.6304, acc 0.9918\n",
      "epoch 1037: train D loss: 0.6691, train F loss: -0.6153, acc 0.9886\n",
      "epoch 1038: train D loss: 0.6737, train F loss: -0.6292, acc 0.9912\n",
      "epoch 1039: train D loss: 0.6711, train F loss: -0.6234, acc 0.9890\n",
      "epoch 1040: train D loss: 0.6711, train F loss: -0.6245, acc 0.9908\n",
      "epoch 1041: train D loss: 0.6699, train F loss: -0.6276, acc 0.9908\n",
      "epoch 1042: train D loss: 0.6749, train F loss: -0.6220, acc 0.9878\n",
      "epoch 1043: train D loss: 0.6670, train F loss: -0.6266, acc 0.9904\n",
      "epoch 1044: train D loss: 0.6736, train F loss: -0.6242, acc 0.9914\n",
      "epoch 1045: train D loss: 0.6745, train F loss: -0.6012, acc 0.9838\n",
      "epoch 1046: train D loss: 0.6663, train F loss: -0.6121, acc 0.9870\n",
      "epoch 1047: train D loss: 0.6660, train F loss: -0.6225, acc 0.9920\n",
      "epoch 1048: train D loss: 0.6682, train F loss: -0.6222, acc 0.9902\n",
      "epoch 1049: train D loss: 0.6743, train F loss: -0.6251, acc 0.9904\n",
      "epoch 1050: train D loss: 0.6666, train F loss: -0.6240, acc 0.9900\n",
      "epoch 1051: train D loss: 0.6667, train F loss: -0.6143, acc 0.9896\n",
      "epoch 1052: train D loss: 0.6740, train F loss: -0.6322, acc 0.9908\n",
      "epoch 1053: train D loss: 0.6717, train F loss: -0.6202, acc 0.9892\n",
      "epoch 1054: train D loss: 0.6698, train F loss: -0.6309, acc 0.9926\n",
      "epoch 1055: train D loss: 0.6679, train F loss: -0.6268, acc 0.9918\n",
      "epoch 1056: train D loss: 0.6738, train F loss: -0.6136, acc 0.9894\n",
      "epoch 1057: train D loss: 0.6636, train F loss: -0.6231, acc 0.9910\n",
      "epoch 1058: train D loss: 0.6709, train F loss: -0.6228, acc 0.9908\n",
      "epoch 1059: train D loss: 0.6668, train F loss: -0.6188, acc 0.9884\n",
      "epoch 1060: train D loss: 0.6654, train F loss: -0.6169, acc 0.9874\n",
      "epoch 1061: train D loss: 0.6698, train F loss: -0.6203, acc 0.9900\n",
      "epoch 1062: train D loss: 0.6641, train F loss: -0.6220, acc 0.9910\n",
      "epoch 1063: train D loss: 0.6699, train F loss: -0.6248, acc 0.9904\n",
      "epoch 1064: train D loss: 0.6696, train F loss: -0.6217, acc 0.9894\n",
      "epoch 1065: train D loss: 0.6723, train F loss: -0.6267, acc 0.9920\n",
      "epoch 1066: train D loss: 0.6710, train F loss: -0.6128, acc 0.9876\n",
      "epoch 1067: train D loss: 0.6684, train F loss: -0.6259, acc 0.9894\n",
      "epoch 1068: train D loss: 0.6755, train F loss: -0.6302, acc 0.9892\n",
      "epoch 1069: train D loss: 0.6725, train F loss: -0.6276, acc 0.9912\n",
      "epoch 1070: train D loss: 0.6760, train F loss: -0.6271, acc 0.9890\n",
      "epoch 1071: train D loss: 0.6715, train F loss: -0.6289, acc 0.9904\n",
      "epoch 1072: train D loss: 0.6707, train F loss: -0.6274, acc 0.9908\n",
      "epoch 1073: train D loss: 0.6760, train F loss: -0.6308, acc 0.9908\n",
      "epoch 1074: train D loss: 0.6690, train F loss: -0.6223, acc 0.9890\n",
      "epoch 1075: train D loss: 0.6721, train F loss: -0.6264, acc 0.9894\n",
      "epoch 1076: train D loss: 0.6695, train F loss: -0.6171, acc 0.9902\n",
      "epoch 1077: train D loss: 0.6705, train F loss: -0.6137, acc 0.9868\n",
      "epoch 1078: train D loss: 0.6694, train F loss: -0.6356, acc 0.9936\n",
      "epoch 1079: train D loss: 0.6736, train F loss: -0.6320, acc 0.9906\n",
      "epoch 1080: train D loss: 0.6722, train F loss: -0.6253, acc 0.9896\n",
      "epoch 1081: train D loss: 0.6695, train F loss: -0.6337, acc 0.9928\n",
      "epoch 1082: train D loss: 0.6744, train F loss: -0.6291, acc 0.9898\n",
      "epoch 1083: train D loss: 0.6718, train F loss: -0.6255, acc 0.9892\n",
      "epoch 1084: train D loss: 0.6716, train F loss: -0.6325, acc 0.9918\n",
      "epoch 1085: train D loss: 0.6724, train F loss: -0.6349, acc 0.9916\n",
      "epoch 1086: train D loss: 0.6729, train F loss: -0.6314, acc 0.9916\n",
      "epoch 1087: train D loss: 0.6709, train F loss: -0.6308, acc 0.9906\n",
      "epoch 1088: train D loss: 0.6737, train F loss: -0.6324, acc 0.9904\n",
      "epoch 1089: train D loss: 0.6740, train F loss: -0.6299, acc 0.9912\n",
      "epoch 1090: train D loss: 0.6775, train F loss: -0.6322, acc 0.9876\n",
      "epoch 1091: train D loss: 0.6738, train F loss: -0.6387, acc 0.9922\n",
      "epoch 1092: train D loss: 0.6747, train F loss: -0.6391, acc 0.9918\n",
      "epoch 1093: train D loss: 0.6761, train F loss: -0.6181, acc 0.9866\n",
      "epoch 1094: train D loss: 0.6712, train F loss: -0.6241, acc 0.9900\n",
      "epoch 1095: train D loss: 0.6700, train F loss: -0.6166, acc 0.9890\n",
      "epoch 1096: train D loss: 0.6704, train F loss: -0.6221, acc 0.9906\n",
      "epoch 1097: train D loss: 0.6701, train F loss: -0.6303, acc 0.9910\n",
      "epoch 1098: train D loss: 0.6754, train F loss: -0.6279, acc 0.9902\n",
      "epoch 1099: train D loss: 0.6716, train F loss: -0.6297, acc 0.9914\n",
      "epoch 1100: train D loss: 0.6665, train F loss: -0.6223, acc 0.9902\n",
      "epoch 1101: train D loss: 0.6782, train F loss: -0.6276, acc 0.9892\n",
      "epoch 1102: train D loss: 0.6689, train F loss: -0.6248, acc 0.9892\n",
      "epoch 1103: train D loss: 0.6667, train F loss: -0.6278, acc 0.9902\n",
      "epoch 1104: train D loss: 0.6732, train F loss: -0.6309, acc 0.9904\n",
      "epoch 1105: train D loss: 0.6732, train F loss: -0.6276, acc 0.9898\n",
      "epoch 1106: train D loss: 0.6693, train F loss: -0.6228, acc 0.9886\n",
      "epoch 1107: train D loss: 0.6729, train F loss: -0.6279, acc 0.9896\n",
      "epoch 1108: train D loss: 0.6744, train F loss: -0.6328, acc 0.9904\n",
      "epoch 1109: train D loss: 0.6721, train F loss: -0.6341, acc 0.9924\n",
      "epoch 1110: train D loss: 0.6709, train F loss: -0.6267, acc 0.9912\n",
      "epoch 1111: train D loss: 0.6752, train F loss: -0.6366, acc 0.9912\n",
      "epoch 1112: train D loss: 0.6768, train F loss: -0.6274, acc 0.9896\n",
      "epoch 1113: train D loss: 0.6715, train F loss: -0.6097, acc 0.9872\n",
      "epoch 1114: train D loss: 0.6696, train F loss: -0.6174, acc 0.9888\n",
      "epoch 1115: train D loss: 0.6734, train F loss: -0.6273, acc 0.9900\n",
      "epoch 1116: train D loss: 0.6674, train F loss: -0.6349, acc 0.9924\n",
      "epoch 1117: train D loss: 0.6724, train F loss: -0.6339, acc 0.9912\n",
      "epoch 1118: train D loss: 0.6723, train F loss: -0.6168, acc 0.9872\n",
      "epoch 1119: train D loss: 0.6748, train F loss: -0.6299, acc 0.9896\n",
      "epoch 1120: train D loss: 0.6747, train F loss: -0.6314, acc 0.9898\n",
      "epoch 1121: train D loss: 0.6708, train F loss: -0.6297, acc 0.9906\n",
      "epoch 1122: train D loss: 0.6705, train F loss: -0.6300, acc 0.9894\n",
      "epoch 1123: train D loss: 0.6670, train F loss: -0.6283, acc 0.9912\n",
      "epoch 1124: train D loss: 0.6710, train F loss: -0.6376, acc 0.9928\n",
      "epoch 1125: train D loss: 0.6707, train F loss: -0.6343, acc 0.9922\n",
      "epoch 1126: train D loss: 0.6726, train F loss: -0.6305, acc 0.9898\n",
      "epoch 1127: train D loss: 0.6730, train F loss: -0.6234, acc 0.9904\n",
      "epoch 1128: train D loss: 0.6703, train F loss: -0.6286, acc 0.9918\n",
      "epoch 1129: train D loss: 0.6689, train F loss: -0.6239, acc 0.9898\n",
      "epoch 1130: train D loss: 0.6718, train F loss: -0.6297, acc 0.9912\n",
      "epoch 1131: train D loss: 0.6674, train F loss: -0.6367, acc 0.9932\n",
      "epoch 1132: train D loss: 0.6745, train F loss: -0.6331, acc 0.9908\n",
      "epoch 1133: train D loss: 0.6729, train F loss: -0.6304, acc 0.9910\n",
      "epoch 1134: train D loss: 0.6709, train F loss: -0.6371, acc 0.9916\n",
      "epoch 1135: train D loss: 0.6742, train F loss: -0.6331, acc 0.9896\n",
      "epoch 1136: train D loss: 0.6683, train F loss: -0.6277, acc 0.9918\n",
      "epoch 1137: train D loss: 0.6746, train F loss: -0.6311, acc 0.9912\n",
      "epoch 1138: train D loss: 0.6765, train F loss: -0.6399, acc 0.9912\n",
      "epoch 1139: train D loss: 0.6712, train F loss: -0.6244, acc 0.9912\n",
      "epoch 1140: train D loss: 0.6770, train F loss: -0.6289, acc 0.9900\n",
      "epoch 1141: train D loss: 0.6704, train F loss: -0.6330, acc 0.9900\n",
      "epoch 1142: train D loss: 0.6745, train F loss: -0.6310, acc 0.9916\n",
      "epoch 1143: train D loss: 0.6721, train F loss: -0.6349, acc 0.9910\n",
      "epoch 1144: train D loss: 0.6701, train F loss: -0.6240, acc 0.9918\n",
      "epoch 1145: train D loss: 0.6714, train F loss: -0.6216, acc 0.9898\n",
      "epoch 1146: train D loss: 0.6725, train F loss: -0.6241, acc 0.9898\n",
      "epoch 1147: train D loss: 0.6725, train F loss: -0.6299, acc 0.9906\n",
      "epoch 1148: train D loss: 0.6760, train F loss: -0.6306, acc 0.9892\n",
      "epoch 1149: train D loss: 0.6728, train F loss: -0.6251, acc 0.9904\n",
      "epoch 1150: train D loss: 0.6706, train F loss: -0.6332, acc 0.9918\n",
      "epoch 1151: train D loss: 0.6677, train F loss: -0.6240, acc 0.9904\n",
      "epoch 1152: train D loss: 0.6718, train F loss: -0.6200, acc 0.9892\n",
      "epoch 1153: train D loss: 0.6730, train F loss: -0.6343, acc 0.9924\n",
      "epoch 1154: train D loss: 0.6753, train F loss: -0.6353, acc 0.9908\n",
      "epoch 1155: train D loss: 0.6728, train F loss: -0.6326, acc 0.9918\n",
      "epoch 1156: train D loss: 0.6761, train F loss: -0.6329, acc 0.9912\n",
      "epoch 1157: train D loss: 0.6720, train F loss: -0.6218, acc 0.9884\n",
      "epoch 1158: train D loss: 0.6730, train F loss: -0.6260, acc 0.9890\n",
      "epoch 1159: train D loss: 0.6703, train F loss: -0.6318, acc 0.9922\n",
      "epoch 1160: train D loss: 0.6731, train F loss: -0.6302, acc 0.9902\n",
      "epoch 1161: train D loss: 0.6682, train F loss: -0.6261, acc 0.9930\n",
      "epoch 1162: train D loss: 0.6703, train F loss: -0.6355, acc 0.9920\n",
      "epoch 1163: train D loss: 0.6739, train F loss: -0.6230, acc 0.9880\n",
      "epoch 1164: train D loss: 0.6719, train F loss: -0.6354, acc 0.9918\n",
      "epoch 1165: train D loss: 0.6705, train F loss: -0.6390, acc 0.9934\n",
      "epoch 1166: train D loss: 0.6724, train F loss: -0.6420, acc 0.9918\n",
      "epoch 1167: train D loss: 0.6732, train F loss: -0.6346, acc 0.9916\n",
      "epoch 1168: train D loss: 0.6703, train F loss: -0.6190, acc 0.9890\n",
      "epoch 1169: train D loss: 0.6761, train F loss: -0.6402, acc 0.9914\n",
      "epoch 1170: train D loss: 0.6710, train F loss: -0.6254, acc 0.9906\n",
      "epoch 1171: train D loss: 0.6743, train F loss: -0.6391, acc 0.9922\n",
      "epoch 1172: train D loss: 0.6742, train F loss: -0.6370, acc 0.9922\n",
      "epoch 1173: train D loss: 0.6738, train F loss: -0.6373, acc 0.9914\n",
      "epoch 1174: train D loss: 0.6746, train F loss: -0.6356, acc 0.9920\n",
      "epoch 1175: train D loss: 0.6750, train F loss: -0.6373, acc 0.9906\n",
      "epoch 1176: train D loss: 0.6759, train F loss: -0.6310, acc 0.9904\n",
      "epoch 1177: train D loss: 0.6753, train F loss: -0.6337, acc 0.9914\n",
      "epoch 1178: train D loss: 0.6762, train F loss: -0.6233, acc 0.9898\n",
      "epoch 1179: train D loss: 0.6722, train F loss: -0.6253, acc 0.9880\n",
      "epoch 1180: train D loss: 0.6776, train F loss: -0.6441, acc 0.9916\n",
      "epoch 1181: train D loss: 0.6724, train F loss: -0.6322, acc 0.9888\n",
      "epoch 1182: train D loss: 0.6748, train F loss: -0.6439, acc 0.9922\n",
      "epoch 1183: train D loss: 0.6729, train F loss: -0.6301, acc 0.9926\n",
      "epoch 1184: train D loss: 0.6730, train F loss: -0.6331, acc 0.9908\n",
      "epoch 1185: train D loss: 0.6739, train F loss: -0.6369, acc 0.9914\n",
      "epoch 1186: train D loss: 0.6730, train F loss: -0.6294, acc 0.9906\n",
      "epoch 1187: train D loss: 0.6728, train F loss: -0.6313, acc 0.9890\n",
      "epoch 1188: train D loss: 0.6690, train F loss: -0.6283, acc 0.9912\n",
      "epoch 1189: train D loss: 0.6729, train F loss: -0.6311, acc 0.9904\n",
      "epoch 1190: train D loss: 0.6763, train F loss: -0.6426, acc 0.9906\n",
      "epoch 1191: train D loss: 0.6757, train F loss: -0.6405, acc 0.9904\n",
      "epoch 1192: train D loss: 0.6775, train F loss: -0.6386, acc 0.9898\n",
      "epoch 1193: train D loss: 0.6757, train F loss: -0.6445, acc 0.9932\n",
      "epoch 1194: train D loss: 0.6748, train F loss: -0.6382, acc 0.9906\n",
      "epoch 1195: train D loss: 0.6713, train F loss: -0.6222, acc 0.9908\n",
      "epoch 1196: train D loss: 0.6704, train F loss: -0.6348, acc 0.9914\n",
      "epoch 1197: train D loss: 0.6750, train F loss: -0.6354, acc 0.9882\n",
      "epoch 1198: train D loss: 0.6761, train F loss: -0.6389, acc 0.9914\n",
      "epoch 1199: train D loss: 0.6755, train F loss: -0.6336, acc 0.9906\n",
      "epoch 1200: train D loss: 0.6728, train F loss: -0.6346, acc 0.9928\n",
      "epoch 1201: train D loss: 0.6758, train F loss: -0.6332, acc 0.9902\n",
      "epoch 1202: train D loss: 0.6693, train F loss: -0.6332, acc 0.9920\n",
      "epoch 1203: train D loss: 0.6747, train F loss: -0.6337, acc 0.9900\n",
      "epoch 1204: train D loss: 0.6716, train F loss: -0.6307, acc 0.9900\n",
      "epoch 1205: train D loss: 0.6669, train F loss: -0.6202, acc 0.9902\n",
      "epoch 1206: train D loss: 0.6747, train F loss: -0.6337, acc 0.9912\n",
      "epoch 1207: train D loss: 0.6675, train F loss: -0.6243, acc 0.9912\n",
      "epoch 1208: train D loss: 0.6760, train F loss: -0.6428, acc 0.9932\n",
      "epoch 1209: train D loss: 0.6720, train F loss: -0.6317, acc 0.9898\n",
      "epoch 1210: train D loss: 0.6738, train F loss: -0.6399, acc 0.9924\n",
      "epoch 1211: train D loss: 0.6734, train F loss: -0.6330, acc 0.9894\n",
      "epoch 1212: train D loss: 0.6726, train F loss: -0.6418, acc 0.9930\n",
      "epoch 1213: train D loss: 0.6746, train F loss: -0.6375, acc 0.9926\n",
      "epoch 1214: train D loss: 0.6769, train F loss: -0.6342, acc 0.9902\n",
      "epoch 1215: train D loss: 0.6754, train F loss: -0.6392, acc 0.9914\n",
      "epoch 1216: train D loss: 0.6743, train F loss: -0.6411, acc 0.9920\n",
      "epoch 1217: train D loss: 0.6752, train F loss: -0.6366, acc 0.9906\n",
      "epoch 1218: train D loss: 0.6736, train F loss: -0.6436, acc 0.9934\n",
      "epoch 1219: train D loss: 0.6723, train F loss: -0.6331, acc 0.9914\n",
      "epoch 1220: train D loss: 0.6765, train F loss: -0.6295, acc 0.9878\n",
      "epoch 1221: train D loss: 0.6754, train F loss: -0.6322, acc 0.9900\n",
      "epoch 1222: train D loss: 0.6697, train F loss: -0.6222, acc 0.9918\n",
      "epoch 1223: train D loss: 0.6703, train F loss: -0.6297, acc 0.9912\n",
      "epoch 1224: train D loss: 0.6710, train F loss: -0.6359, acc 0.9922\n",
      "epoch 1225: train D loss: 0.6767, train F loss: -0.6357, acc 0.9902\n",
      "epoch 1226: train D loss: 0.6762, train F loss: -0.6375, acc 0.9904\n",
      "epoch 1227: train D loss: 0.6750, train F loss: -0.6363, acc 0.9914\n",
      "epoch 1228: train D loss: 0.6709, train F loss: -0.6293, acc 0.9908\n",
      "epoch 1229: train D loss: 0.6754, train F loss: -0.6323, acc 0.9898\n",
      "epoch 1230: train D loss: 0.6685, train F loss: -0.6234, acc 0.9892\n",
      "epoch 1231: train D loss: 0.6716, train F loss: -0.6335, acc 0.9896\n",
      "epoch 1232: train D loss: 0.6709, train F loss: -0.6260, acc 0.9904\n",
      "epoch 1233: train D loss: 0.6697, train F loss: -0.6273, acc 0.9900\n",
      "epoch 1234: train D loss: 0.6719, train F loss: -0.6335, acc 0.9918\n",
      "epoch 1235: train D loss: 0.6730, train F loss: -0.6328, acc 0.9918\n",
      "epoch 1236: train D loss: 0.6781, train F loss: -0.6419, acc 0.9902\n",
      "epoch 1237: train D loss: 0.6729, train F loss: -0.6420, acc 0.9926\n",
      "epoch 1238: train D loss: 0.6747, train F loss: -0.6297, acc 0.9904\n",
      "epoch 1239: train D loss: 0.6748, train F loss: -0.6279, acc 0.9912\n",
      "epoch 1240: train D loss: 0.6712, train F loss: -0.6296, acc 0.9892\n",
      "epoch 1241: train D loss: 0.6715, train F loss: -0.6350, acc 0.9916\n",
      "epoch 1242: train D loss: 0.6738, train F loss: -0.6362, acc 0.9912\n",
      "epoch 1243: train D loss: 0.6702, train F loss: -0.6372, acc 0.9924\n",
      "epoch 1244: train D loss: 0.6724, train F loss: -0.6399, acc 0.9918\n",
      "epoch 1245: train D loss: 0.6739, train F loss: -0.6398, acc 0.9906\n",
      "epoch 1246: train D loss: 0.6758, train F loss: -0.6456, acc 0.9930\n",
      "epoch 1247: train D loss: 0.6761, train F loss: -0.6354, acc 0.9918\n",
      "epoch 1248: train D loss: 0.6745, train F loss: -0.6008, acc 0.9850\n",
      "epoch 1249: train D loss: 0.6715, train F loss: -0.5929, acc 0.9882\n",
      "epoch 1250: train D loss: 0.6605, train F loss: -0.6234, acc 0.9924\n",
      "epoch 1251: train D loss: 0.6670, train F loss: -0.6324, acc 0.9920\n",
      "epoch 1252: train D loss: 0.6749, train F loss: -0.6317, acc 0.9898\n",
      "epoch 1253: train D loss: 0.6713, train F loss: -0.6299, acc 0.9894\n",
      "epoch 1254: train D loss: 0.6699, train F loss: -0.6216, acc 0.9880\n",
      "epoch 1255: train D loss: 0.6677, train F loss: -0.6324, acc 0.9926\n",
      "epoch 1256: train D loss: 0.6732, train F loss: -0.6377, acc 0.9920\n",
      "epoch 1257: train D loss: 0.6739, train F loss: -0.6424, acc 0.9934\n",
      "epoch 1258: train D loss: 0.6719, train F loss: -0.6354, acc 0.9916\n",
      "epoch 1259: train D loss: 0.6713, train F loss: -0.6334, acc 0.9918\n",
      "epoch 1260: train D loss: 0.6729, train F loss: -0.6376, acc 0.9916\n",
      "epoch 1261: train D loss: 0.6731, train F loss: -0.6342, acc 0.9916\n",
      "epoch 1262: train D loss: 0.6750, train F loss: -0.6370, acc 0.9912\n",
      "epoch 1263: train D loss: 0.6750, train F loss: -0.6411, acc 0.9924\n",
      "epoch 1264: train D loss: 0.6754, train F loss: -0.6429, acc 0.9924\n",
      "epoch 1265: train D loss: 0.6697, train F loss: -0.5840, acc 0.9862\n",
      "epoch 1266: train D loss: 0.6711, train F loss: -0.6152, acc 0.9866\n",
      "epoch 1267: train D loss: 0.6717, train F loss: -0.6308, acc 0.9894\n",
      "epoch 1268: train D loss: 0.6669, train F loss: -0.6255, acc 0.9902\n",
      "epoch 1269: train D loss: 0.6683, train F loss: -0.6346, acc 0.9906\n",
      "epoch 1270: train D loss: 0.6735, train F loss: -0.6388, acc 0.9908\n",
      "epoch 1271: train D loss: 0.6718, train F loss: -0.6342, acc 0.9918\n",
      "epoch 1272: train D loss: 0.6684, train F loss: -0.6328, acc 0.9912\n",
      "epoch 1273: train D loss: 0.6686, train F loss: -0.6325, acc 0.9918\n",
      "epoch 1274: train D loss: 0.6718, train F loss: -0.6414, acc 0.9934\n",
      "epoch 1275: train D loss: 0.6666, train F loss: -0.6325, acc 0.9922\n",
      "epoch 1276: train D loss: 0.6718, train F loss: -0.6320, acc 0.9916\n",
      "epoch 1277: train D loss: 0.6714, train F loss: -0.6308, acc 0.9908\n",
      "epoch 1278: train D loss: 0.6733, train F loss: -0.6305, acc 0.9892\n",
      "epoch 1279: train D loss: 0.6723, train F loss: -0.6321, acc 0.9894\n",
      "epoch 1280: train D loss: 0.6722, train F loss: -0.6397, acc 0.9930\n",
      "epoch 1281: train D loss: 0.6728, train F loss: -0.6386, acc 0.9902\n",
      "epoch 1282: train D loss: 0.6759, train F loss: -0.6428, acc 0.9930\n",
      "epoch 1283: train D loss: 0.6762, train F loss: -0.6471, acc 0.9922\n",
      "epoch 1284: train D loss: 0.6747, train F loss: -0.6406, acc 0.9906\n",
      "epoch 1285: train D loss: 0.6723, train F loss: -0.6371, acc 0.9922\n",
      "epoch 1286: train D loss: 0.6731, train F loss: -0.6362, acc 0.9918\n",
      "epoch 1287: train D loss: 0.6730, train F loss: -0.6363, acc 0.9904\n",
      "epoch 1288: train D loss: 0.6783, train F loss: -0.6381, acc 0.9894\n",
      "epoch 1289: train D loss: 0.6744, train F loss: -0.6384, acc 0.9914\n",
      "epoch 1290: train D loss: 0.6740, train F loss: -0.6302, acc 0.9912\n",
      "epoch 1291: train D loss: 0.6764, train F loss: -0.6405, acc 0.9918\n",
      "epoch 1292: train D loss: 0.6751, train F loss: -0.6358, acc 0.9902\n",
      "epoch 1293: train D loss: 0.6725, train F loss: -0.6371, acc 0.9916\n",
      "epoch 1294: train D loss: 0.6692, train F loss: -0.6267, acc 0.9906\n",
      "epoch 1295: train D loss: 0.6740, train F loss: -0.6294, acc 0.9884\n",
      "epoch 1296: train D loss: 0.6680, train F loss: -0.6116, acc 0.9906\n",
      "epoch 1297: train D loss: 0.6665, train F loss: -0.6257, acc 0.9904\n",
      "epoch 1298: train D loss: 0.6721, train F loss: -0.6352, acc 0.9914\n",
      "epoch 1299: train D loss: 0.6717, train F loss: -0.6295, acc 0.9902\n",
      "epoch 1300: train D loss: 0.6744, train F loss: -0.6343, acc 0.9910\n",
      "epoch 1301: train D loss: 0.6725, train F loss: -0.6343, acc 0.9910\n",
      "epoch 1302: train D loss: 0.6708, train F loss: -0.6304, acc 0.9902\n",
      "epoch 1303: train D loss: 0.6738, train F loss: -0.6446, acc 0.9944\n",
      "epoch 1304: train D loss: 0.6684, train F loss: -0.6335, acc 0.9920\n",
      "epoch 1305: train D loss: 0.6759, train F loss: -0.6321, acc 0.9890\n",
      "epoch 1306: train D loss: 0.6718, train F loss: -0.6338, acc 0.9904\n",
      "epoch 1307: train D loss: 0.6747, train F loss: -0.6380, acc 0.9918\n",
      "epoch 1308: train D loss: 0.6769, train F loss: -0.6385, acc 0.9904\n",
      "epoch 1309: train D loss: 0.6747, train F loss: -0.6430, acc 0.9938\n",
      "epoch 1310: train D loss: 0.6737, train F loss: -0.6377, acc 0.9910\n",
      "epoch 1311: train D loss: 0.6767, train F loss: -0.6418, acc 0.9924\n",
      "epoch 1312: train D loss: 0.6669, train F loss: -0.6293, acc 0.9916\n",
      "epoch 1313: train D loss: 0.6718, train F loss: -0.6299, acc 0.9906\n",
      "epoch 1314: train D loss: 0.6765, train F loss: -0.6341, acc 0.9894\n",
      "epoch 1315: train D loss: 0.6750, train F loss: -0.6361, acc 0.9906\n",
      "epoch 1316: train D loss: 0.6737, train F loss: -0.6445, acc 0.9938\n",
      "epoch 1317: train D loss: 0.6720, train F loss: -0.6396, acc 0.9922\n",
      "epoch 1318: train D loss: 0.6674, train F loss: -0.6286, acc 0.9904\n",
      "epoch 1319: train D loss: 0.6762, train F loss: -0.6385, acc 0.9928\n",
      "epoch 1320: train D loss: 0.6747, train F loss: -0.6371, acc 0.9906\n",
      "epoch 1321: train D loss: 0.6755, train F loss: -0.6290, acc 0.9866\n",
      "epoch 1322: train D loss: 0.6696, train F loss: -0.6099, acc 0.9878\n",
      "epoch 1323: train D loss: 0.6687, train F loss: -0.6163, acc 0.9868\n",
      "epoch 1324: train D loss: 0.6735, train F loss: -0.6364, acc 0.9912\n",
      "epoch 1325: train D loss: 0.6749, train F loss: -0.6229, acc 0.9858\n",
      "epoch 1326: train D loss: 0.6751, train F loss: -0.6399, acc 0.9908\n",
      "epoch 1327: train D loss: 0.6693, train F loss: -0.6412, acc 0.9938\n",
      "epoch 1328: train D loss: 0.6692, train F loss: -0.6336, acc 0.9930\n",
      "epoch 1329: train D loss: 0.6703, train F loss: -0.6342, acc 0.9916\n",
      "epoch 1330: train D loss: 0.6697, train F loss: -0.6405, acc 0.9940\n",
      "epoch 1331: train D loss: 0.6733, train F loss: -0.6368, acc 0.9920\n",
      "epoch 1332: train D loss: 0.6759, train F loss: -0.6380, acc 0.9916\n",
      "epoch 1333: train D loss: 0.6707, train F loss: -0.6409, acc 0.9924\n",
      "epoch 1334: train D loss: 0.6719, train F loss: -0.6265, acc 0.9902\n",
      "epoch 1335: train D loss: 0.6706, train F loss: -0.6367, acc 0.9922\n",
      "epoch 1336: train D loss: 0.6708, train F loss: -0.6405, acc 0.9932\n",
      "epoch 1337: train D loss: 0.6741, train F loss: -0.6317, acc 0.9904\n",
      "epoch 1338: train D loss: 0.6731, train F loss: -0.6345, acc 0.9922\n",
      "epoch 1339: train D loss: 0.6743, train F loss: -0.6413, acc 0.9916\n",
      "epoch 1340: train D loss: 0.6732, train F loss: -0.6309, acc 0.9888\n",
      "epoch 1341: train D loss: 0.6719, train F loss: -0.6398, acc 0.9930\n",
      "epoch 1342: train D loss: 0.6667, train F loss: -0.6313, acc 0.9924\n",
      "epoch 1343: train D loss: 0.6734, train F loss: -0.6133, acc 0.9904\n",
      "epoch 1344: train D loss: 0.6713, train F loss: -0.6270, acc 0.9902\n",
      "epoch 1345: train D loss: 0.6691, train F loss: -0.6227, acc 0.9898\n",
      "epoch 1346: train D loss: 0.6677, train F loss: -0.6338, acc 0.9912\n",
      "epoch 1347: train D loss: 0.6703, train F loss: -0.6382, acc 0.9930\n",
      "epoch 1348: train D loss: 0.6698, train F loss: -0.6354, acc 0.9916\n",
      "epoch 1349: train D loss: 0.6711, train F loss: -0.6449, acc 0.9944\n",
      "epoch 1350: train D loss: 0.6710, train F loss: -0.6368, acc 0.9930\n",
      "epoch 1351: train D loss: 0.6720, train F loss: -0.6381, acc 0.9920\n",
      "epoch 1352: train D loss: 0.6670, train F loss: -0.6309, acc 0.9906\n",
      "epoch 1353: train D loss: 0.6716, train F loss: -0.6314, acc 0.9904\n",
      "epoch 1354: train D loss: 0.6739, train F loss: -0.6388, acc 0.9920\n",
      "epoch 1355: train D loss: 0.6755, train F loss: -0.6458, acc 0.9936\n",
      "epoch 1356: train D loss: 0.6754, train F loss: -0.6407, acc 0.9914\n",
      "epoch 1357: train D loss: 0.6738, train F loss: -0.6453, acc 0.9938\n",
      "epoch 1358: train D loss: 0.6777, train F loss: -0.6436, acc 0.9910\n",
      "epoch 1359: train D loss: 0.6804, train F loss: -0.6417, acc 0.9896\n",
      "epoch 1360: train D loss: 0.6705, train F loss: -0.6364, acc 0.9912\n",
      "epoch 1361: train D loss: 0.6693, train F loss: -0.6401, acc 0.9940\n",
      "epoch 1362: train D loss: 0.6745, train F loss: -0.6446, acc 0.9916\n",
      "epoch 1363: train D loss: 0.6768, train F loss: -0.6313, acc 0.9880\n",
      "epoch 1364: train D loss: 0.6774, train F loss: -0.6465, acc 0.9924\n",
      "epoch 1365: train D loss: 0.6748, train F loss: -0.6384, acc 0.9910\n",
      "epoch 1366: train D loss: 0.6742, train F loss: -0.6468, acc 0.9936\n",
      "epoch 1367: train D loss: 0.6728, train F loss: -0.6343, acc 0.9912\n",
      "epoch 1368: train D loss: 0.6748, train F loss: -0.6402, acc 0.9906\n",
      "epoch 1369: train D loss: 0.6764, train F loss: -0.6346, acc 0.9900\n",
      "epoch 1370: train D loss: 0.6768, train F loss: -0.6397, acc 0.9906\n",
      "epoch 1371: train D loss: 0.6772, train F loss: -0.6414, acc 0.9908\n",
      "epoch 1372: train D loss: 0.6732, train F loss: -0.6277, acc 0.9894\n",
      "epoch 1373: train D loss: 0.6720, train F loss: -0.6109, acc 0.9844\n",
      "epoch 1374: train D loss: 0.6702, train F loss: -0.6322, acc 0.9900\n",
      "epoch 1375: train D loss: 0.6716, train F loss: -0.6374, acc 0.9922\n",
      "epoch 1376: train D loss: 0.6689, train F loss: -0.6397, acc 0.9936\n",
      "epoch 1377: train D loss: 0.6769, train F loss: -0.6445, acc 0.9928\n",
      "epoch 1378: train D loss: 0.6731, train F loss: -0.6360, acc 0.9914\n",
      "epoch 1379: train D loss: 0.6760, train F loss: -0.6298, acc 0.9906\n",
      "epoch 1380: train D loss: 0.6737, train F loss: -0.6463, acc 0.9934\n",
      "epoch 1381: train D loss: 0.6760, train F loss: -0.6407, acc 0.9924\n",
      "epoch 1382: train D loss: 0.6725, train F loss: -0.6408, acc 0.9922\n",
      "epoch 1383: train D loss: 0.6789, train F loss: -0.6542, acc 0.9942\n",
      "epoch 1384: train D loss: 0.6757, train F loss: -0.6409, acc 0.9914\n",
      "epoch 1385: train D loss: 0.6735, train F loss: -0.6439, acc 0.9918\n",
      "epoch 1386: train D loss: 0.6710, train F loss: -0.6448, acc 0.9936\n",
      "epoch 1387: train D loss: 0.6710, train F loss: -0.6307, acc 0.9900\n",
      "epoch 1388: train D loss: 0.6744, train F loss: -0.6277, acc 0.9884\n",
      "epoch 1389: train D loss: 0.6664, train F loss: -0.6292, acc 0.9920\n",
      "epoch 1390: train D loss: 0.6774, train F loss: -0.6385, acc 0.9910\n",
      "epoch 1391: train D loss: 0.6742, train F loss: -0.6339, acc 0.9906\n",
      "epoch 1392: train D loss: 0.6738, train F loss: -0.6410, acc 0.9920\n",
      "epoch 1393: train D loss: 0.6720, train F loss: -0.6413, acc 0.9914\n",
      "epoch 1394: train D loss: 0.6746, train F loss: -0.6493, acc 0.9948\n",
      "epoch 1395: train D loss: 0.6773, train F loss: -0.6452, acc 0.9908\n",
      "epoch 1396: train D loss: 0.6708, train F loss: -0.6410, acc 0.9926\n",
      "epoch 1397: train D loss: 0.6737, train F loss: -0.6416, acc 0.9920\n",
      "epoch 1398: train D loss: 0.6750, train F loss: -0.6349, acc 0.9912\n",
      "epoch 1399: train D loss: 0.6785, train F loss: -0.6458, acc 0.9916\n",
      "epoch 1400: train D loss: 0.6715, train F loss: -0.6375, acc 0.9920\n",
      "epoch 1401: train D loss: 0.6737, train F loss: -0.6273, acc 0.9904\n",
      "epoch 1402: train D loss: 0.6746, train F loss: -0.6239, acc 0.9896\n",
      "epoch 1403: train D loss: 0.6754, train F loss: -0.6315, acc 0.9888\n",
      "epoch 1404: train D loss: 0.6711, train F loss: -0.6264, acc 0.9902\n",
      "epoch 1405: train D loss: 0.6732, train F loss: -0.6401, acc 0.9918\n",
      "epoch 1406: train D loss: 0.6699, train F loss: -0.6372, acc 0.9908\n",
      "epoch 1407: train D loss: 0.6758, train F loss: -0.6395, acc 0.9916\n",
      "epoch 1408: train D loss: 0.6757, train F loss: -0.6462, acc 0.9936\n",
      "epoch 1409: train D loss: 0.6718, train F loss: -0.6433, acc 0.9932\n",
      "epoch 1410: train D loss: 0.6728, train F loss: -0.6380, acc 0.9920\n",
      "epoch 1411: train D loss: 0.6756, train F loss: -0.6404, acc 0.9910\n",
      "epoch 1412: train D loss: 0.6766, train F loss: -0.6382, acc 0.9904\n",
      "epoch 1413: train D loss: 0.6734, train F loss: -0.6424, acc 0.9918\n",
      "epoch 1414: train D loss: 0.6752, train F loss: -0.6397, acc 0.9918\n",
      "epoch 1415: train D loss: 0.6731, train F loss: -0.6312, acc 0.9890\n",
      "epoch 1416: train D loss: 0.6718, train F loss: -0.6315, acc 0.9908\n",
      "epoch 1417: train D loss: 0.6741, train F loss: -0.6454, acc 0.9932\n",
      "epoch 1418: train D loss: 0.6778, train F loss: -0.6530, acc 0.9946\n",
      "epoch 1419: train D loss: 0.6733, train F loss: -0.6398, acc 0.9926\n",
      "epoch 1420: train D loss: 0.6733, train F loss: -0.6426, acc 0.9948\n",
      "epoch 1421: train D loss: 0.6775, train F loss: -0.6473, acc 0.9928\n",
      "epoch 1422: train D loss: 0.6745, train F loss: -0.6252, acc 0.9886\n",
      "epoch 1423: train D loss: 0.6750, train F loss: -0.6409, acc 0.9906\n",
      "epoch 1424: train D loss: 0.6746, train F loss: -0.6247, acc 0.9892\n",
      "epoch 1425: train D loss: 0.6662, train F loss: -0.6259, acc 0.9898\n",
      "epoch 1426: train D loss: 0.6721, train F loss: -0.6387, acc 0.9902\n",
      "epoch 1427: train D loss: 0.6705, train F loss: -0.6322, acc 0.9896\n",
      "epoch 1428: train D loss: 0.6769, train F loss: -0.6368, acc 0.9914\n",
      "epoch 1429: train D loss: 0.6698, train F loss: -0.6383, acc 0.9918\n",
      "epoch 1430: train D loss: 0.6727, train F loss: -0.6416, acc 0.9934\n",
      "epoch 1431: train D loss: 0.6726, train F loss: -0.6341, acc 0.9914\n",
      "epoch 1432: train D loss: 0.6769, train F loss: -0.6490, acc 0.9940\n",
      "epoch 1433: train D loss: 0.6783, train F loss: -0.6478, acc 0.9910\n",
      "epoch 1434: train D loss: 0.6750, train F loss: -0.6225, acc 0.9870\n",
      "epoch 1435: train D loss: 0.6734, train F loss: -0.6376, acc 0.9916\n",
      "epoch 1436: train D loss: 0.6695, train F loss: -0.6339, acc 0.9922\n",
      "epoch 1437: train D loss: 0.6720, train F loss: -0.6353, acc 0.9904\n",
      "epoch 1438: train D loss: 0.6687, train F loss: -0.6384, acc 0.9934\n",
      "epoch 1439: train D loss: 0.6714, train F loss: -0.6352, acc 0.9938\n",
      "epoch 1440: train D loss: 0.6717, train F loss: -0.6420, acc 0.9936\n",
      "epoch 1441: train D loss: 0.6765, train F loss: -0.6270, acc 0.9876\n",
      "epoch 1442: train D loss: 0.6742, train F loss: -0.6389, acc 0.9916\n",
      "epoch 1443: train D loss: 0.6753, train F loss: -0.6447, acc 0.9922\n",
      "epoch 1444: train D loss: 0.6778, train F loss: -0.6503, acc 0.9934\n",
      "epoch 1445: train D loss: 0.6689, train F loss: -0.5867, acc 0.9858\n",
      "epoch 1446: train D loss: 0.6604, train F loss: -0.6166, acc 0.9898\n",
      "epoch 1447: train D loss: 0.6718, train F loss: -0.6367, acc 0.9924\n",
      "epoch 1448: train D loss: 0.6695, train F loss: -0.6389, acc 0.9934\n",
      "epoch 1449: train D loss: 0.6668, train F loss: -0.6354, acc 0.9924\n",
      "epoch 1450: train D loss: 0.6709, train F loss: -0.6321, acc 0.9902\n",
      "epoch 1451: train D loss: 0.6730, train F loss: -0.6326, acc 0.9912\n",
      "epoch 1452: train D loss: 0.6734, train F loss: -0.6389, acc 0.9918\n",
      "epoch 1453: train D loss: 0.6685, train F loss: -0.6386, acc 0.9930\n",
      "epoch 1454: train D loss: 0.6743, train F loss: -0.6406, acc 0.9932\n",
      "epoch 1455: train D loss: 0.6691, train F loss: -0.6380, acc 0.9924\n",
      "epoch 1456: train D loss: 0.6719, train F loss: -0.6395, acc 0.9922\n",
      "epoch 1457: train D loss: 0.6761, train F loss: -0.6501, acc 0.9942\n",
      "epoch 1458: train D loss: 0.6707, train F loss: -0.6435, acc 0.9936\n",
      "epoch 1459: train D loss: 0.6756, train F loss: -0.6451, acc 0.9936\n",
      "epoch 1460: train D loss: 0.6780, train F loss: -0.6486, acc 0.9922\n",
      "epoch 1461: train D loss: 0.6749, train F loss: -0.6406, acc 0.9910\n",
      "epoch 1462: train D loss: 0.6763, train F loss: -0.6446, acc 0.9924\n",
      "epoch 1463: train D loss: 0.6725, train F loss: -0.6354, acc 0.9916\n",
      "epoch 1464: train D loss: 0.6736, train F loss: -0.6400, acc 0.9916\n",
      "epoch 1465: train D loss: 0.6686, train F loss: -0.6313, acc 0.9928\n",
      "epoch 1466: train D loss: 0.6730, train F loss: -0.6277, acc 0.9902\n",
      "epoch 1467: train D loss: 0.6729, train F loss: -0.6337, acc 0.9896\n",
      "epoch 1468: train D loss: 0.6741, train F loss: -0.6371, acc 0.9902\n",
      "epoch 1469: train D loss: 0.6735, train F loss: -0.6413, acc 0.9928\n",
      "epoch 1470: train D loss: 0.6766, train F loss: -0.6483, acc 0.9926\n",
      "epoch 1471: train D loss: 0.6744, train F loss: -0.6435, acc 0.9916\n",
      "epoch 1472: train D loss: 0.6740, train F loss: -0.6404, acc 0.9908\n",
      "epoch 1473: train D loss: 0.6770, train F loss: -0.6436, acc 0.9916\n",
      "epoch 1474: train D loss: 0.6757, train F loss: -0.6429, acc 0.9928\n",
      "epoch 1475: train D loss: 0.6765, train F loss: -0.6462, acc 0.9924\n",
      "epoch 1476: train D loss: 0.6758, train F loss: -0.6455, acc 0.9916\n",
      "epoch 1477: train D loss: 0.6773, train F loss: -0.6518, acc 0.9938\n",
      "epoch 1478: train D loss: 0.6765, train F loss: -0.6446, acc 0.9912\n",
      "epoch 1479: train D loss: 0.6767, train F loss: -0.6421, acc 0.9922\n",
      "epoch 1480: train D loss: 0.6746, train F loss: -0.6498, acc 0.9940\n",
      "epoch 1481: train D loss: 0.6728, train F loss: -0.6306, acc 0.9904\n",
      "epoch 1482: train D loss: 0.6759, train F loss: -0.6411, acc 0.9928\n",
      "epoch 1483: train D loss: 0.6773, train F loss: -0.6484, acc 0.9926\n",
      "epoch 1484: train D loss: 0.6808, train F loss: -0.6536, acc 0.9926\n",
      "epoch 1485: train D loss: 0.6780, train F loss: -0.6428, acc 0.9894\n",
      "epoch 1486: train D loss: 0.6731, train F loss: -0.6413, acc 0.9916\n",
      "epoch 1487: train D loss: 0.6761, train F loss: -0.6454, acc 0.9934\n",
      "epoch 1488: train D loss: 0.6745, train F loss: -0.6339, acc 0.9896\n",
      "epoch 1489: train D loss: 0.6768, train F loss: -0.6501, acc 0.9938\n",
      "epoch 1490: train D loss: 0.6767, train F loss: -0.6381, acc 0.9902\n",
      "epoch 1491: train D loss: 0.6721, train F loss: -0.6344, acc 0.9906\n",
      "epoch 1492: train D loss: 0.6719, train F loss: -0.6395, acc 0.9920\n",
      "epoch 1493: train D loss: 0.6759, train F loss: -0.6338, acc 0.9908\n",
      "epoch 1494: train D loss: 0.6743, train F loss: -0.6410, acc 0.9922\n",
      "epoch 1495: train D loss: 0.6704, train F loss: -0.6339, acc 0.9916\n",
      "epoch 1496: train D loss: 0.6770, train F loss: -0.6392, acc 0.9892\n",
      "epoch 1497: train D loss: 0.6724, train F loss: -0.6372, acc 0.9914\n",
      "epoch 1498: train D loss: 0.6707, train F loss: -0.6375, acc 0.9910\n",
      "epoch 1499: train D loss: 0.6730, train F loss: -0.6411, acc 0.9928\n",
      "epoch 1500: train D loss: 0.6771, train F loss: -0.6438, acc 0.9914\n",
      "epoch 1501: train D loss: 0.6744, train F loss: -0.6423, acc 0.9928\n",
      "epoch 1502: train D loss: 0.6745, train F loss: -0.6339, acc 0.9910\n",
      "epoch 1503: train D loss: 0.6751, train F loss: -0.6439, acc 0.9922\n",
      "epoch 1504: train D loss: 0.6724, train F loss: -0.6387, acc 0.9904\n",
      "epoch 1505: train D loss: 0.6774, train F loss: -0.6450, acc 0.9920\n",
      "epoch 1506: train D loss: 0.6704, train F loss: -0.6399, acc 0.9918\n",
      "epoch 1507: train D loss: 0.6710, train F loss: -0.6358, acc 0.9922\n",
      "epoch 1508: train D loss: 0.6715, train F loss: -0.6281, acc 0.9894\n",
      "epoch 1509: train D loss: 0.6750, train F loss: -0.6400, acc 0.9914\n",
      "epoch 1510: train D loss: 0.6735, train F loss: -0.6412, acc 0.9926\n",
      "epoch 1511: train D loss: 0.6751, train F loss: -0.6446, acc 0.9932\n",
      "epoch 1512: train D loss: 0.6713, train F loss: -0.6353, acc 0.9922\n",
      "epoch 1513: train D loss: 0.6736, train F loss: -0.6379, acc 0.9908\n",
      "epoch 1514: train D loss: 0.6741, train F loss: -0.6404, acc 0.9910\n",
      "epoch 1515: train D loss: 0.6741, train F loss: -0.6463, acc 0.9938\n",
      "epoch 1516: train D loss: 0.6758, train F loss: -0.6434, acc 0.9936\n",
      "epoch 1517: train D loss: 0.6791, train F loss: -0.6511, acc 0.9924\n",
      "epoch 1518: train D loss: 0.6790, train F loss: -0.6448, acc 0.9920\n",
      "epoch 1519: train D loss: 0.6745, train F loss: -0.6383, acc 0.9900\n",
      "epoch 1520: train D loss: 0.6762, train F loss: -0.6451, acc 0.9922\n",
      "epoch 1521: train D loss: 0.6744, train F loss: -0.6355, acc 0.9904\n",
      "epoch 1522: train D loss: 0.6750, train F loss: -0.6481, acc 0.9934\n",
      "epoch 1523: train D loss: 0.6731, train F loss: -0.6398, acc 0.9924\n",
      "epoch 1524: train D loss: 0.6793, train F loss: -0.6439, acc 0.9910\n",
      "epoch 1525: train D loss: 0.6789, train F loss: -0.6524, acc 0.9932\n",
      "epoch 1526: train D loss: 0.6780, train F loss: -0.6429, acc 0.9918\n",
      "epoch 1527: train D loss: 0.6758, train F loss: -0.6479, acc 0.9934\n",
      "epoch 1528: train D loss: 0.6768, train F loss: -0.6437, acc 0.9922\n",
      "epoch 1529: train D loss: 0.6753, train F loss: -0.6371, acc 0.9906\n",
      "epoch 1530: train D loss: 0.6749, train F loss: -0.6441, acc 0.9922\n",
      "epoch 1531: train D loss: 0.6748, train F loss: -0.6330, acc 0.9900\n",
      "epoch 1532: train D loss: 0.6737, train F loss: -0.6462, acc 0.9938\n",
      "epoch 1533: train D loss: 0.6786, train F loss: -0.6514, acc 0.9930\n",
      "epoch 1534: train D loss: 0.6728, train F loss: -0.6375, acc 0.9918\n",
      "epoch 1535: train D loss: 0.6756, train F loss: -0.6417, acc 0.9920\n",
      "epoch 1536: train D loss: 0.6734, train F loss: -0.6483, acc 0.9950\n",
      "epoch 1537: train D loss: 0.6768, train F loss: -0.6407, acc 0.9916\n",
      "epoch 1538: train D loss: 0.6768, train F loss: -0.6435, acc 0.9912\n",
      "epoch 1539: train D loss: 0.6763, train F loss: -0.6382, acc 0.9904\n",
      "epoch 1540: train D loss: 0.6744, train F loss: -0.6317, acc 0.9908\n",
      "epoch 1541: train D loss: 0.6739, train F loss: -0.6396, acc 0.9928\n",
      "epoch 1542: train D loss: 0.6767, train F loss: -0.6344, acc 0.9906\n",
      "epoch 1543: train D loss: 0.6725, train F loss: -0.6399, acc 0.9924\n",
      "epoch 1544: train D loss: 0.6765, train F loss: -0.6453, acc 0.9918\n",
      "epoch 1545: train D loss: 0.6736, train F loss: -0.6413, acc 0.9904\n",
      "epoch 1546: train D loss: 0.6750, train F loss: -0.6464, acc 0.9926\n",
      "epoch 1547: train D loss: 0.6729, train F loss: -0.6372, acc 0.9930\n",
      "epoch 1548: train D loss: 0.6720, train F loss: -0.6313, acc 0.9914\n",
      "epoch 1549: train D loss: 0.6751, train F loss: -0.6446, acc 0.9928\n",
      "epoch 1550: train D loss: 0.6708, train F loss: -0.6392, acc 0.9930\n",
      "epoch 1551: train D loss: 0.6764, train F loss: -0.6439, acc 0.9928\n",
      "epoch 1552: train D loss: 0.6748, train F loss: -0.6497, acc 0.9946\n",
      "epoch 1553: train D loss: 0.6742, train F loss: -0.6423, acc 0.9922\n",
      "epoch 1554: train D loss: 0.6794, train F loss: -0.6412, acc 0.9918\n",
      "epoch 1555: train D loss: 0.6761, train F loss: -0.6436, acc 0.9912\n",
      "epoch 1556: train D loss: 0.6772, train F loss: -0.6395, acc 0.9926\n",
      "epoch 1557: train D loss: 0.6829, train F loss: -0.6464, acc 0.9914\n",
      "epoch 1558: train D loss: 0.6770, train F loss: -0.6448, acc 0.9920\n",
      "epoch 1559: train D loss: 0.6719, train F loss: -0.6392, acc 0.9920\n",
      "epoch 1560: train D loss: 0.6780, train F loss: -0.6461, acc 0.9916\n",
      "epoch 1561: train D loss: 0.6783, train F loss: -0.6466, acc 0.9926\n",
      "epoch 1562: train D loss: 0.6755, train F loss: -0.6521, acc 0.9940\n",
      "epoch 1563: train D loss: 0.6742, train F loss: -0.6441, acc 0.9924\n",
      "epoch 1564: train D loss: 0.6751, train F loss: -0.6453, acc 0.9928\n",
      "epoch 1565: train D loss: 0.6727, train F loss: -0.6429, acc 0.9926\n",
      "epoch 1566: train D loss: 0.6765, train F loss: -0.6501, acc 0.9932\n",
      "epoch 1567: train D loss: 0.6778, train F loss: -0.6519, acc 0.9928\n",
      "epoch 1568: train D loss: 0.6770, train F loss: -0.6347, acc 0.9892\n",
      "epoch 1569: train D loss: 0.6791, train F loss: -0.6410, acc 0.9902\n",
      "epoch 1570: train D loss: 0.6783, train F loss: -0.6504, acc 0.9910\n",
      "epoch 1571: train D loss: 0.6732, train F loss: -0.6458, acc 0.9932\n",
      "epoch 1572: train D loss: 0.6731, train F loss: -0.6140, acc 0.9890\n",
      "epoch 1573: train D loss: 0.6709, train F loss: -0.6309, acc 0.9900\n",
      "epoch 1574: train D loss: 0.6773, train F loss: -0.6253, acc 0.9912\n",
      "epoch 1575: train D loss: 0.6705, train F loss: -0.6327, acc 0.9904\n",
      "epoch 1576: train D loss: 0.6694, train F loss: -0.6370, acc 0.9922\n",
      "epoch 1577: train D loss: 0.6720, train F loss: -0.6410, acc 0.9912\n",
      "epoch 1578: train D loss: 0.6772, train F loss: -0.6362, acc 0.9910\n",
      "epoch 1579: train D loss: 0.6719, train F loss: -0.6451, acc 0.9926\n",
      "epoch 1580: train D loss: 0.6705, train F loss: -0.6413, acc 0.9936\n",
      "epoch 1581: train D loss: 0.6732, train F loss: -0.6422, acc 0.9924\n",
      "epoch 1582: train D loss: 0.6771, train F loss: -0.6422, acc 0.9918\n",
      "epoch 1583: train D loss: 0.6785, train F loss: -0.6449, acc 0.9924\n",
      "epoch 1584: train D loss: 0.6748, train F loss: -0.6492, acc 0.9942\n",
      "epoch 1585: train D loss: 0.6725, train F loss: -0.6361, acc 0.9906\n",
      "epoch 1586: train D loss: 0.6760, train F loss: -0.6407, acc 0.9904\n",
      "epoch 1587: train D loss: 0.6764, train F loss: -0.6361, acc 0.9906\n",
      "epoch 1588: train D loss: 0.6724, train F loss: -0.6334, acc 0.9910\n",
      "epoch 1589: train D loss: 0.6715, train F loss: -0.6399, acc 0.9924\n",
      "epoch 1590: train D loss: 0.6735, train F loss: -0.6443, acc 0.9922\n",
      "epoch 1591: train D loss: 0.6772, train F loss: -0.6483, acc 0.9936\n",
      "epoch 1592: train D loss: 0.6716, train F loss: -0.6314, acc 0.9922\n",
      "epoch 1593: train D loss: 0.6772, train F loss: -0.6511, acc 0.9938\n",
      "epoch 1594: train D loss: 0.6765, train F loss: -0.6436, acc 0.9918\n",
      "epoch 1595: train D loss: 0.6768, train F loss: -0.6501, acc 0.9936\n",
      "epoch 1596: train D loss: 0.6749, train F loss: -0.6468, acc 0.9932\n",
      "epoch 1597: train D loss: 0.6777, train F loss: -0.6454, acc 0.9912\n",
      "epoch 1598: train D loss: 0.6791, train F loss: -0.6490, acc 0.9906\n",
      "epoch 1599: train D loss: 0.6757, train F loss: -0.6396, acc 0.9906\n",
      "epoch 1600: train D loss: 0.6746, train F loss: -0.6467, acc 0.9922\n",
      "epoch 1601: train D loss: 0.6766, train F loss: -0.6514, acc 0.9936\n",
      "epoch 1602: train D loss: 0.6772, train F loss: -0.6525, acc 0.9934\n",
      "epoch 1603: train D loss: 0.6758, train F loss: -0.6402, acc 0.9918\n",
      "epoch 1604: train D loss: 0.6754, train F loss: -0.6525, acc 0.9942\n",
      "epoch 1605: train D loss: 0.6745, train F loss: -0.6381, acc 0.9918\n",
      "epoch 1606: train D loss: 0.6754, train F loss: -0.6399, acc 0.9916\n",
      "epoch 1607: train D loss: 0.6771, train F loss: -0.6454, acc 0.9924\n",
      "epoch 1608: train D loss: 0.6750, train F loss: -0.6392, acc 0.9916\n",
      "epoch 1609: train D loss: 0.6791, train F loss: -0.6434, acc 0.9892\n",
      "epoch 1610: train D loss: 0.6773, train F loss: -0.6457, acc 0.9910\n",
      "epoch 1611: train D loss: 0.6781, train F loss: -0.6487, acc 0.9926\n",
      "epoch 1612: train D loss: 0.6765, train F loss: -0.6227, acc 0.9874\n",
      "epoch 1613: train D loss: 0.6753, train F loss: -0.6458, acc 0.9938\n",
      "epoch 1614: train D loss: 0.6727, train F loss: -0.6466, acc 0.9940\n",
      "epoch 1615: train D loss: 0.6730, train F loss: -0.6420, acc 0.9938\n",
      "epoch 1616: train D loss: 0.6760, train F loss: -0.6438, acc 0.9920\n",
      "epoch 1617: train D loss: 0.6771, train F loss: -0.6451, acc 0.9926\n",
      "epoch 1618: train D loss: 0.6754, train F loss: -0.6430, acc 0.9926\n",
      "epoch 1619: train D loss: 0.6759, train F loss: -0.6447, acc 0.9918\n",
      "epoch 1620: train D loss: 0.6777, train F loss: -0.6504, acc 0.9932\n",
      "epoch 1621: train D loss: 0.6768, train F loss: -0.6453, acc 0.9916\n",
      "epoch 1622: train D loss: 0.6766, train F loss: -0.6476, acc 0.9922\n",
      "epoch 1623: train D loss: 0.6772, train F loss: -0.6419, acc 0.9906\n",
      "epoch 1624: train D loss: 0.6727, train F loss: -0.6368, acc 0.9918\n",
      "epoch 1625: train D loss: 0.6781, train F loss: -0.6419, acc 0.9910\n",
      "epoch 1626: train D loss: 0.6756, train F loss: -0.6336, acc 0.9896\n",
      "epoch 1627: train D loss: 0.6740, train F loss: -0.6359, acc 0.9920\n",
      "epoch 1628: train D loss: 0.6711, train F loss: -0.6382, acc 0.9916\n",
      "epoch 1629: train D loss: 0.6808, train F loss: -0.6517, acc 0.9924\n",
      "epoch 1630: train D loss: 0.6755, train F loss: -0.6447, acc 0.9926\n",
      "epoch 1631: train D loss: 0.6769, train F loss: -0.6438, acc 0.9918\n",
      "epoch 1632: train D loss: 0.6769, train F loss: -0.6445, acc 0.9912\n",
      "epoch 1633: train D loss: 0.6778, train F loss: -0.6466, acc 0.9914\n",
      "epoch 1634: train D loss: 0.6751, train F loss: -0.6467, acc 0.9932\n",
      "epoch 1635: train D loss: 0.6766, train F loss: -0.6506, acc 0.9948\n",
      "epoch 1636: train D loss: 0.6753, train F loss: -0.6459, acc 0.9924\n",
      "epoch 1637: train D loss: 0.6798, train F loss: -0.6543, acc 0.9938\n",
      "epoch 1638: train D loss: 0.6749, train F loss: -0.6335, acc 0.9900\n",
      "epoch 1639: train D loss: 0.6728, train F loss: -0.6379, acc 0.9918\n",
      "epoch 1640: train D loss: 0.6796, train F loss: -0.6451, acc 0.9906\n",
      "epoch 1641: train D loss: 0.6772, train F loss: -0.6474, acc 0.9926\n",
      "epoch 1642: train D loss: 0.6748, train F loss: -0.6472, acc 0.9934\n",
      "epoch 1643: train D loss: 0.6775, train F loss: -0.6451, acc 0.9912\n",
      "epoch 1644: train D loss: 0.6784, train F loss: -0.6499, acc 0.9928\n",
      "epoch 1645: train D loss: 0.6771, train F loss: -0.6468, acc 0.9926\n",
      "epoch 1646: train D loss: 0.6731, train F loss: -0.6221, acc 0.9898\n",
      "epoch 1647: train D loss: 0.6759, train F loss: -0.6455, acc 0.9912\n",
      "epoch 1648: train D loss: 0.6728, train F loss: -0.6410, acc 0.9928\n",
      "epoch 1649: train D loss: 0.6787, train F loss: -0.6551, acc 0.9952\n",
      "epoch 1650: train D loss: 0.6764, train F loss: -0.6426, acc 0.9916\n",
      "epoch 1651: train D loss: 0.6746, train F loss: -0.6315, acc 0.9898\n",
      "epoch 1652: train D loss: 0.6760, train F loss: -0.6398, acc 0.9908\n",
      "epoch 1653: train D loss: 0.6769, train F loss: -0.6493, acc 0.9926\n",
      "epoch 1654: train D loss: 0.6754, train F loss: -0.6293, acc 0.9890\n",
      "epoch 1655: train D loss: 0.6725, train F loss: -0.6333, acc 0.9894\n",
      "epoch 1656: train D loss: 0.6757, train F loss: -0.6417, acc 0.9922\n",
      "epoch 1657: train D loss: 0.6702, train F loss: -0.6451, acc 0.9944\n",
      "epoch 1658: train D loss: 0.6756, train F loss: -0.6425, acc 0.9930\n",
      "epoch 1659: train D loss: 0.6739, train F loss: -0.6366, acc 0.9918\n",
      "epoch 1660: train D loss: 0.6761, train F loss: -0.6401, acc 0.9908\n",
      "epoch 1661: train D loss: 0.6766, train F loss: -0.6460, acc 0.9916\n",
      "epoch 1662: train D loss: 0.6755, train F loss: -0.6479, acc 0.9930\n",
      "epoch 1663: train D loss: 0.6774, train F loss: -0.6484, acc 0.9922\n",
      "epoch 1664: train D loss: 0.6733, train F loss: -0.6451, acc 0.9926\n",
      "epoch 1665: train D loss: 0.6770, train F loss: -0.6468, acc 0.9928\n",
      "epoch 1666: train D loss: 0.6774, train F loss: -0.6456, acc 0.9924\n",
      "epoch 1667: train D loss: 0.6756, train F loss: -0.6469, acc 0.9928\n",
      "epoch 1668: train D loss: 0.6759, train F loss: -0.6335, acc 0.9892\n",
      "epoch 1669: train D loss: 0.6733, train F loss: -0.6408, acc 0.9912\n",
      "epoch 1670: train D loss: 0.6753, train F loss: -0.6478, acc 0.9924\n",
      "epoch 1671: train D loss: 0.6746, train F loss: -0.6455, acc 0.9914\n",
      "epoch 1672: train D loss: 0.6756, train F loss: -0.6529, acc 0.9944\n",
      "epoch 1673: train D loss: 0.6811, train F loss: -0.6362, acc 0.9898\n",
      "epoch 1674: train D loss: 0.6772, train F loss: -0.6463, acc 0.9916\n",
      "epoch 1675: train D loss: 0.6734, train F loss: -0.6363, acc 0.9908\n",
      "epoch 1676: train D loss: 0.6736, train F loss: -0.6445, acc 0.9940\n",
      "epoch 1677: train D loss: 0.6810, train F loss: -0.6482, acc 0.9916\n",
      "epoch 1678: train D loss: 0.6758, train F loss: -0.6458, acc 0.9938\n",
      "epoch 1679: train D loss: 0.6782, train F loss: -0.6559, acc 0.9940\n",
      "epoch 1680: train D loss: 0.6769, train F loss: -0.6525, acc 0.9942\n",
      "epoch 1681: train D loss: 0.6795, train F loss: -0.6518, acc 0.9926\n",
      "epoch 1682: train D loss: 0.6775, train F loss: -0.6528, acc 0.9946\n",
      "epoch 1683: train D loss: 0.6769, train F loss: -0.6492, acc 0.9928\n",
      "epoch 1684: train D loss: 0.6757, train F loss: -0.6465, acc 0.9938\n",
      "epoch 1685: train D loss: 0.6782, train F loss: -0.6469, acc 0.9922\n",
      "epoch 1686: train D loss: 0.6758, train F loss: -0.6469, acc 0.9930\n",
      "epoch 1687: train D loss: 0.6740, train F loss: -0.6424, acc 0.9930\n",
      "epoch 1688: train D loss: 0.6779, train F loss: -0.6370, acc 0.9904\n",
      "epoch 1689: train D loss: 0.6770, train F loss: -0.6493, acc 0.9930\n",
      "epoch 1690: train D loss: 0.6762, train F loss: -0.6468, acc 0.9914\n",
      "epoch 1691: train D loss: 0.6730, train F loss: -0.6413, acc 0.9930\n",
      "epoch 1692: train D loss: 0.6755, train F loss: -0.6482, acc 0.9940\n",
      "epoch 1693: train D loss: 0.6764, train F loss: -0.6514, acc 0.9944\n",
      "epoch 1694: train D loss: 0.6792, train F loss: -0.6499, acc 0.9928\n",
      "epoch 1695: train D loss: 0.6778, train F loss: -0.6480, acc 0.9916\n",
      "epoch 1696: train D loss: 0.6811, train F loss: -0.6390, acc 0.9906\n",
      "epoch 1697: train D loss: 0.6806, train F loss: -0.6506, acc 0.9924\n",
      "epoch 1698: train D loss: 0.6748, train F loss: -0.6127, acc 0.9884\n",
      "epoch 1699: train D loss: 0.6749, train F loss: -0.6435, acc 0.9926\n",
      "epoch 1700: train D loss: 0.6756, train F loss: -0.6470, acc 0.9920\n",
      "epoch 1701: train D loss: 0.6754, train F loss: -0.6502, acc 0.9930\n",
      "epoch 1702: train D loss: 0.6757, train F loss: -0.6447, acc 0.9922\n",
      "epoch 1703: train D loss: 0.6803, train F loss: -0.6568, acc 0.9936\n",
      "epoch 1704: train D loss: 0.6753, train F loss: -0.6507, acc 0.9942\n",
      "epoch 1705: train D loss: 0.6766, train F loss: -0.6471, acc 0.9914\n",
      "epoch 1706: train D loss: 0.6801, train F loss: -0.6484, acc 0.9914\n",
      "epoch 1707: train D loss: 0.6754, train F loss: -0.6420, acc 0.9918\n",
      "epoch 1708: train D loss: 0.6801, train F loss: -0.6481, acc 0.9930\n",
      "epoch 1709: train D loss: 0.6750, train F loss: -0.6453, acc 0.9924\n",
      "epoch 1710: train D loss: 0.6770, train F loss: -0.6496, acc 0.9922\n",
      "epoch 1711: train D loss: 0.6793, train F loss: -0.6484, acc 0.9930\n",
      "epoch 1712: train D loss: 0.6775, train F loss: -0.6447, acc 0.9926\n",
      "epoch 1713: train D loss: 0.6756, train F loss: -0.6485, acc 0.9932\n",
      "epoch 1714: train D loss: 0.6822, train F loss: -0.6637, acc 0.9956\n",
      "epoch 1715: train D loss: 0.6788, train F loss: -0.6486, acc 0.9924\n",
      "epoch 1716: train D loss: 0.6752, train F loss: -0.6485, acc 0.9938\n",
      "epoch 1717: train D loss: 0.6755, train F loss: -0.6409, acc 0.9908\n",
      "epoch 1718: train D loss: 0.6765, train F loss: -0.6429, acc 0.9918\n",
      "epoch 1719: train D loss: 0.6770, train F loss: -0.6454, acc 0.9918\n",
      "epoch 1720: train D loss: 0.6760, train F loss: -0.6441, acc 0.9916\n",
      "epoch 1721: train D loss: 0.6716, train F loss: -0.6125, acc 0.9888\n",
      "epoch 1722: train D loss: 0.6739, train F loss: -0.6425, acc 0.9922\n",
      "epoch 1723: train D loss: 0.6756, train F loss: -0.6457, acc 0.9934\n",
      "epoch 1724: train D loss: 0.6737, train F loss: -0.6480, acc 0.9942\n",
      "epoch 1725: train D loss: 0.6740, train F loss: -0.6388, acc 0.9920\n",
      "epoch 1726: train D loss: 0.6738, train F loss: -0.6467, acc 0.9934\n",
      "epoch 1727: train D loss: 0.6750, train F loss: -0.6457, acc 0.9932\n",
      "epoch 1728: train D loss: 0.6772, train F loss: -0.6483, acc 0.9932\n",
      "epoch 1729: train D loss: 0.6732, train F loss: -0.6458, acc 0.9932\n",
      "epoch 1730: train D loss: 0.6749, train F loss: -0.6487, acc 0.9940\n",
      "epoch 1731: train D loss: 0.6776, train F loss: -0.6515, acc 0.9928\n",
      "epoch 1732: train D loss: 0.6774, train F loss: -0.6433, acc 0.9928\n",
      "epoch 1733: train D loss: 0.6778, train F loss: -0.6404, acc 0.9888\n",
      "epoch 1734: train D loss: 0.6764, train F loss: -0.6419, acc 0.9900\n",
      "epoch 1735: train D loss: 0.6754, train F loss: -0.6483, acc 0.9930\n",
      "epoch 1736: train D loss: 0.6745, train F loss: -0.6369, acc 0.9892\n",
      "epoch 1737: train D loss: 0.6758, train F loss: -0.6494, acc 0.9940\n",
      "epoch 1738: train D loss: 0.6777, train F loss: -0.6488, acc 0.9918\n",
      "epoch 1739: train D loss: 0.6775, train F loss: -0.6515, acc 0.9920\n",
      "epoch 1740: train D loss: 0.6815, train F loss: -0.6511, acc 0.9922\n",
      "epoch 1741: train D loss: 0.6769, train F loss: -0.6453, acc 0.9926\n",
      "epoch 1742: train D loss: 0.6778, train F loss: -0.6510, acc 0.9920\n",
      "epoch 1743: train D loss: 0.6778, train F loss: -0.6535, acc 0.9944\n",
      "epoch 1744: train D loss: 0.6747, train F loss: -0.6354, acc 0.9906\n",
      "epoch 1745: train D loss: 0.6746, train F loss: -0.6444, acc 0.9918\n",
      "epoch 1746: train D loss: 0.6777, train F loss: -0.6557, acc 0.9946\n",
      "epoch 1747: train D loss: 0.6766, train F loss: -0.6386, acc 0.9898\n",
      "epoch 1748: train D loss: 0.6761, train F loss: -0.6535, acc 0.9940\n",
      "epoch 1749: train D loss: 0.6802, train F loss: -0.6563, acc 0.9948\n",
      "epoch 1750: train D loss: 0.6781, train F loss: -0.6507, acc 0.9938\n",
      "epoch 1751: train D loss: 0.6790, train F loss: -0.6524, acc 0.9930\n",
      "epoch 1752: train D loss: 0.6771, train F loss: -0.6449, acc 0.9930\n",
      "epoch 1753: train D loss: 0.6766, train F loss: -0.6434, acc 0.9904\n",
      "epoch 1754: train D loss: 0.6762, train F loss: -0.6514, acc 0.9928\n",
      "epoch 1755: train D loss: 0.6762, train F loss: -0.6467, acc 0.9920\n",
      "epoch 1756: train D loss: 0.6760, train F loss: -0.6508, acc 0.9938\n",
      "epoch 1757: train D loss: 0.6773, train F loss: -0.6493, acc 0.9936\n",
      "epoch 1758: train D loss: 0.6818, train F loss: -0.6568, acc 0.9928\n",
      "epoch 1759: train D loss: 0.6778, train F loss: -0.6410, acc 0.9900\n",
      "epoch 1760: train D loss: 0.6764, train F loss: -0.6403, acc 0.9900\n",
      "epoch 1761: train D loss: 0.6731, train F loss: -0.6404, acc 0.9918\n",
      "epoch 1762: train D loss: 0.6772, train F loss: -0.6427, acc 0.9908\n",
      "epoch 1763: train D loss: 0.6746, train F loss: -0.6487, acc 0.9942\n",
      "epoch 1764: train D loss: 0.6762, train F loss: -0.6516, acc 0.9934\n",
      "epoch 1765: train D loss: 0.6763, train F loss: -0.6490, acc 0.9914\n",
      "epoch 1766: train D loss: 0.6748, train F loss: -0.6355, acc 0.9900\n",
      "epoch 1767: train D loss: 0.6769, train F loss: -0.6428, acc 0.9920\n",
      "epoch 1768: train D loss: 0.6778, train F loss: -0.6495, acc 0.9924\n",
      "epoch 1769: train D loss: 0.6802, train F loss: -0.6471, acc 0.9926\n",
      "epoch 1770: train D loss: 0.6747, train F loss: -0.6446, acc 0.9914\n",
      "epoch 1771: train D loss: 0.6764, train F loss: -0.6476, acc 0.9928\n",
      "epoch 1772: train D loss: 0.6783, train F loss: -0.6481, acc 0.9914\n",
      "epoch 1773: train D loss: 0.6793, train F loss: -0.6435, acc 0.9908\n",
      "epoch 1774: train D loss: 0.6771, train F loss: -0.6492, acc 0.9932\n",
      "epoch 1775: train D loss: 0.6773, train F loss: -0.6509, acc 0.9934\n",
      "epoch 1776: train D loss: 0.6764, train F loss: -0.6553, acc 0.9946\n",
      "epoch 1777: train D loss: 0.6765, train F loss: -0.6411, acc 0.9916\n",
      "epoch 1778: train D loss: 0.6767, train F loss: -0.6416, acc 0.9924\n",
      "epoch 1779: train D loss: 0.6756, train F loss: -0.6455, acc 0.9908\n",
      "epoch 1780: train D loss: 0.6757, train F loss: -0.6297, acc 0.9894\n",
      "epoch 1781: train D loss: 0.6772, train F loss: -0.6442, acc 0.9926\n",
      "epoch 1782: train D loss: 0.6761, train F loss: -0.6482, acc 0.9928\n",
      "epoch 1783: train D loss: 0.6769, train F loss: -0.6400, acc 0.9924\n",
      "epoch 1784: train D loss: 0.6795, train F loss: -0.6484, acc 0.9912\n",
      "epoch 1785: train D loss: 0.6769, train F loss: -0.6443, acc 0.9904\n",
      "epoch 1786: train D loss: 0.6744, train F loss: -0.6484, acc 0.9912\n",
      "epoch 1787: train D loss: 0.6766, train F loss: -0.6533, acc 0.9936\n",
      "epoch 1788: train D loss: 0.6770, train F loss: -0.6467, acc 0.9926\n",
      "epoch 1789: train D loss: 0.6760, train F loss: -0.6501, acc 0.9920\n",
      "epoch 1790: train D loss: 0.6754, train F loss: -0.6464, acc 0.9924\n",
      "epoch 1791: train D loss: 0.6750, train F loss: -0.6478, acc 0.9930\n",
      "epoch 1792: train D loss: 0.6766, train F loss: -0.6472, acc 0.9928\n",
      "epoch 1793: train D loss: 0.6802, train F loss: -0.6592, acc 0.9940\n",
      "epoch 1794: train D loss: 0.6764, train F loss: -0.6423, acc 0.9916\n",
      "epoch 1795: train D loss: 0.6789, train F loss: -0.6541, acc 0.9938\n",
      "epoch 1796: train D loss: 0.6760, train F loss: -0.6523, acc 0.9938\n",
      "epoch 1797: train D loss: 0.6803, train F loss: -0.6462, acc 0.9924\n",
      "epoch 1798: train D loss: 0.6770, train F loss: -0.6390, acc 0.9906\n",
      "epoch 1799: train D loss: 0.6746, train F loss: -0.6504, acc 0.9940\n",
      "epoch 1800: train D loss: 0.6754, train F loss: -0.6412, acc 0.9922\n",
      "epoch 1801: train D loss: 0.6798, train F loss: -0.6477, acc 0.9906\n",
      "epoch 1802: train D loss: 0.6756, train F loss: -0.6420, acc 0.9912\n",
      "epoch 1803: train D loss: 0.6754, train F loss: -0.6505, acc 0.9932\n",
      "epoch 1804: train D loss: 0.6757, train F loss: -0.6454, acc 0.9922\n",
      "epoch 1805: train D loss: 0.6747, train F loss: -0.6181, acc 0.9916\n",
      "epoch 1806: train D loss: 0.6749, train F loss: -0.6445, acc 0.9942\n",
      "epoch 1807: train D loss: 0.6753, train F loss: -0.6479, acc 0.9932\n",
      "epoch 1808: train D loss: 0.6734, train F loss: -0.6417, acc 0.9928\n",
      "epoch 1809: train D loss: 0.6757, train F loss: -0.6494, acc 0.9932\n",
      "epoch 1810: train D loss: 0.6778, train F loss: -0.6555, acc 0.9946\n",
      "epoch 1811: train D loss: 0.6769, train F loss: -0.6510, acc 0.9940\n",
      "epoch 1812: train D loss: 0.6826, train F loss: -0.6549, acc 0.9938\n",
      "epoch 1813: train D loss: 0.6772, train F loss: -0.6475, acc 0.9924\n",
      "epoch 1814: train D loss: 0.6775, train F loss: -0.6570, acc 0.9946\n",
      "epoch 1815: train D loss: 0.6761, train F loss: -0.6477, acc 0.9918\n",
      "epoch 1816: train D loss: 0.6763, train F loss: -0.6470, acc 0.9926\n",
      "epoch 1817: train D loss: 0.6748, train F loss: -0.6446, acc 0.9918\n",
      "epoch 1818: train D loss: 0.6740, train F loss: -0.6456, acc 0.9926\n",
      "epoch 1819: train D loss: 0.6798, train F loss: -0.6444, acc 0.9898\n",
      "epoch 1820: train D loss: 0.6753, train F loss: -0.6497, acc 0.9936\n",
      "epoch 1821: train D loss: 0.6785, train F loss: -0.6551, acc 0.9936\n",
      "epoch 1822: train D loss: 0.6806, train F loss: -0.6481, acc 0.9920\n",
      "epoch 1823: train D loss: 0.6769, train F loss: -0.6534, acc 0.9938\n",
      "epoch 1824: train D loss: 0.6768, train F loss: -0.6502, acc 0.9926\n",
      "epoch 1825: train D loss: 0.6792, train F loss: -0.6519, acc 0.9924\n",
      "epoch 1826: train D loss: 0.6793, train F loss: -0.6413, acc 0.9914\n",
      "epoch 1827: train D loss: 0.6779, train F loss: -0.6468, acc 0.9914\n",
      "epoch 1828: train D loss: 0.6776, train F loss: -0.6504, acc 0.9940\n",
      "epoch 1829: train D loss: 0.6768, train F loss: -0.6469, acc 0.9920\n",
      "epoch 1830: train D loss: 0.6806, train F loss: -0.6514, acc 0.9918\n",
      "epoch 1831: train D loss: 0.6809, train F loss: -0.6559, acc 0.9940\n",
      "epoch 1832: train D loss: 0.6787, train F loss: -0.6549, acc 0.9944\n",
      "epoch 1833: train D loss: 0.6754, train F loss: -0.6340, acc 0.9920\n",
      "epoch 1834: train D loss: 0.6783, train F loss: -0.6450, acc 0.9906\n",
      "epoch 1835: train D loss: 0.6792, train F loss: -0.6436, acc 0.9916\n",
      "epoch 1836: train D loss: 0.6792, train F loss: -0.6383, acc 0.9900\n",
      "epoch 1837: train D loss: 0.6739, train F loss: -0.6297, acc 0.9902\n",
      "epoch 1838: train D loss: 0.6751, train F loss: -0.6481, acc 0.9932\n",
      "epoch 1839: train D loss: 0.6783, train F loss: -0.6495, acc 0.9932\n",
      "epoch 1840: train D loss: 0.6771, train F loss: -0.6467, acc 0.9934\n",
      "epoch 1841: train D loss: 0.6775, train F loss: -0.6475, acc 0.9928\n",
      "epoch 1842: train D loss: 0.6748, train F loss: -0.6466, acc 0.9928\n",
      "epoch 1843: train D loss: 0.6773, train F loss: -0.6574, acc 0.9952\n",
      "epoch 1844: train D loss: 0.6772, train F loss: -0.6443, acc 0.9924\n",
      "epoch 1845: train D loss: 0.6772, train F loss: -0.6496, acc 0.9936\n",
      "epoch 1846: train D loss: 0.6749, train F loss: -0.6443, acc 0.9934\n",
      "epoch 1847: train D loss: 0.6793, train F loss: -0.6527, acc 0.9930\n",
      "epoch 1848: train D loss: 0.6786, train F loss: -0.6564, acc 0.9936\n",
      "epoch 1849: train D loss: 0.6774, train F loss: -0.6595, acc 0.9954\n",
      "epoch 1850: train D loss: 0.6773, train F loss: -0.6487, acc 0.9916\n",
      "epoch 1851: train D loss: 0.6806, train F loss: -0.6535, acc 0.9932\n",
      "epoch 1852: train D loss: 0.6771, train F loss: -0.6504, acc 0.9928\n",
      "epoch 1853: train D loss: 0.6781, train F loss: -0.6512, acc 0.9924\n",
      "epoch 1854: train D loss: 0.6807, train F loss: -0.6579, acc 0.9934\n",
      "epoch 1855: train D loss: 0.6761, train F loss: -0.6399, acc 0.9912\n",
      "epoch 1856: train D loss: 0.6782, train F loss: -0.6514, acc 0.9936\n",
      "epoch 1857: train D loss: 0.6779, train F loss: -0.6406, acc 0.9920\n",
      "epoch 1858: train D loss: 0.6809, train F loss: -0.6581, acc 0.9942\n",
      "epoch 1859: train D loss: 0.6763, train F loss: -0.6453, acc 0.9918\n",
      "epoch 1860: train D loss: 0.6747, train F loss: -0.6509, acc 0.9936\n",
      "epoch 1861: train D loss: 0.6776, train F loss: -0.6521, acc 0.9934\n",
      "epoch 1862: train D loss: 0.6775, train F loss: -0.6380, acc 0.9908\n",
      "epoch 1863: train D loss: 0.6777, train F loss: -0.6440, acc 0.9918\n",
      "epoch 1864: train D loss: 0.6759, train F loss: -0.6401, acc 0.9918\n",
      "epoch 1865: train D loss: 0.6756, train F loss: -0.6493, acc 0.9932\n",
      "epoch 1866: train D loss: 0.6761, train F loss: -0.6494, acc 0.9922\n",
      "epoch 1867: train D loss: 0.6761, train F loss: -0.6425, acc 0.9916\n",
      "epoch 1868: train D loss: 0.6781, train F loss: -0.6415, acc 0.9912\n",
      "epoch 1869: train D loss: 0.6732, train F loss: -0.6491, acc 0.9938\n",
      "epoch 1870: train D loss: 0.6760, train F loss: -0.6480, acc 0.9932\n",
      "epoch 1871: train D loss: 0.6775, train F loss: -0.6485, acc 0.9916\n",
      "epoch 1872: train D loss: 0.6744, train F loss: -0.6433, acc 0.9928\n",
      "epoch 1873: train D loss: 0.6743, train F loss: -0.6516, acc 0.9952\n",
      "epoch 1874: train D loss: 0.6785, train F loss: -0.6539, acc 0.9938\n",
      "epoch 1875: train D loss: 0.6750, train F loss: -0.6091, acc 0.9866\n",
      "epoch 1876: train D loss: 0.6727, train F loss: -0.6437, acc 0.9918\n",
      "epoch 1877: train D loss: 0.6737, train F loss: -0.6448, acc 0.9930\n",
      "epoch 1878: train D loss: 0.6744, train F loss: -0.6459, acc 0.9922\n",
      "epoch 1879: train D loss: 0.6760, train F loss: -0.6458, acc 0.9912\n",
      "epoch 1880: train D loss: 0.6743, train F loss: -0.6456, acc 0.9934\n",
      "epoch 1881: train D loss: 0.6730, train F loss: -0.6406, acc 0.9920\n",
      "epoch 1882: train D loss: 0.6710, train F loss: -0.6437, acc 0.9936\n",
      "epoch 1883: train D loss: 0.6738, train F loss: -0.6363, acc 0.9908\n",
      "epoch 1884: train D loss: 0.6726, train F loss: -0.6456, acc 0.9932\n",
      "epoch 1885: train D loss: 0.6752, train F loss: -0.6487, acc 0.9938\n",
      "epoch 1886: train D loss: 0.6765, train F loss: -0.6419, acc 0.9916\n",
      "epoch 1887: train D loss: 0.6720, train F loss: -0.6462, acc 0.9932\n",
      "epoch 1888: train D loss: 0.6769, train F loss: -0.6501, acc 0.9926\n",
      "epoch 1889: train D loss: 0.6765, train F loss: -0.6497, acc 0.9940\n",
      "epoch 1890: train D loss: 0.6774, train F loss: -0.6467, acc 0.9914\n",
      "epoch 1891: train D loss: 0.6776, train F loss: -0.6505, acc 0.9930\n",
      "epoch 1892: train D loss: 0.6762, train F loss: -0.6459, acc 0.9922\n",
      "epoch 1893: train D loss: 0.6741, train F loss: -0.6512, acc 0.9934\n",
      "epoch 1894: train D loss: 0.6771, train F loss: -0.6541, acc 0.9940\n",
      "epoch 1895: train D loss: 0.6734, train F loss: -0.6477, acc 0.9940\n",
      "epoch 1896: train D loss: 0.6816, train F loss: -0.6516, acc 0.9912\n",
      "epoch 1897: train D loss: 0.6793, train F loss: -0.6429, acc 0.9896\n",
      "epoch 1898: train D loss: 0.6783, train F loss: -0.6490, acc 0.9928\n",
      "epoch 1899: train D loss: 0.6765, train F loss: -0.6463, acc 0.9922\n",
      "epoch 1900: train D loss: 0.6733, train F loss: -0.6428, acc 0.9928\n",
      "epoch 1901: train D loss: 0.6790, train F loss: -0.6549, acc 0.9940\n",
      "epoch 1902: train D loss: 0.6814, train F loss: -0.6554, acc 0.9924\n",
      "epoch 1903: train D loss: 0.6771, train F loss: -0.6516, acc 0.9938\n",
      "epoch 1904: train D loss: 0.6767, train F loss: -0.6434, acc 0.9926\n",
      "epoch 1905: train D loss: 0.6765, train F loss: -0.6433, acc 0.9924\n",
      "epoch 1906: train D loss: 0.6750, train F loss: -0.6162, acc 0.9892\n",
      "epoch 1907: train D loss: 0.6743, train F loss: -0.6460, acc 0.9936\n",
      "epoch 1908: train D loss: 0.6737, train F loss: -0.6437, acc 0.9932\n",
      "epoch 1909: train D loss: 0.6756, train F loss: -0.6495, acc 0.9940\n",
      "epoch 1910: train D loss: 0.6751, train F loss: -0.6515, acc 0.9944\n",
      "epoch 1911: train D loss: 0.6776, train F loss: -0.6528, acc 0.9946\n",
      "epoch 1912: train D loss: 0.6759, train F loss: -0.6483, acc 0.9932\n",
      "epoch 1913: train D loss: 0.6767, train F loss: -0.6408, acc 0.9908\n",
      "epoch 1914: train D loss: 0.6821, train F loss: -0.6598, acc 0.9936\n",
      "epoch 1915: train D loss: 0.6814, train F loss: -0.6597, acc 0.9938\n",
      "epoch 1916: train D loss: 0.6760, train F loss: -0.6535, acc 0.9952\n",
      "epoch 1917: train D loss: 0.6775, train F loss: -0.6439, acc 0.9918\n",
      "epoch 1918: train D loss: 0.6762, train F loss: -0.6404, acc 0.9896\n",
      "epoch 1919: train D loss: 0.6738, train F loss: -0.6377, acc 0.9920\n",
      "epoch 1920: train D loss: 0.6778, train F loss: -0.6525, acc 0.9924\n",
      "epoch 1921: train D loss: 0.6775, train F loss: -0.6418, acc 0.9910\n",
      "epoch 1922: train D loss: 0.6679, train F loss: -0.5745, acc 0.9828\n",
      "epoch 1923: train D loss: 0.6735, train F loss: -0.6414, acc 0.9902\n",
      "epoch 1924: train D loss: 0.6741, train F loss: -0.6409, acc 0.9896\n",
      "epoch 1925: train D loss: 0.6754, train F loss: -0.6505, acc 0.9948\n",
      "epoch 1926: train D loss: 0.6734, train F loss: -0.6485, acc 0.9944\n",
      "epoch 1927: train D loss: 0.6758, train F loss: -0.6497, acc 0.9938\n",
      "epoch 1928: train D loss: 0.6739, train F loss: -0.6454, acc 0.9934\n",
      "epoch 1929: train D loss: 0.6736, train F loss: -0.6473, acc 0.9934\n",
      "epoch 1930: train D loss: 0.6775, train F loss: -0.6369, acc 0.9906\n",
      "epoch 1931: train D loss: 0.6758, train F loss: -0.6476, acc 0.9920\n",
      "epoch 1932: train D loss: 0.6746, train F loss: -0.6353, acc 0.9924\n",
      "epoch 1933: train D loss: 0.6748, train F loss: -0.6455, acc 0.9922\n",
      "epoch 1934: train D loss: 0.6757, train F loss: -0.6505, acc 0.9934\n",
      "epoch 1935: train D loss: 0.6771, train F loss: -0.6567, acc 0.9946\n",
      "epoch 1936: train D loss: 0.6757, train F loss: -0.6500, acc 0.9938\n",
      "epoch 1937: train D loss: 0.6752, train F loss: -0.6531, acc 0.9948\n",
      "epoch 1938: train D loss: 0.6795, train F loss: -0.6512, acc 0.9926\n",
      "epoch 1939: train D loss: 0.6786, train F loss: -0.6539, acc 0.9936\n",
      "epoch 1940: train D loss: 0.6785, train F loss: -0.6490, acc 0.9922\n",
      "epoch 1941: train D loss: 0.6766, train F loss: -0.6526, acc 0.9930\n",
      "epoch 1942: train D loss: 0.6730, train F loss: -0.6378, acc 0.9918\n",
      "epoch 1943: train D loss: 0.6808, train F loss: -0.6563, acc 0.9934\n",
      "epoch 1944: train D loss: 0.6795, train F loss: -0.6599, acc 0.9950\n",
      "epoch 1945: train D loss: 0.6760, train F loss: -0.6523, acc 0.9934\n",
      "epoch 1946: train D loss: 0.6788, train F loss: -0.6349, acc 0.9906\n",
      "epoch 1947: train D loss: 0.6734, train F loss: -0.6455, acc 0.9926\n",
      "epoch 1948: train D loss: 0.6786, train F loss: -0.6574, acc 0.9950\n",
      "epoch 1949: train D loss: 0.6784, train F loss: -0.6514, acc 0.9932\n",
      "epoch 1950: train D loss: 0.6734, train F loss: -0.6376, acc 0.9922\n",
      "epoch 1951: train D loss: 0.6775, train F loss: -0.6502, acc 0.9932\n",
      "epoch 1952: train D loss: 0.6737, train F loss: -0.6433, acc 0.9926\n",
      "epoch 1953: train D loss: 0.6786, train F loss: -0.6472, acc 0.9910\n",
      "epoch 1954: train D loss: 0.6814, train F loss: -0.6554, acc 0.9926\n",
      "epoch 1955: train D loss: 0.6790, train F loss: -0.6539, acc 0.9938\n",
      "epoch 1956: train D loss: 0.6779, train F loss: -0.6544, acc 0.9944\n",
      "epoch 1957: train D loss: 0.6772, train F loss: -0.6488, acc 0.9930\n",
      "epoch 1958: train D loss: 0.6778, train F loss: -0.6439, acc 0.9912\n",
      "epoch 1959: train D loss: 0.6743, train F loss: -0.6445, acc 0.9920\n",
      "epoch 1960: train D loss: 0.6774, train F loss: -0.6417, acc 0.9912\n",
      "epoch 1961: train D loss: 0.6805, train F loss: -0.6520, acc 0.9926\n",
      "epoch 1962: train D loss: 0.6777, train F loss: -0.6550, acc 0.9946\n",
      "epoch 1963: train D loss: 0.6807, train F loss: -0.6545, acc 0.9936\n",
      "epoch 1964: train D loss: 0.6753, train F loss: -0.6477, acc 0.9944\n",
      "epoch 1965: train D loss: 0.6795, train F loss: -0.6501, acc 0.9936\n",
      "epoch 1966: train D loss: 0.6772, train F loss: -0.6468, acc 0.9914\n",
      "epoch 1967: train D loss: 0.6775, train F loss: -0.6512, acc 0.9942\n",
      "epoch 1968: train D loss: 0.6764, train F loss: -0.6507, acc 0.9938\n",
      "epoch 1969: train D loss: 0.6795, train F loss: -0.6532, acc 0.9924\n",
      "epoch 1970: train D loss: 0.6816, train F loss: -0.6532, acc 0.9914\n",
      "epoch 1971: train D loss: 0.6813, train F loss: -0.6527, acc 0.9930\n",
      "epoch 1972: train D loss: 0.6760, train F loss: -0.6505, acc 0.9926\n",
      "epoch 1973: train D loss: 0.6800, train F loss: -0.6484, acc 0.9912\n",
      "epoch 1974: train D loss: 0.6766, train F loss: -0.6569, acc 0.9954\n",
      "epoch 1975: train D loss: 0.6750, train F loss: -0.6441, acc 0.9924\n",
      "epoch 1976: train D loss: 0.6797, train F loss: -0.6452, acc 0.9908\n",
      "epoch 1977: train D loss: 0.6789, train F loss: -0.6492, acc 0.9922\n",
      "epoch 1978: train D loss: 0.6773, train F loss: -0.6558, acc 0.9948\n",
      "epoch 1979: train D loss: 0.6774, train F loss: -0.6411, acc 0.9930\n",
      "epoch 1980: train D loss: 0.6708, train F loss: -0.6174, acc 0.9862\n",
      "epoch 1981: train D loss: 0.6748, train F loss: -0.6436, acc 0.9922\n",
      "epoch 1982: train D loss: 0.6750, train F loss: -0.6513, acc 0.9942\n",
      "epoch 1983: train D loss: 0.6725, train F loss: -0.6334, acc 0.9948\n",
      "epoch 1984: train D loss: 0.6731, train F loss: -0.6402, acc 0.9910\n",
      "epoch 1985: train D loss: 0.6751, train F loss: -0.6410, acc 0.9912\n",
      "epoch 1986: train D loss: 0.6738, train F loss: -0.6502, acc 0.9940\n",
      "epoch 1987: train D loss: 0.6747, train F loss: -0.6505, acc 0.9944\n",
      "epoch 1988: train D loss: 0.6770, train F loss: -0.6530, acc 0.9934\n",
      "epoch 1989: train D loss: 0.6785, train F loss: -0.6523, acc 0.9930\n",
      "epoch 1990: train D loss: 0.6726, train F loss: -0.6512, acc 0.9946\n",
      "epoch 1991: train D loss: 0.6789, train F loss: -0.6555, acc 0.9930\n",
      "epoch 1992: train D loss: 0.6757, train F loss: -0.6530, acc 0.9946\n",
      "epoch 1993: train D loss: 0.6804, train F loss: -0.6618, acc 0.9958\n",
      "epoch 1994: train D loss: 0.6831, train F loss: -0.6524, acc 0.9914\n",
      "epoch 1995: train D loss: 0.6787, train F loss: -0.6490, acc 0.9928\n",
      "epoch 1996: train D loss: 0.6784, train F loss: -0.6358, acc 0.9900\n",
      "epoch 1997: train D loss: 0.6780, train F loss: -0.6425, acc 0.9902\n",
      "epoch 1998: train D loss: 0.6774, train F loss: -0.6496, acc 0.9916\n",
      "epoch 1999: train D loss: 0.6744, train F loss: -0.6466, acc 0.9936\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 2000\n",
    "\n",
    "# train num_epoch\n",
    "for epoch in range(num_epoch):\n",
    "    # You should chooose lamnda cleverly.\n",
    "    lamb = adaptive_lambda(epoch, num_epoch)\n",
    "    train_D_loss, train_F_loss, train_acc = train_epoch(source_dataloader, target_dataloader, lamb=lamb)\n",
    "\n",
    "    torch.save(feature_extractor.state_dict(), f'extractor_model.bin')\n",
    "    torch.save(label_predictor.state_dict(), f'predictor_model.bin')\n",
    "\n",
    "    print('epoch {:>3d}: train D loss: {:6.4f}, train F loss: {:6.4f}, acc {:6.4f}'.format(epoch, train_D_loss, train_F_loss, train_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o8_-0iSSje4w"
   },
   "source": [
    "# Inference\n",
    "\n",
    "就跟前幾次作業一樣。這裡我使用pd來生產csv，因為看起來比較潮(?)\n",
    "\n",
    "此外，200 epochs的Accuracy可能會不太穩定，可以多丟幾次或train久一點。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Wly5AgH2jePv"
   },
   "outputs": [],
   "source": [
    "result = []\n",
    "label_predictor.eval()\n",
    "feature_extractor.eval()\n",
    "for i, (test_data, _) in enumerate(test_dataloader):\n",
    "    test_data = test_data.cuda()\n",
    "\n",
    "    class_logits = label_predictor(feature_extractor(test_data))\n",
    "\n",
    "    x = torch.argmax(class_logits, dim=1).cpu().detach().numpy()\n",
    "    result.append(x)\n",
    "\n",
    "import pandas as pd\n",
    "result = np.concatenate(result)\n",
    "\n",
    "# Generate your submission\n",
    "df = pd.DataFrame({'id': np.arange(0,len(result)), 'label': result})\n",
    "df.to_csv('DaNN_submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "ISLqG6MoqXMC",
    "outputId": "5877a3c0-19af-4f54-c8c7-c417da9215ae"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_72fb0735-a73b-49ea-a20c-25ea09510e5e\", \"DaNN_submission.csv\", 788899)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from google.colab import files\n",
    "files.download('DaNN_submission.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZtXnEMUNCE78"
   },
   "source": [
    "# Training Statistics\n",
    "\n",
    "- Number of parameters:\n",
    "  - Feature Extractor: 2, 142, 336\n",
    "  - Label Predictor: 530, 442\n",
    "  - Domain Classifier: 1, 055, 233\n",
    "\n",
    "- Simple\n",
    " - Training time on colab: ~ 1 hr\n",
    "- Medium\n",
    " - Training time on colab: 2 ~ 4 hr\n",
    "- Strong\n",
    " - Training time on colab: 5 ~ 6 hrs\n",
    "- Boss\n",
    " - **Unmeasurable**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "duk7k43Am9xH"
   },
   "source": [
    "# Learning Curve (Strong Baseline)\n",
    "* This method is slightly different from colab.\n",
    "\n",
    "![Loss Curve](https://i.imgur.com/vIujQyo.png)\n",
    "\n",
    "# Accuracy Curve (Strong Baseline)\n",
    "* Note that you cannot access testing accuracy. But this plot tells you that even though the model overfits the training data, the testing accuracy is still improving, and that's why you need to train more epochs.\n",
    "\n",
    "![Acc Curve](https://i.imgur.com/4W1otXG.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s6UfXzef-wNl"
   },
   "source": [
    "# Q&A\n",
    "\n",
    "有任何問題 Domain Adaptation 的問題可以寄信到ntu-ml-2021spring-ta@googlegroups.com。\n",
    "\n",
    "時間允許的話我會更新在這裡。\n",
    "\n",
    "# Special Thanks\n",
    "這次的作業其實是我出在 2019FALL 的 ML Final Project，以下是我認為在 Final Report 不錯的幾組，有興趣的話歡迎大家參考看看。\n",
    "\n",
    "[NTU_r08942071_太神啦 / 組長: 劉正仁同學](https://drive.google.com/open?id=11uNDcz7_eMS8dMQxvnWsbrdguu9k4c-c)\n",
    "\n",
    "[NTU_r08921a08_CAT / 組長: 廖子毅同學](https://drive.google.com/open?id=1xIkSs8HAShdcfV1E0NEnf4JDbL7POZTf)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "「hw11_domain_adaptation.ipynb」的副本",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
